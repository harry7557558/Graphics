{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard ICP (Iterative Closest Point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2158\n",
      "108\n"
     ]
    }
   ],
   "source": [
    "import rosbag\n",
    "import rospy\n",
    "\n",
    "bag = rosbag.Bag(\"/home/harry7557558/fast_lio_ws/bag/utias/2024-02-21-13-06-52.bag\")\n",
    "\n",
    "messages = {}\n",
    "for topic, msg, msg_t in bag.read_messages():\n",
    "    if topic not in messages:\n",
    "        messages[topic] = []\n",
    "    messages[topic].append(msg)\n",
    "\n",
    "livox_imu = messages['/livox/imu']\n",
    "livox_lidar = messages['/livox/lidar']\n",
    "\n",
    "bag.close()\n",
    "\n",
    "print(len(livox_imu))\n",
    "print(len(livox_lidar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate IMU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "angular_velocities = np.array([[msg.angular_velocity.x, msg.angular_velocity.y, msg.angular_velocity.z] for msg in livox_imu])\n",
    "linear_accelerations = 9.81 * np.array([[msg.linear_acceleration.x, msg.linear_acceleration.y, msg.linear_acceleration.z] for msg in livox_imu])\n",
    "\n",
    "# Extract timestamps from IMU messages\n",
    "timestamps_raw = np.array([msg.header.stamp.to_sec() for msg in livox_imu])\n",
    "timestamps = timestamps_raw - timestamps_raw[0]\n",
    "\n",
    "# Create cubic spline interpolations for angular velocity and linear acceleration\n",
    "angular_interp = CubicSpline(timestamps, angular_velocities, axis=0)\n",
    "linear_interp = CubicSpline(timestamps, linear_accelerations, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsample point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plyfile import PlyData, PlyElement\n",
    "import point_cloud_utils as pcu\n",
    "\n",
    "\n",
    "def downsample_point_cloud(pc):\n",
    "\n",
    "    # H = np.dot(pc.T, pc) / pc.shape[0]\n",
    "    # U, S, Vt = np.linalg.svd(H)\n",
    "    # print(S**0.5)\n",
    "\n",
    "    bbox_size = np.amax(pc, 0) - np.amin(pc, 0)\n",
    "    size = np.prod(bbox_size)**(1/3)\n",
    "\n",
    "    sizeof_voxel = bbox_size / (8*size)\n",
    "\n",
    "    return pcu.downsample_point_cloud_on_voxel_grid(sizeof_voxel, pc)\n",
    "\n",
    "\n",
    "def write_point_cloud(points, filename):\n",
    "    vertex = np.array([(x, y, z) for x, y, z in points], dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4')])\n",
    "    vertex_element = PlyElement.describe(vertex, 'vertex')\n",
    "    PlyData([vertex_element], text=False).write(filename)\n",
    "\n",
    "\n",
    "def get_cloud_time(lidar_msg):\n",
    "    points = sorted(lidar_msg.points, key=lambda _: _.offset_time)\n",
    "    time_start = lidar_msg.header.stamp.to_sec()\n",
    "    # points = [p for p in points if timestamps_raw[0]<=time_start+1e-9*p.offset_time<=timestamps_raw[-1]]\n",
    "    times = 1e-9 * np.array([p.offset_time for p in points])\n",
    "    return time_start + times - timestamps_raw[0]\n",
    "\n",
    "\n",
    "def get_cloud(lidar_msg, id=-1):\n",
    "    points = sorted(lidar_msg.points, key=lambda _: _.offset_time)\n",
    "    time_start = lidar_msg.header.stamp.to_sec()\n",
    "    # points = [p for p in points if timestamps_raw[0]<=time_start+1e-9*p.offset_time<=timestamps_raw[-1]]\n",
    "    points = np.array([(p.x, p.y, p.z) for p in points])\n",
    "    # print(times.shape)\n",
    "    # print(points.shape)\n",
    "    points_downsampled = downsample_point_cloud(points)\n",
    "    # print(points_downsampled.shape)\n",
    "    if id >= 0:\n",
    "        write_point_cloud(points, \"{:04d}-raw.ply\".format(id))\n",
    "        write_point_cloud(points_downsampled, \"{:04d}-downsampled.ply\".format(id))\n",
    "    return points_downsampled\n",
    "\n",
    "pcl1 = get_cloud(livox_lidar[0], 0)\n",
    "pcl1_times = get_cloud_time(livox_lidar[0])\n",
    "pcl2 = get_cloud(livox_lidar[40], 40)\n",
    "pcl2_times = get_cloud_time(livox_lidar[40])\n",
    "pcl3 = get_cloud(livox_lidar[100], 100)\n",
    "pcl3_times = get_cloud_time(livox_lidar[100])\n",
    "\n",
    "# print(livox_imu[0], 0)\n",
    "# print(livox_imu[0], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICP point cloud matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 icp iterations 0.17671405290094902\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import KDTree\n",
    "\n",
    "def find_nearest_neighbors(pc1, pc2, n):\n",
    "    kdtree_pc1 = KDTree(pc1)\n",
    "    distances, indices = kdtree_pc1.query(pc2, k=n)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def icp(pc1, pc2, T_guess=np.identity(4), max_iterations=200, tolerance=1e-4):\n",
    "    # Initialize transformation matrix\n",
    "    T = T_guess\n",
    "    kdtree_pc1 = KDTree(pc1)\n",
    "\n",
    "    old_mean_distance = np.inf\n",
    "    for iteration in range(max_iterations):\n",
    "        # Find nearest neighbors\n",
    "        distances, indices = kdtree_pc1.query(pc2)\n",
    "\n",
    "        if True:\n",
    "            # 0.95 reject outlier - drops convergence, better or worse?\n",
    "            sorted_indices = np.argsort(distances)\n",
    "            num_indices = len(sorted_indices)\n",
    "            num_to_use = round(num_indices*0.95) if num_indices > 1 else 1  # Ensure at least one point is selected\n",
    "            indices = indices[sorted_indices[:num_to_use]]\n",
    "            distances = distances[sorted_indices[:num_to_use]]\n",
    "            pc2_filtered = pc2[sorted_indices[:num_to_use]]  # Filter pc2\n",
    "        else:\n",
    "            pc2_filtered = pc2\n",
    "\n",
    "        # Compute transformation\n",
    "        mean_distance = np.mean(distances)\n",
    "        closest_points_pc1 = pc1[indices]\n",
    "        centroid_pc1 = np.mean(closest_points_pc1, axis=0)\n",
    "        centroid_pc2 = np.mean(pc2_filtered, axis=0)\n",
    "        H = np.dot((closest_points_pc1 - centroid_pc1).T, (pc2_filtered - centroid_pc2))\n",
    "        U, S, Vt = np.linalg.svd(H)\n",
    "        R = np.dot(U, Vt)\n",
    "        t = centroid_pc1 - np.dot(R, centroid_pc2)\n",
    "\n",
    "        # Update transformation matrix\n",
    "        T[:3, :3] = R\n",
    "        T[:3, 3] = t\n",
    "\n",
    "        # Apply transformation to pc2\n",
    "        pc2_transformed = np.concatenate([pc2, np.ones((len(pc2), 1))], axis=1)\n",
    "        pc2_transformed = np.dot(T, pc2_transformed.T).T[:, :3]\n",
    "        pc2 = pc2_transformed\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.abs(old_mean_distance-mean_distance) < tolerance:\n",
    "            break\n",
    "        old_mean_distance = mean_distance\n",
    "        # print(mean_distance)\n",
    "    print(iteration+1, 'icp iterations', mean_distance)\n",
    "\n",
    "    return pc2_transformed, T\n",
    "\n",
    "pc2_transformed, T = icp(pcl1, pcl2)\n",
    "# pc1_transformed, T = icp(pcl2, pcl1)\n",
    "write_point_cloud(pc2_transformed, \"icp.ply\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try it on a bunch of point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 icp iterations 0.05532616802891236\n",
      "3 icp iterations 0.040070940629021536\n",
      "41 icp iterations 0.3357640548077795\n",
      "101 icp iterations 0.12183369340817149\n",
      "71 icp iterations 0.13874466240094707\n",
      "26 icp iterations 0.20360765366506972\n",
      "84 icp iterations 0.09983765012889767\n",
      "89 icp iterations 0.0989831234166814\n",
      "(52150, 3)\n"
     ]
    }
   ],
   "source": [
    "pcls = [\n",
    "    get_cloud(livox_lidar[0], -1)\n",
    "]\n",
    "all_pcl = pcls[-1]\n",
    "T = np.identity(4)\n",
    "for i in range(1, 80, 10):\n",
    "    pcl = get_cloud(livox_lidar[i])\n",
    "    # pcl_transformed, T = icp(pcls[-1], pcl, T)\n",
    "    pcl_transformed, T = icp(all_pcl, pcl, T)\n",
    "    pcls.append(pcl_transformed)\n",
    "    all_pcl = np.concatenate((all_pcl, pcl_transformed))\n",
    "print(all_pcl.shape)\n",
    "write_point_cloud(all_pcl, \"icp_all.ply\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
