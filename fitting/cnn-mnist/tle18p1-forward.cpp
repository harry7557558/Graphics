// Solve the "Hello, World" problem - https://dmoj.ca/problem/tle18p1
// Try to get 0.95 accuracy on the judge in as short time as possible.

#pragma GCC optimize "Ofast,unroll-loops"
#include <bits/stdc++.h>

using std::abs;
using std::max;
using std::min;


// Uncomment this line when debugging analytical gradient
// #define float double


// Quick-and-dirty PRNG from Numerical Recipes
uint32_t _IDUM = 1u;
uint32_t randu() { return _IDUM = _IDUM * 1664525u + 1013904223u; }
float randf() { return (float)randu() / 4294967296.0f; }


// Pooling - 28x28 to 8x8 to save weights
void pool(int ndata, const float* src, float* res) {
    for (int k = 0; k < ndata; k++) {
        const float* s = &src[28 * 28 * k];
        float* r = &res[8 * 8 * k];
        for (int i = 0; i < 8; i++) {
            for (int j = 0; j < 8; j++) {
                float v = 0.0f;
                for (int di = 0; di < 3; di++) {
                    for (int dj = 0; dj < 3; dj++) {
                        int i_ = 3 * i + 1 + di;
                        int j_ = 3 * j + 1 + dj;
                        v = max(v, s[i_ * 28 + j_]);
                    }
                }
                r[i * 8 + j] = v;
            }
        }
    }
}

// Load the MNIST dataset
int loadMnist(const char* x_path, const char* y_path, float*& x, float*& y) {

    // read x - between 0 and 1
    FILE* fp = fopen(x_path, "rb");
    fseek(fp, 0, SEEK_END);
    size_t fsize = ftell(fp);
    fseek(fp, 0, SEEK_SET);
    uint8_t* raw = new uint8_t[fsize];
    fread(raw, fsize, 1, fp);
    fclose(fp);
    int ndata = (int)(fsize / 784);
    float* x28 = new float[ndata * 784];
    for (int i = 0; i < ndata * 784; i++)
        x28[i] = (float)raw[i] / 255.0f;
    x = new float[ndata * 196];
    pool(ndata, x28, x);
    delete x28;

    // read y - vectors of 10, 0 false 1 true
    fp = fopen(y_path, "rb");
    fread(raw, ndata, 1, fp);
    fclose(fp);
    y = new float[ndata * 10];
    for (int i = 0; i < ndata; i++)
        for (int j = 0; j < 10; j++)
            y[10 * i + j] = (raw[i] == j ? 1.0f : 0.0f);
    delete raw;

    return ndata;
}

// Plot a digit
void plotDigit(const float* x) {
    for (int i = 0; i < 8; i++) {
        for (int j = 0; j < 8; j++) {
            fprintf(stderr, "%s", x[i * 8 + j] > 0.5f ? "##" : ". ");
        }
        fprintf(stderr, "\n");
    }
}


// Checks if the analytical gradient of a function is correct, returns a representation of error
float checkGrad(
    int ndim,
    std::function<void(const float* x, float* val, float* grad)> fun,
    float* x,
    float eps = 0.004f,
    bool verbose = false
) {
    float val;
    float* grad0 = new float[ndim];  // reference gradient
    float* grad = new float[ndim];  // temp evaluation gradient
    for (int i = 0; i < ndim; i++)
        grad0[i] = 2.0f * randf() - 1.0f;  // random
    fun(x, &val, grad0);  // evaluate reference gradient
    float maxerr = 0.0f;
    for (int i = 0; i < ndim; i++) {  // for each dimension
        float val1, val0;
        float x0 = x[i];  // forward difference
        x[i] = x0 + eps;
        fun(x, &val1, grad);
        x[i] = x0 - eps;  // backward difference
        fun(x, &val0, grad);
        x[i] = x0;
        float dvdx = (val1 - val0) / (2.0f * eps);  // central difference
        float err = abs(grad0[i]) < 1.0f ?  // adaptive error
            dvdx - grad0[i] : dvdx / grad0[i] - 1.0f;
        maxerr = max(maxerr, err);
        if (verbose) {
            fprintf(stderr, "%.4lf ", abs(err));
            // fprintf(stderr, "%.4lf ", fmod(100.0f * abs(err), 1.0f));
            // fprintf(stderr, "%d ", max(min((int)round(-log10(abs(err))), 9), 0));
        }
    }
    delete grad; delete grad0;
    return maxerr;
}



// Architecture

#define X_DIM (8*8)  /* input dimension, pooled */
#define L1N 32  /* size of layer 1 */
#define L2N 24  /* size of layer 2 */
#define L3N 16  /* size of layer 3 */
#define Y_DIM 10  /* output dimension */

// Number of weights in each layer
#define W1N ((X_DIM+1)*L1N)
#define W2N ((L1N+1)*L2N)
#define W3N ((L2N+1)*L3N)
#define W4N ((L3N+1)*Y_DIM)
#define WN (W1N+W2N+W3N+W4N)

// A struct of floats for easy access
struct FloatWN {
    float w1[L1N][X_DIM + 1];
    float w2[L2N][L1N + 1];
    float w3[L3N][L2N + 1];
    float w4[Y_DIM][L3N + 1];
};

// Activation function
void activate(float x, float* y, float* dydx) {
    // *y = max(x, 0.0f); *dydx = x > 0.0f ? 1.0f : 0.0f;  // standard ReLU
    *y = max(x, 0.1f * x); *dydx = x > 0.0f ? 1.0f : 0.1f;  // much better?!
    // *y = sin(x); *dydx = cos(x);  // easier to see if the gradient goes wrong
}


// Put weights here

float WEIGHTS[WN] = { -0.0584,-4.0618,-0.4856,-1.2262,-1.3538,-3.2185,-5.2904,-2.2534,2.4673,-1.0388,-2.0282,0.0664,-2.8166,-1.0474,-2.2359,-4.6248,2.1544,1.6229,-0.3400,-0.7672,0.3282,0.1758,-0.6325,-0.1286,1.4346,1.5148,0.1059,1.0530,0.4416,0.7320,-0.3267,-0.8941,-0.4049,-0.9982,1.9715,-4.1458,1.0052,-0.9956,0.8943,-1.1961,2.3669,0.6045,0.4270,-1.4791,-1.6652,-0.0024,1.3186,0.5301,-2.3433,2.0889,-0.7775,-1.2166,0.5653,-1.6783,0.3183,-3.2901,-6.7525,-3.0920,-6.0796,-1.7122,-0.2493,2.2535,-1.3675,-1.9858,-0.4690,5.2979,-0.4035,1.5649,-0.7880,0.7682,0.7410,0.2400,-2.4578,0.9924,0.1898,0.9164,0.6755,0.1239,3.3158,0.4676,0.6723,1.1525,0.9734,-0.7578,0.6756,-1.4787,-2.9455,-3.1658,-3.6640,1.9459,0.0798,0.5609,-0.8827,0.8058,0.4644,0.0780,-0.5355,1.0734,-0.9353,2.5654,-0.5174,-1.1444,-1.0392,-1.5861,-1.5908,-2.3455,-1.1233,-1.4921,-0.6353,-0.4903,0.4943,-0.4782,-0.3170,2.7964,1.0128,0.1003,0.9906,0.1449,0.7955,0.1899,1.6195,1.0581,0.6356,0.7124,-1.1857,0.1796,-0.0734,-0.0232,-0.0167,-0.7709,2.8065,1.0186,0.4117,1.9648,0.1990,-0.3004,0.3116,1.8617,0.4803,0.0817,0.6438,-1.0469,0.3042,-0.1056,0.1862,0.6732,-0.1121,-0.4557,-0.5625,0.2155,0.2878,0.1593,-0.1744,-0.3294,-4.8604,0.9753,0.5566,-1.4575,0.1777,1.0599,-1.0080,-0.3440,0.0553,-0.5662,-0.5398,-0.6220,1.5677,-1.5359,-2.4045,-0.9662,-0.2365,0.0687,-0.4659,1.4849,-0.0032,-0.3767,0.9378,0.0498,-1.7266,-0.5855,-0.4892,1.5655,0.3793,0.7596,0.9685,0.4312,0.1196,-0.1886,-0.4591,-0.0030,0.1698,-0.1365,-0.4702,1.4185,-1.1443,3.6521,-0.2692,0.5539,0.8678,-0.9015,0.8421,0.5236,0.1906,-1.2637,1.9888,-0.1749,-0.4157,1.0404,0.7642,0.6178,0.7636,0.2271,-0.5532,0.0028,-1.1564,-1.3510,1.0554,-0.4200,0.4341,-4.2582,0.0643,0.2970,-0.8044,1.0126,0.0824,0.4264,1.1403,1.8468,2.4225,0.5135,-0.6165,0.0685,-0.2533,-0.4799,0.1740,3.4720,-0.6834,2.3259,-2.0342,-0.1572,-0.7436,-0.3975,1.3170,-0.4582,1.4661,1.4804,-1.5475,-0.3729,0.1064,0.6951,-0.4527,-1.8096,0.3922,1.3959,0.7468,-0.0828,-1.2251,-1.3993,-0.4299,0.4344,2.1707,0.3439,0.7597,0.8356,-0.0991,-0.5274,0.8083,-0.6824,-0.2140,1.8864,0.2686,0.2638,0.6650,0.8553,0.4878,0.4855,-3.9669,-0.1142,-0.4184,-0.2987,-0.1055,-0.8843,-0.8260,0.3815,3.3819,-1.9873,-0.0266,1.1928,2.3166,0.5560,0.1542,-0.4834,0.7874,-0.8859,-2.8874,-2.9035,-0.4091,0.0771,-0.1141,-0.6526,0.8942,1.5756,0.6750,-0.6045,-0.1944,-0.6497,0.6355,-1.2876,-2.3121,1.2626,1.5436,1.0846,0.4083,-0.1024,-0.3936,0.2536,-1.5518,-2.0495,-0.9461,-0.0992,0.0560,0.2863,-0.2621,-2.4817,-1.4531,2.7570,1.8216,-2.0983,-1.0950,0.1947,-0.1438,1.1892,0.5686,-0.8315,0.0328,-0.3052,0.3111,-0.0673,-0.1068,0.4954,1.6608,1.0942,-0.7039,1.1274,1.5175,1.1721,0.1398,-0.5865,-0.7041,-0.9018,-0.4314,1.2108,-0.9002,-0.1280,0.1203,0.4612,-0.0030,1.7295,0.0398,0.6847,-1.1920,-0.7716,-0.5627,0.8191,0.4061,-0.1258,-0.3435,-0.0594,0.2985,-0.9921,-1.0676,0.7991,-0.2289,0.0034,-0.6093,-1.1390,0.4592,-0.5558,0.0740,1.1046,0.9034,0.4927,-0.7374,-0.8517,-0.1338,0.1105,0.8318,1.0610,0.6847,-0.3123,-2.6513,2.0264,2.1730,1.6820,-0.0580,-0.4304,1.0747,-1.5192,2.0157,-0.5913,0.1295,-0.5713,0.0276,-0.0819,-0.5549,-0.7813,-2.3008,0.3375,0.3226,0.0633,-1.3099,-0.3512,0.0259,0.4397,-1.0317,0.1052,-0.5939,0.0891,0.0426,0.9190,1.1295,2.5368,-0.7483,-0.5932,-0.4272,0.9045,0.8790,-2.4310,-1.4730,-2.5522,1.3528,-0.5545,-1.4076,0.9699,0.0594,-0.0909,-0.1470,-2.9510,-2.2125,0.3955,-0.3723,0.5868,-0.3013,1.2543,0.5213,-4.2235,-1.2293,-0.1392,0.7888,0.7024,0.6672,0.1817,-0.3982,-1.8453,0.7784,4.7791,-2.6409,-1.3331,-1.6784,1.3197,0.7190,0.0694,-1.7509,-1.7981,-0.2821,-0.3876,-1.2703,-0.5988,0.9579,0.2695,-1.0646,0.7649,0.1486,-0.2189,-1.1675,-0.4632,1.5165,1.6803,0.1380,1.2586,0.5173,-0.8892,1.7267,-0.8987,-1.2792,0.4468,1.5799,-0.4957,0.1772,0.0259,0.3309,-0.8376,-0.2882,1.1059,1.1036,0.4552,0.6423,-0.0322,-0.6512,-0.4507,0.8232,-0.2775,0.1771,0.6659,-0.1829,-0.4839,0.8354,0.1362,-0.1431,-0.0214,0.4194,-1.5314,0.1894,0.6296,0.1808,0.1114,0.1124,-1.2878,-0.3327,0.0731,4.1839,-1.6896,0.9559,-0.3600,-2.2690,-1.6668,0.3458,-0.7252,2.4049,-0.8320,-0.7050,-0.3282,-0.6928,-0.8010,-1.8373,-0.8568,0.0378,0.8770,0.8859,1.1314,2.5117,-0.5269,-1.6214,-2.4591,1.1214,0.4231,-0.0448,0.2765,0.3298,1.0128,-0.4229,0.4330,1.5655,0.0797,-0.2434,-0.1826,0.1351,0.2755,-1.0982,-0.6370,0.8228,-0.1169,-0.2612,-0.5843,1.0884,0.0162,0.0534,0.7959,-0.7611,0.0605,0.2331,-0.1437,-0.1050,-0.0914,0.2798,0.2310,-2.4988,-0.5494,-0.1976,0.2985,0.4192,0.0261,-0.8201,-1.0526,0.0078,2.1195,-0.9630,1.2101,0.6698,1.4892,1.0218,-1.3088,-4.6097,2.3634,-1.0185,-0.0097,0.3668,1.3649,0.2905,-0.0845,-1.2962,2.9304,1.3276,0.4902,-2.1508,-0.6795,1.4037,0.0018,0.0366,1.0100,-0.6882,-1.3029,-0.3789,2.5847,0.4018,-0.9071,-1.7080,-0.5776,0.0672,1.3825,-0.0546,0.6025,-0.4593,-0.9632,-0.3365,-0.5642,-0.0065,-0.4018,0.0866,0.0670,0.0291,0.5943,0.7165,1.5207,-0.1350,-0.6605,0.7207,-1.3149,0.8586,-0.0726,-0.3265,1.3933,0.2309,0.4885,-0.0813,0.8926,0.1584,-0.3733,-0.1448,-2.0207,-4.1342,1.0014,1.6473,-0.3521,0.2370,1.1501,0.5677,-0.2303,2.6000,1.4437,0.0663,0.0379,1.0045,0.4625,-0.2199,0.2456,0.6125,0.4580,0.6335,0.6213,0.0600,0.1382,-0.0661,-0.0302,-4.3175,-4.5202,-2.9190,-1.5182,0.1122,-1.2425,-0.2719,-0.1776,1.4594,0.8962,0.4294,2.5640,1.0854,0.4028,0.0213,-0.3347,1.4385,1.2336,1.1605,-1.4111,0.3708,0.9003,1.0748,-0.2912,0.6168,-0.8655,-0.3368,-0.6298,0.2766,-0.5158,-0.7471,2.0591,-2.0100,-0.7793,-0.6996,0.3847,-0.4634,-0.7615,-1.2219,-2.6868,-0.0155,-0.6619,0.3024,0.0138,-0.0133,-2.3192,0.5499,2.0355,-1.2313,2.7293,0.7725,0.8239,-0.3530,-1.6053,2.5253,-0.2652,-1.0653,-1.2301,1.0833,1.6664,-1.3832,0.7346,0.8108,-1.8335,0.5345,-1.0905,-1.4260,0.5113,-0.7598,0.1925,0.2682,-0.5483,0.0595,3.0112,1.8873,0.2485,-0.8414,-0.9948,1.0568,0.4986,0.0436,0.2303,-0.4240,-1.0451,-1.4209,1.1814,-0.1692,-0.1450,0.0596,-0.0870,-0.0859,-1.2380,-0.0857,0.5371,0.4764,-0.4424,-0.9100,-0.0592,-3.1975,-1.3823,-0.1414,-0.2610,0.9537,-1.1400,0.3096,0.1317,2.6984,-1.4218,-2.3257,-3.9531,-0.8958,0.1467,-0.1186,0.0047,1.2180,1.0481,-1.1539,-2.8128,-0.5907,-0.1694,1.0850,0.2508,-3.0753,1.6754,-4.7398,-2.3981,0.1253,1.2751,-0.0683,-0.1279,-1.0389,0.0677,-1.9650,0.8953,0.8816,-1.4902,-2.0730,-1.4730,-0.2448,-1.2372,0.5362,1.2867,-0.4079,1.0607,-1.3933,-1.3201,-0.2547,0.3278,-0.4384,0.1012,0.3497,0.8336,1.4892,-1.5511,-1.0371,-0.8399,0.2444,0.8563,1.0643,-0.6107,-1.7266,1.4229,-5.8871,-0.8395,-0.9155,-0.0405,-0.7151,-0.2109,0.4738,1.7676,0.1172,-2.0177,-0.4686,-2.7350,1.3222,-0.5016,-0.0646,-1.6105,0.4165,-1.1947,1.1116,0.8105,0.1840,-0.7835,-0.3358,-0.7149,-1.0412,-0.7832,-0.4283,0.2310,0.0845,-0.2713,0.5016,0.5341,-0.3683,-0.7165,-0.2809,1.1014,-0.2669,-2.0946,0.0964,0.5409,1.4394,1.8292,3.5964,1.3038,1.9393,0.4925,-0.1857,-0.4805,0.2896,2.8149,-0.5109,0.1712,0.0122,-0.6606,1.1471,-0.4220,-0.7238,-0.6468,-0.9298,-0.2940,-0.6245,-0.8196,1.0656,-1.1567,-0.6882,-0.9511,-0.5652,-0.3740,0.3718,0.5548,0.1388,-1.1161,-0.3361,1.2956,1.5686,-2.0358,0.5039,2.3769,1.6953,0.0007,-2.5157,-0.9691,1.8680,2.7979,0.0201,-1.3481,0.9465,-0.8422,0.2347,-1.1813,5.7132,-1.2192,-1.0604,-1.6995,-0.7333,-0.8891,-2.4775,-1.2743,1.5116,0.4630,0.1450,-1.3938,2.7352,0.8085,-1.4353,-2.4284,-3.8532,-1.4161,-1.6130,-2.8336,-1.4080,-0.8053,-0.0146,1.2398,-1.5268,-3.3123,-0.9299,0.6390,-0.2343,2.2907,1.3127,0.7457,-3.1764,0.1282,-0.8478,-3.1356,-0.8268,3.2979,-2.6100,-2.0047,-3.8555,1.2091,0.4284,-1.3633,-0.7213,0.1798,1.6574,-2.1864,-0.6401,-3.6122,1.5372,1.3017,1.1594,-1.7436,0.3258,-2.5068,0.3622,3.5393,-0.3114,-0.8610,0.1844,-0.1979,1.0604,2.3498,2.0167,1.3540,-1.0597,-0.1040,-0.2045,0.4629,-0.2035,-1.5824,-2.2972,-0.6829,0.7766,-0.3001,0.0296,-0.4818,-1.8209,-2.2917,-4.6711,0.4031,-0.4918,-0.9928,0.4392,0.0633,0.6697,0.9322,-0.3285,2.8642,-0.8176,-1.3930,-0.2132,-0.7932,1.3946,0.5498,1.6757,1.7995,0.6725,-0.4964,0.0384,-0.0788,0.8410,1.4656,1.5327,0.5351,0.8213,1.0662,0.7940,-0.3188,-0.3610,-1.0948,0.5436,-0.3174,2.2135,1.5995,-1.3924,1.5386,-1.0386,-4.1747,0.5191,1.0046,0.7430,3.0172,2.7506,1.5489,-0.0538,0.1727,-1.5771,-2.2221,2.7045,0.2731,-3.7449,0.1511,2.0109,-0.9668,-0.8669,-1.7624,-0.7661,-4.0704,0.2258,1.6521,-0.2372,-0.6944,0.5453,-3.3902,3.8217,1.8379,1.6765,0.0969,-1.5653,-0.9470,0.2803,0.1891,-0.4918,-1.4196,-0.4542,-0.0101,-0.5150,0.1384,-1.2839,-1.2439,0.2152,1.0678,-0.4207,0.7112,2.0179,0.5887,0.5434,0.4618,-1.6714,-0.6547,-0.1685,-0.3308,-0.9290,-0.1482,-0.2793,2.4668,-0.7814,3.3135,1.6244,1.9574,-0.1818,3.2430,0.3092,2.5022,2.4158,-0.4705,0.3883,0.5445,0.1344,-0.5699,0.5787,0.6716,0.4942,0.5146,0.2440,-0.2890,1.0012,0.4539,0.2619,-0.1910,0.6592,1.6239,-0.8597,0.5578,-0.4797,-1.2757,-0.2970,-0.9925,0.7874,-0.2838,0.9258,0.1094,-2.6759,-1.0654,-0.3600,-0.3651,0.1081,0.9840,-0.3438,0.0988,-0.1741,-0.1731,-2.7285,-1.1436,-1.7977,-0.2072,-0.9575,-1.1417,0.8253,0.5089,0.0826,1.2924,0.9195,2.6496,1.4321,0.9425,1.2779,1.9192,1.5874,0.7642,1.4923,0.1468,-0.7046,-0.4142,1.2737,-0.8349,0.6105,-0.3478,0.4539,-1.2155,-0.6361,-0.2424,-1.0055,-0.9512,1.1139,0.8185,-0.0783,0.2800,0.3558,0.4660,-1.5907,1.4724,1.3819,-0.5249,-1.4160,-0.1263,0.5637,-3.6660,-1.8715,1.4312,0.2890,-0.1765,0.5798,-0.5289,-1.8002,-0.7001,-0.4540,0.0267,0.4149,1.1988,0.4953,-0.8117,0.3779,0.9246,0.4666,0.4659,-0.2628,0.3144,-0.1385,-1.1422,0.0793,-0.9806,-0.6891,-0.3283,0.4190,0.4043,-1.0398,0.8093,0.6336,-0.1697,-0.6659,0.8317,0.8450,-0.0864,-1.7572,0.0531,-1.4285,-2.4855,1.7744,1.0117,2.0868,-0.7132,-0.7798,1.1263,0.6771,0.8281,0.1775,-0.6856,-0.3833,0.3934,-0.0246,0.4347,0.5902,-1.0530,-0.5975,-0.6067,0.1515,-0.6598,0.2852,-0.1430,-0.5754,-0.7778,-0.2222,-0.3167,-0.2346,-1.5228,0.0008,-0.4556,-0.1056,0.2699,0.2868,0.2573,-1.2868,-1.7234,-1.3533,0.0683,-0.4727,2.5135,2.3500,2.1819,1.0325,1.6972,1.3893,1.5536,0.8051,0.4063,0.5734,-0.2424,1.3272,1.1145,0.8748,0.4149,0.3856,-0.9985,-1.3491,-0.7572,-0.9164,-1.0085,-0.3351,-0.3286,0.2717,0.9840,0.2559,0.4656,-5.0305,-0.4532,-4.7012,-1.3884,-0.3326,-1.4715,0.5768,-1.8346,0.3601,-0.3993,0.4158,-0.6180,-0.0933,0.8216,1.8104,-0.4147,-0.0771,0.2346,-2.2620,1.0190,-0.2598,0.3842,-4.2808,-0.9883,-0.2627,-0.5608,-0.0604,1.7109,0.8741,-0.8465,1.1174,0.3680,0.4621,1.9898,3.3095,0.2659,0.9031,1.1989,1.6463,-2.0212,-1.8712,0.7231,-0.8108,0.6441,-0.7602,-0.9581,-1.3069,-0.0603,-1.9526,-1.7247,-1.4137,-1.1038,-0.2102,-0.3659,0.5692,0.0205,0.6402,-0.9228,-1.8264,-1.3041,0.1334,0.1900,0.4481,-1.7609,-1.7491,-2.8594,0.2649,-1.8962,-0.9212,0.6621,1.1319,0.7146,0.7446,-1.0972,0.6491,0.2529,0.1203,0.5854,-0.0785,0.3575,-0.2914,0.7649,0.4431,-1.0442,0.2797,1.2162,0.4862,0.4024,-0.1939,-0.4869,0.1179,1.0830,0.0511,-0.0785,0.8203,-0.0582,-0.1942,0.6641,-0.6730,1.2790,-0.8753,0.2716,-0.7752,-1.0029,0.4566,-0.9181,-0.9187,1.8262,-0.1501,0.1126,-0.3775,0.0107,-1.8221,-1.7729,1.1490,-0.5425,-1.5995,0.3157,-0.7185,0.7284,0.9468,1.3270,1.2480,-0.9302,-1.7016,-1.6087,1.9412,-0.6710,1.4930,3.2876,0.1773,1.7979,0.0303,-1.5111,1.9322,-0.0602,-2.2772,-1.3766,0.5557,1.5841,3.1738,0.9041,0.7206,1.5258,-3.0384,0.4175,-0.0413,-1.0617,-2.8640,-0.1418,-1.5484,-1.7556,-3.1792,0.5205,-0.0104,-0.5047,-0.6268,0.4260,0.2054,-0.3548,1.8963,-0.1941,0.1188,0.9425,-0.8947,-0.5022,0.0640,-0.1142,0.8008,-0.3243,-0.8889,-0.5284,-0.6376,0.8392,0.0395,0.0647,0.9891,0.0222,0.0400,0.1193,0.1158,0.3707,0.0402,-0.0933,2.0496,0.4282,0.8506,1.1713,0.2600,0.1923,-0.3872,0.5917,-0.0288,2.6985,0.4655,-0.5941,1.9294,1.2812,0.5807,3.2616,-0.2464,-0.5926,-0.3610,-0.2093,-0.3235,0.1915,-0.4423,-1.6007,-1.0947,0.0666,0.1349,-0.1015,0.6071,-0.4021,-1.6640,-1.1558,-2.2472,0.7053,0.0282,-0.0389,-0.5304,0.7746,0.1186,0.6417,2.6382,1.1063,0.8378,0.5021,-0.3336,1.7797,-0.9676,0.5817,0.0806,-0.5479,0.0151,0.0607,-0.8585,0.9142,-0.3599,0.8305,0.7569,-0.7788,-0.8846,0.3229,1.5117,0.6942,0.5986,-0.1853,-0.6627,-0.7986,-0.6157,-0.2748,-0.8224,-1.4211,-1.9334,-1.1010,-6.7722,0.3560,1.0275,-1.1730,1.8924,-0.1901,0.9314,-1.4496,0.1087,-0.4652,0.2869,0.3049,0.5124,0.0541,-1.2468,-0.5362,0.9790,0.6679,-0.0233,0.5491,0.0523,-0.1137,-1.5732,-0.7954,-0.5119,-0.9354,2.0320,0.4015,0.0339,1.6751,-0.8556,0.9909,-1.2862,-1.0120,0.9977,-0.4198,-0.0857,0.8503,0.4941,0.7038,0.8594,1.8309,1.0422,0.6823,0.3453,-0.2131,-0.3579,0.3511,0.4209,0.6748,1.1501,-0.6855,-0.2859,-0.1446,-0.2642,-0.0375,1.0592,0.5153,0.0482,-0.2937,-0.1954,0.1982,0.7593,1.4265,-0.0435,-0.6085,-0.2154,2.1535,-3.5345,-2.9874,0.0835,-1.0433,-0.3353,-1.0442,0.2144,1.0418,1.8072,-0.8489,-0.9371,0.0237,1.6000,0.7388,-0.0805,2.6409,-0.0570,-0.3553,1.4464,-0.6215,0.7034,0.3768,0.6225,-1.0862,-0.1743,-0.3471,-1.2254,-2.3960,1.0904,0.9439,-0.6521,1.2431,0.0014,-1.1321,-1.1225,2.0217,0.4151,-0.1199,-1.6073,1.3451,0.3058,0.3411,1.2004,0.5901,-0.4058,-1.4393,0.1360,1.2073,0.0577,0.1561,-0.0860,-0.6144,1.4628,0.6813,0.3516,-3.2686,1.1106,0.6979,0.1348,-0.2753,0.1602,-0.3474,0.8616,-0.9998,-2.9502,1.9541,1.2665,1.4359,-0.3907,-0.0331,0.1485,0.1024,-0.4418,-0.5946,0.1633,0.5494,-0.9257,-0.9177,0.4223,-0.1659,-0.6096,1.3287,1.1375,0.8349,-0.7451,0.5690,0.2610,-0.0896,1.3873,-0.1290,0.8563,0.1650,-1.3314,0.6348,0.0462,1.8911,0.4158,0.8912,-0.7578,-1.3402,-0.4608,0.4951,1.7397,1.4987,-1.8100,-0.6527,-1.4692,2.0820,1.5697,0.0917,-0.4855,-0.8615,-3.6791,-1.3957,-1.1249,1.9493,-0.4604,-1.5194,-1.3723,-0.2036,-2.8939,-0.1367,-0.2220,-0.0241,0.5438,-0.5837,-1.0744,-3.4437,0.4155,1.6767,0.1430,0.6141,0.9570,2.6172,0.3743,-0.4808,2.0413,-2.2478,0.3678,-1.2653,0.4408,0.6457,-0.6818,0.6504,0.2985,1.4878,-1.5622,-0.7034,0.4897,-0.7673,-0.7215,0.9110,-0.2371,0.7629,0.7576,0.0118,-0.5478,-0.4575,-0.1821,0.1145,1.4768,-0.7487,-0.1325,0.0244,1.6817,1.7489,0.3783,-0.0365,-0.7942,2.0701,-1.1712,-1.6358,-1.7369,0.7544,0.6607,0.3509,1.2050,-0.2818,-1.2321,-1.8013,-0.2888,1.3242,-0.5879,0.0199,-0.6109,-3.5364,-0.2003,-2.0719,-0.8023,-0.6374,0.1034,1.3382,-1.0778,-0.1407,3.5438,0.9337,-2.1607,1.4057,-0.1662,0.9930,-0.0966,-0.3733,4.0831,-2.3710,-0.5773,0.0551,0.5648,-1.0398,-0.4792,-0.1301,0.9776,0.6527,0.0827,0.3565,0.2188,2.2259,2.5711,2.9002,0.7856,-0.9231,-0.1735,-0.4645,-0.2454,-2.3698,-3.3715,-3.3987,-0.0255,-0.8630,0.6112,-0.6478,-0.0753,-0.4207,-0.4823,-1.9805,-0.8341,1.0915,0.2131,-0.6711,0.3193,0.7395,-0.2865,0.7921,-0.7359,0.0182,0.4434,1.0317,-0.4771,0.2209,-0.2403,-0.9745,-3.9162,0.7064,0.1476,-0.3436,0.3063,0.3161,-0.3059,1.3807,-0.4967,3.2225,0.0910,5.4409,1.6607,0.3638,1.7958,-0.7846,0.5149,-2.7257,-0.3752,0.2432,0.8300,0.5208,0.5929,-0.4910,-0.2248,-1.0446,0.4950,0.6631,0.5452,-1.0649,-1.0306,0.3863,0.5847,3.4345,-1.7517,-0.0602,-1.3295,-0.2860,-0.0780,0.7200,-0.6457,-0.7873,-2.3991,-0.5164,1.6862,0.0063,-0.6716,0.0356,0.5066,-2.9127,-0.2526,0.1888,1.1095,-1.2294,-0.5789,0.2831,-1.4643,-1.3836,-1.4228,0.7817,2.5811,-0.5473,0.7548,-0.0767,0.9268,-0.7978,-2.2606,-0.4840,0.9396,0.6009,0.6093,-0.2395,-0.6850,-1.9650,-1.2368,-1.9478,0.4617,0.9142,1.4275,1.5125,0.3369,-1.4079,-0.9011,0.5060,0.5551,-0.3506,-0.4684,0.0055,0.4558,0.2754,-0.1569,1.0948,0.4160,0.8221,1.2652,1.8390,1.2373,0.0897,1.4087,-0.7757,0.3493,-1.2460,-0.0771,1.4586,0.2931,0.9234,0.2502,0.5248,0.1167,0.1873,-0.4680,-0.5021,-0.1172,0.4209,1.9740,-0.1846,-1.8511,-1.3176,-0.4259,0.1701,0.5038,0.3100,1.0186,-0.4286,-0.8766,0.5633,0.5933,0.9374,-0.3175,-1.0138,-1.0632,0.7107,0.2087,-0.0063,0.6333,-0.2461,-0.7400,-2.2642,-1.4350,-2.6914,-4.4280,-5.6737,-1.6643,-1.3191,-1.6287,-6.5730,-3.4089,-2.1146,-1.2385,-1.4030,0.3700,0.5434,-1.9992,-0.0271,-0.0228,0.0665,0.0231,-0.3798,0.2576,-0.9923,0.0538,1.2614,-0.5590,-1.1719,1.2602,-0.9696,2.2807,-1.1767,0.7571,0.8533,1.0837,1.2273,0.6424,1.6625,-0.3500,-2.5512,0.1491,-1.2694,-2.5797,1.2502,-0.2317,-1.0692,-1.3030,0.0003,0.5334,0.1952,1.0961,-0.0899,-1.0424,-0.0300,0.2395,0.5134,0.3507,0.4507,-0.2536,-3.0104,0.3113,-0.2901,0.2394,0.3867,-0.0076,-0.5674,0.6205,0.6576,-0.8569,0.8514,1.1972,0.5013,-0.0749,0.4094,0.4772,0.2444,-0.7179,0.0193,-0.0410,-0.9589,-0.1804,-0.2674,-1.1842,1.5148,-0.7654,-1.1368,0.6146,-0.6665,-1.4048,-0.3394,-0.4025,0.2723,0.1276,0.0369,0.0927,-1.4888,-0.4002,0.7853,-0.7733,-0.1077,-1.6484,-1.1826,0.3302,0.2007,-0.1180,-0.0455,0.8907,1.1994,0.9569,0.2998,0.7372,-0.3297,0.0369,-0.6337,-0.6873,-3.2879,-1.0788,0.4650,0.0947,-0.2835,-1.1649,-1.3631,-1.8668,0.6491,-0.6205,0.1674,0.3236,0.7229,1.2448,0.8523,-0.3650,-0.4418,-0.0790,-0.7340,-0.6643,1.0030,-1.7100,0.7154,-0.8768,1.7456,0.5972,1.5735,0.4731,-0.0809,-0.8809,-1.4740,0.3838,-0.3999,-0.0668,0.4806,0.0671,0.6614,1.0887,0.0361,-1.6046,-0.6877,1.0180,0.4085,0.7485,-1.3520,-1.5789,-1.9746,0.7633,1.2222,1.1294,-0.5255,-0.7100,-1.1280,0.5117,-0.8809,-0.7435,1.2625,0.1270,-0.2434,0.5706,0.8207,-0.4639,-0.1549,-1.3569,-0.1119,0.3387,-0.2598,0.5782,-0.3424,-0.1825,0.0425,-0.3804,-0.1857,0.3508,1.7677,-0.3415,0.0213,0.4099,-0.8626,0.2630,-0.1211,0.7993,0.6168,-2.0214,-0.1911,4.9959,-0.6914,1.3554,0.6328,2.0194,-1.0245,1.6817,1.3110,-0.6535,-0.5124,-0.2629,-1.0851,0.2086,-0.2819,2.1730,1.5799,0.5025,0.8228,-0.6499,-0.7964,0.6431,0.1147,-2.3884,1.4068,0.9505,-2.3008,0.1393,-0.5182,-2.2037,1.2474,-0.4983,1.9568,2.3048,0.6702,-0.9202,-1.1220,0.4329,-2.7695,-0.7277,-0.7816,0.7913,0.5995,-0.1814,1.6141,-3.2273,-1.4451,0.1401,-0.8312,-0.3796,-0.0207,1.2441,-0.6765,0.8718,-2.1515,-0.3555,0.2241,-0.4679,-1.0901,-0.4227,-1.8550,-1.4407,-0.6507,0.0542,-1.1070,-0.4656,-0.6038,0.1598,0.1753,-1.7664,0.6993,1.6981,-1.1710,0.2218,-0.2199,0.3039,1.6964,0.1232,-1.3754,-0.0195,-0.7144,-0.8414,0.3411,0.8389,0.0467,-0.3593,0.4898,0.2863,-0.5041,-0.2529,0.3447,0.5077,0.0864,-1.3981,-0.9955,-0.6531,0.2191,1.4806,0.4566,-1.1639,-1.6191,-1.4615,-0.5463,0.1902,-1.4684,-0.5049,0.4782,0.9557,0.9324,0.7791,0.1642,-0.0931,1.5307,-1.9313,2.1163,-1.2252,1.6212,-1.1523,0.4168,-0.5051,0.0302,0.0112,-0.0071,-0.6131,-0.5039,-0.2017,-0.2770,1.0544,-2.2367,0.3438,-0.6329,-1.2001,0.4634,0.8811,-2.0993,-0.7085,-1.7044,-1.2159,-0.6488,-1.7280,-1.5307,1.2476,-1.4183,1.7897,0.0414,2.1656,-0.4354,0.7315,-0.0187,0.6634,0.2920,-0.4758,-0.1055,0.1712,-0.8687,1.3295,1.6845,0.4589,0.1994,1.0974,-1.2382,-2.8983,0.2257,0.3613,1.1325,-1.5514,-0.8230,-1.9414,1.6865,0.0103,0.3531,-0.0202,-0.2354,0.7097,0.5776,1.7950,0.1147,-0.0341,-1.4486,-0.4441,0.8228,1.1070,1.0122,-0.0902,-0.2724,-0.6391,-1.1799,-0.5020,-0.5305,0.5524,-0.7933,-1.7082,0.1415,0.3656,-0.6941,0.0130,-0.8276,-2.3261,0.0985,2.6629,1.3485,-3.6643,0.2544,-0.1087,0.9889,0.0852,1.9903,-0.8318,-1.0970,-0.0234,-0.5520,-0.1467,0.4293,3.0117,0.5538,0.6339,0.1962,0.1479,0.3840,0.7399,-0.1889,0.1323,-1.9197,0.3249,-0.2656,-1.5490,-0.1431,-1.7752,-0.0945,-0.2472,-0.6979,-1.0370,-2.2492,1.5564,0.3123,0.5883,-1.8161,-1.5862,-0.2201,-0.3071,-1.4334,0.8765,-0.5119,1.0549,0.5909,0.4365,0.2857,0.3214,0.1763,0.2822,0.3551,1.2344,1.2797,-0.5172,-1.3322,0.8094,1.6128,-0.3167,1.0442,2.4049,-0.3776,1.8894,-1.4067,0.0166,-0.0977,0.3338,2.3053,-0.0001,0.3844,0.4488,1.1412,-0.3709,-0.6625,-1.0601,-1.1320,-0.4797,-0.1122,-0.0943,0.2910,-1.3835,1.2280,-0.2015,-0.4968,1.2586,-0.6447,-0.7789,0.1685,0.2309,1.5066,0.5723,-0.3959,-0.1964,0.0885,0.3032,0.3324,-0.5521,-1.8745,-1.6038,1.9588,-0.2771,-0.2053,-0.3948,-0.2707,0.8720,0.2392,1.1507,0.5942,-0.1232,-0.8583,-1.3625,0.4593,1.0356,-1.7352,0.2553,-0.0737,0.0786,0.2689,0.6898,0.9183,-0.2526,1.0102,-0.9465,-1.5355,0.2032,-0.2405,0.7502,0.8709,-0.5130,-0.3562,-1.7973,-0.5460,0.6678,2.8567,0.7090,-0.2690,-0.3288,-0.4799,-1.5432,-0.2577,-0.1584,-0.4792,1.2556,0.2072,-0.0031,0.4842,1.0236,-1.7957,-0.3025,-1.0919,0.1334,0.6244,-1.3953,0.6476,-0.0846,-1.1152,0.1212,-0.1865,0.7092,-0.0492,0.4650,-1.2067,0.5774,1.3264,0.7459,-0.9285,-0.1477,0.0447,0.0037,-0.0265,0.8653,-0.1114,-0.2350,0.1576,1.2279,1.1155,0.5867,0.0932,-0.0120,0.7497,0.5828,0.7264,-0.1017,-0.6586,-2.4390,-0.3027,0.4020,-0.2988,-0.8113,0.8910,1.0619,-0.2491,-0.9115,-0.7307,-1.7988,-0.1198,1.0187,-1.3675,2.7881,-2.7556,0.5442,1.7837,-0.0234,-1.0859,-0.1019,-0.8407,-0.9867,-0.5798,0.9235,-0.2700,0.2703,-0.6463,1.6356,0.4551,1.7021,-1.7867,-0.3624,0.0968,0.7302,-0.4005,-0.8751,0.1037,-1.0345,-0.2818,2.0713,-0.4563,0.2008,1.4163,0.8803,-0.9917,0.4335,0.3226,-1.1558,-0.8457,0.3251,0.0022,0.1472,-0.3276,-0.8867,-0.1029,0.1184,0.0103,0.7022,-0.3055,0.4454,-0.6456,-0.9517,0.4407,-2.2769,0.5229,-0.5114,0.1415,1.5270,-0.5625,-0.2255,0.5531,0.0895,0.6477,-0.1861,-0.0470,0.5261,-0.3438,-0.1032,1.4943,-1.8098,-0.7480,0.9638,-1.2671,-0.2084,-1.2277,-0.5822,1.1887,0.8866,-0.2737,-0.1074,0.5426,-2.6312,-3.6788,0.0219,-0.2899,1.5503,1.0118,-1.8380,0.9547,-2.5435,-0.3884,0.6288,-0.5978,-2.0669,1.0709,0.1013,1.0958,-1.5101,0.9628,0.0618,0.8778,0.2149,2.0207,-2.0280,0.7963,-1.4669,-1.6351,-0.8517,0.1929,-0.2133,0.4828,-0.7171,1.3700,-0.7836,-1.8899,-0.4988,1.5116,-0.1975,-2.7576,0.2633,-0.3799,0.0243,0.2562,0.1856,-0.2430,-0.7815,0.0999,0.7050,-0.2956,-1.3352,-1.6532,-0.1137,0.8233,-1.4756,0.7463,0.2862,2.7196,0.9370,0.0758,0.4873,-3.6015,-0.0142,0.4031,0.7055,-0.2046,0.2087,0.0549,0.1924,-0.6503,-1.6815,0.8858,-0.0573,2.3693,-1.5571,-0.5711,1.0384,-0.5654,-0.7107,1.6367,-0.4239,0.7032,-0.9514,0.3809,1.1748,0.6666,-1.0019,0.7539,0.6295,-0.9753,0.4673,-0.5442,0.3790,0.5037,-2.1464,-0.4877,0.2363,-0.0545,2.5568,2.3504,-1.0070,-0.9264,-0.7355,-1.2989,-0.8030,2.8546,0.3657,-1.8380,0.4168,-0.0857,-0.1258,0.8859,-0.6458,-0.8173,0.7214,-0.5221,0.5836,-0.2059,-0.1606,0.4322,-0.6896,-2.8245,0.8342,3.2311,-1.2913,-0.2228,0.3727,-1.5410,-0.5407,2.1237,0.9580,-0.0087,1.1546,-0.3272,-2.3441,-0.4867,-3.5748,-0.6733,-2.6340,-1.9095,-0.3558,-2.8758,0.4782,-0.9227,2.2909,0.6363,0.2671,-1.2015,0.0902,0.2284,-2.6353,-0.9833,0.3419,-0.4998,-0.7190,-1.6740,0.5720,-0.6719,-0.6915,0.4144,-0.0679,1.5029,-0.8306,-1.1026,-0.6885,1.3754,0.2744,-0.2626,0.8768,-1.4866,1.1716,-0.2886,0.0870,-0.0501,0.3139,0.0426,0.3947,-1.1547,-0.4544,0.3293,-1.5221,-0.8302,-0.4633,1.2725,-0.9530,-0.6197,0.2247,2.5178,0.5096,0.0417,-0.5711,-0.9418,0.0130,0.6748,0.8232,0.1348,0.0618,0.9242,-0.3242,0.5077,-0.0234,-0.4253,0.5307,-0.5109,-0.1144,-0.0181,-0.2167,1.7900,0.1124,-0.0285,0.0527,0.5416,-1.1036,-0.1867,-0.2208,0.8741,-0.6158,-0.1463,0.3684,-0.5949,0.3206,0.2914,-0.7212,-0.1220,0.0661,-0.0282,-0.2306,0.2666,-3.0516,0.0695,0.2544,-0.4315,-1.1388,0.1507,-0.2983,-0.3166,-0.4248,-1.8248,-0.4687,-0.7138,0.5816,0.3322,0.6067,0.6779,-1.9572,-0.5239,0.3162,-0.0698,0.1140,0.3225,-0.3348,-0.0334,-0.4318,-0.2721,-0.2206,-0.6097,-1.7569,0.8543,0.2775,1.4428,0.1311,-0.4372,-0.5861,-0.2590,-0.0739,-0.3579,0.6298,-0.2686,-0.4759,0.5529,0.3180,-0.4990,-1.6400,-0.7498,-0.2840,1.1314,0.1948,0.2871,0.0315,-0.1410,-0.3689,0.3057,-2.0474,0.2570,-1.2815,0.6659,0.7686,-2.7888,-0.0697,-0.4290,0.3075,0.9575,-0.4085,0.8413,-0.6787,0.3480,-0.2223,0.4853,0.1180,-0.1760,-1.8126,0.5515,0.9123,-0.2171,-0.2568,0.2443,-0.4805,-1.6368,1.1539,-0.8216,-0.1617,-1.3015,0.6105,1.3107,0.1184,-0.7592,0.1736,0.3582,0.8050,-0.0567,-1.5821,1.1826,-0.5277,-0.2687,-0.1404,-1.9688,0.8658,-1.6565,-0.6737,-0.2186,-0.5540,0.2962,0.1745,0.6164,0.3177,-0.5247,0.9179,0.1614,0.0004,-0.2467,0.2223,0.2944,0.6270,0.9209,0.3724,-0.4915,0.1634,0.4601,-1.2416,-0.5088,-0.4776,1.6149,-0.4733,0.8842,-0.0296,0.0206,-1.0861,0.4232,-1.5755,0.2912,-0.1682,-0.4521,-0.3634,1.5243,1.2542,-1.2486,0.1218,-0.8245,1.0527,0.2778,-0.9380,-0.4870,-0.2132,-1.5105,-0.2279,0.4266,0.0083,0.1390,-0.2302,-0.3509,0.3773,0.0492,-0.3270,-0.3870,-0.0899,0.5609,-0.3490,-0.2647,-1.6870,-0.7177,-0.1988,-0.8699,-0.7711,0.0690,-0.0618,0.3451,-0.3283,-0.1131,0.9165,0.0348,-0.1697,0.4105,0.1766,-0.6382,-1.2637,0.9028,0.3867,-0.0251,0.7561,-1.5727,0.2944,0.2176,2.9195,0.6091,0.0930,0.5324,-0.4953,-0.8639,-0.3289,-0.0100,-0.4434,0.3408,0.6463,0.2466,0.6187,0.0328,-0.1992,-0.4672,0.0819,0.4725,-1.1067,0.3222,-0.2572,-0.6730,0.3072,0.0570,-0.0771,-2.1171,-0.0718,0.2085,0.1009,0.2809,-0.6295,-0.4419,-0.5048,0.0175,-0.8801,0.1094,-2.2282,0.2457,0.4016,-0.6687,-0.0254,0.0027,0.2824,-0.3131,-0.8853,-0.1897,0.4000,-0.1950,-0.3131,0.7978,-0.7542,0.0678,0.0401,0.6699,0.2428,-2.0931,-0.1588,-0.6307,-0.1301,0.3468,0.4524,-1.2765,-0.1311,-0.4960,-0.5752,0.3038,-0.2292,0.5459,-0.7620,0.2187,-0.5567,1.7773,0.0459,-0.1833,-2.6118,0.1528,-0.5365,0.0724,0.8051,-0.0509,0.5160,0.2463,-0.7323,-0.2358,-1.3849,-2.1471,0.7602,-1.5055,-1.1479,-0.4246,0.2223,0.1262,0.4841,0.3551,-1.6919,-1.1587,0.2655,-0.2924,-0.7316,0.1658,1.3606,-0.2927,-1.2683,-0.4685,0.3480,-1.9280,0.5334,-0.0169,-0.5590,-0.4623,0.1673,1.0331,0.6775,-2.3413,-0.5825,-0.6594,0.3690,-0.3714,-0.1066,0.5135,-0.6661,-1.1757,-0.4719,-0.0457,0.2352,-2.2011,-0.6397,0.0621,0.0406,0.7755,0.1683,-1.2984,0.3985,0.1015,-0.2437,0.0603,-0.3920,-0.3274,0.6570,-0.3326,-0.0035,0.0917,-0.6385,-0.1887,-0.5928,0.9158,0.3298,0.2195,0.0506,0.9090,-0.1491,-0.0317,-0.2392,0.3605,0.0759,0.9796,-1.6030,-0.7105,-0.7362,0.8441,0.0876,0.7811,-0.8022,-0.8982,0.5974,0.2562,-0.2411,0.1347,0.2661,0.7489,1.0778,0.2091,1.1820,-0.1450,0.2202,-0.0983,0.8349,0.6346,0.9159,0.9305,0.2537,-0.3407,0.7837,0.2240,-0.9130,0.9631,0.1942,0.3911,0.5814,0.2198,-0.0730,-0.0771,0.4342,1.3726,-1.1987,0.1939,0.5006,0.4518,-0.5677,-0.0850,-0.1417,-0.4400,0.0280,0.6455,0.8703,0.3254,-1.0258,0.2579,-0.1822,-0.1651,1.2718,-0.1338,1.2858,1.3633,0.2685,-0.0787,0.5908,0.4361,-0.4331,1.0087,-0.3061,0.8251,0.4141,0.5877,-0.2573,0.1480,1.2827,1.0825,0.1740,0.9499,0.9328,-0.2087,-0.3455,0.9170,0.4280,-0.2822,0.9940,0.1463,0.5235,0.3445,0.1484,0.6899,0.1874,-0.0246,1.3434,0.3009,0.5224,0.9764,-0.2643,-1.0955,0.6988,0.4348,0.2547,0.8428,0.2290,-0.0372,0.2526,1.2008,0.1548,-0.2296,0.8880,0.9273,0.0900,1.0235,1.4365,-0.2748,-0.0306,0.7343,0.1379,-0.1756,1.0982,0.5601,0.4140,0.7188,-0.6733,0.3749,0.1142,-0.8575,0.9901,0.6668,0.6703,1.3946,-0.2375,-0.0671,0.6882,-0.0746,-0.3983,1.1803,0.8766,0.0647,0.2558,-1.1424,-0.5066,-1.6957,-3.9181,1.3964,-0.1776,1.2253,0.7223,-0.0047,-1.1301,0.3667,0.3835,0.1512,0.0067,0.1368,0.9295,0.8788,-0.0671,-0.0345,0.5146,-0.5247,0.9828,0.2627,0.8387,1.0032,0.0361,-0.2831,0.7320,0.6815,-0.2940,0.9679,0.6166,0.3410,0.3834,-0.6060,0.2032,-0.0887,0.5361,0.6027,0.1671,1.0533,1.2138,0.1051,-0.7107,0.9054,0.4556,0.2802,0.8696,0.2705,0.4161,0.3659,0.2194,0.4135,0.2670,-1.1402, };


// Training

// Loss function for a single data
void lossFun(const FloatWN* w, const float* x, const float* y, float* val, FloatWN* grad) {

    // 4 full-connected layers
    const int LN_I[4] = { X_DIM, L1N, L2N, L3N };  // layer input sizes
    const int LN_O[4] = { L1N, L2N, L3N, Y_DIM };  // layer output sizes
    float v1[L1N], v2[L2N], v3[L3N], v4r[Y_DIM];
    float g1[L1N], g2[L2N], g3[L3N];
    const float* fci[4] = { x, v1, v2, v3 };  // layer inputs
    float* fcv[4] = { v1, v2, v3, v4r },  // layer outputs
        * fcg[4] = { g1, g2, g3, nullptr };  // gradient outputs
    const float* fcw[4] = { &w->w1[0][0], &w->w2[0][0], &w->w3[0][0], &w->w4[0][0] };  // layer weights
    for (int l = 0; l < 4; l++) {  // for each layer
        for (int i = 0; i < LN_O[l]; i++) {
            float s = fcw[l][i * (LN_I[l] + 1) + LN_I[l]];
            for (int j = 0; j < LN_I[l]; j++)  // multiply matrix
                s += fcw[l][i * (LN_I[l] + 1) + j] * fci[l][j];
            if (l < 3) activate(s, &fcv[l][i], &fcg[l][i]);  // activate
            else fcv[l][i] = s;  // output layer
        }
    }

    // Softmax
    float v4[Y_DIM];
    float g4[Y_DIM][Y_DIM];  // Jacobian, g6[i][j] = ∂v6[i]/∂v6r[j]
    float smax = -1e10f;  // prevent overflow in exp
    for (int i = 0; i < Y_DIM; i++)
        smax = max(smax, v4r[i]);
    float sexp = 0.0f;  // sum of exps
    for (int i = 0; i < Y_DIM; i++)
        sexp += (v4[i] = exp(v4r[i] -= smax));
    for (int i = 0; i < Y_DIM; i++)  // value
        v4[i] /= sexp;
    for (int i = 0; i < Y_DIM; i++)  // gradient
        for (int j = 0; j < Y_DIM; j++)
            g4[i][j] = i == j
            ? v4[i] * (1.0f - v4[i])
            : -v4[i] * v4[j];

    // Cross-entropy
    float loss = 0.0f;  // final value
    float go[Y_DIM];  // output derivative
    for (int i = 0; i < Y_DIM; i++) {
        v4[i] = max(min(v4[i], (float)(1.0f - 1e-7f)), (float)1e-7f);
        float dv = -y[i] * log(v4[i]) - (1.0f - y[i]) * log(1.0f - v4[i]);
        go[i] = -y[i] / v4[i] + (1.0f - y[i]) / (1.0f - v4[i]);
        loss += dv;
    }
    *val = loss;

    // Backprop output layer
    float b4[Y_DIM];  // ∂[loss]/∂[v4r]
    for (int i = 0; i < Y_DIM; i++) {
        b4[i] = 0.0f;
        for (int j = 0; j < Y_DIM; j++)
            b4[i] += g4[i][j] * go[j];
        for (int j = 0; j < L3N; j++)
            grad->w4[i][j] = b4[i] * v3[j];
        grad->w4[i][L3N] = b4[i];
    }

    // Backprop full-connected layers
    float b3[L3N], b2[L2N], b1[L1N];
    float* fcb[4] = { b1, b2, b3, b4 };  // ∂[loss]/∂[v]
    float* fcwg[3] = { &grad->w1[0][0], &grad->w2[0][0], &grad->w3[0][0] };  // gradient of weights
    for (int l = 3; l--;) {
        for (int i = 0; i < LN_O[l]; i++) {
            fcb[l][i] = 0.0f;  // gradient to layer
            for (int j = 0; j < LN_O[l + 1]; j++)
                fcb[l][i] += fcb[l + 1][j] * fcw[l + 1][j * (LN_I[l + 1] + 1) + i] * fcg[l][i];
            fcwg[l][i * (LN_I[l] + 1) + LN_I[l]] = fcb[l][i];
            for (int j = 0; j < LN_I[l]; j++)  // gradient to weights
                fcwg[l][i * (LN_I[l] + 1) + j] = fcb[l][i] * fci[l][j];
        }
    }

}

// Loss function for a data batch
void lossFunBatch(int ndata, const float* w, const float* x, const float* y, float* val, float* grad) {
    *val = 0.0f;
    for (int i = 0; i < WN; i++) grad[i] = 0.0f;  // init to zero
    float val_t, grad_t[WN];
    for (int di = 0; di < ndata; di++) {
        lossFun((const FloatWN*)w, &x[di * X_DIM], &y[di * Y_DIM], &val_t, (FloatWN*)grad_t);
        *val += val_t;  // add data
        for (int i = 0; i < WN; i++) grad[i] += grad_t[i];
    }
    // divide by number of data to average
    *val /= float(ndata);
    for (int i = 0; i < WN; i++) grad[i] /= float(ndata);
}

// Adam optimizer
void minimizeAdam(
    int ndim, int ndata,
    std::function<void(int ndata, const float* w, const float* x, const float* y, float* val, float* grad)> lossfun,
    float* w, const float* x, const float* y,
    int batch_size, float learning_step,
    float beta_1, float beta_2, int max_epoch, float gtol,
    std::function<void(int epoch)> callback
) {

    // loss and gradient
    float loss_t, loss = 0.0f;
    float* grad_t = new float[ndim];  // gradient in evaluation
    float* grad = new float[ndim];  // smoothed gradient
    float* grad2 = new float[ndim];  // smoothed squared gradient
    for (int i = 0; i < ndim; i++) {
        grad[i] = 0.0f;
        grad2[i] = 0.0f;
    }

    // epoches
    for (int epoch = 0; epoch < max_epoch; callback(++epoch)) {

        // batches
        for (int batch = 0; batch < ndata; batch += batch_size) {
            // get data
            int n_batch = min(ndata - batch, batch_size);
            // evaluate function
            lossfun(n_batch, w, &x[batch * X_DIM], &y[batch * Y_DIM], &loss_t, grad_t);
            // check NAN
            bool has_nan = false;
            for (int i = 0; i < WN; i++) {
                if (std::isnan(grad_t[i])) has_nan = true;
            }
            if (has_nan) continue;
            // update
            loss = beta_1 * loss + (1.0f - beta_1) * loss_t;
            for (int i = 0; i < ndim; i++) {
                grad[i] = beta_1 * grad[i] + (1.0f - beta_1) * grad_t[i];
                grad2[i] = beta_2 * grad2[i] + (1.0f - beta_2) * grad_t[i] * grad_t[i];
                float inv_g2 = 1.0f / (sqrt(grad2[i]) + 1e-8f);
                // w[i] -= learning_step * grad[i] * inv_g2;
                // w[i] -= 5.0f * learning_step * grad[i] * inv_g2 * (1.0f - exp(-abs(grad_t[i])));
                w[i] -= learning_step * grad[i] * inv_g2 * (1.0f - exp(-abs(grad_t[i]) * inv_g2));
            }
            // verbose
            fprintf(stderr, "\r%.1f%% loss=%f", float(batch + batch_size) * 100.0f / float(ndata), loss);
        }

        // check
        float grad_norm = 0.0f;
        for (int i = 0; i < ndim; i++) grad_norm += grad[i] * grad[i];
        grad_norm = sqrt(grad_norm);
        fprintf(stderr, "\rEpoch %d/%d, loss=%f, grad=%f\n", epoch + 1, max_epoch, loss, grad_norm);
        if (grad_norm < gtol) break;
    }

    delete x; delete y;
    delete grad_t; delete grad; delete grad2;
}



// Testing

// Evaluate a test data
void evalFun(const FloatWN* w, const float* x, float* y) {
    float gtemp;

    // 4 full-connected layers
    const int LN_I[4] = { X_DIM, L1N, L2N, L3N };  // layer input sizes
    const int LN_O[4] = { L1N, L2N, L3N, Y_DIM };  // layer output sizes
    float v1[L1N], v2[L2N], v3[L3N], v4r[Y_DIM];
    const float* fci[4] = { x, v1, v2, v3 };  // layer inputs
    float* fcv[4] = { v1, v2, v3, v4r };
    const float* fcw[4] = { &w->w1[0][0], &w->w2[0][0], &w->w3[0][0], &w->w4[0][0] };  // layer weights
    for (int l = 0; l < 4; l++) {  // for each layer
        for (int i = 0; i < LN_O[l]; i++) {
            float s = fcw[l][i * (LN_I[l] + 1) + LN_I[l]];
            for (int j = 0; j < LN_I[l]; j++)  // multiply matrix
                s += fcw[l][i * (LN_I[l] + 1) + j] * fci[l][j];
            if (l < 3) activate(s, &fcv[l][i], &gtemp);  // activate
            else fcv[l][i] = s;  // output layer
        }
    }

    // output
    for (int i = 0; i < Y_DIM; i++)
        y[i] = v4r[i];
}

// Convert output vector to class
int getClass(const float* y) {
    int maxi = -1;
    float maxval = -1e10f;
    for (int i = 0; i < Y_DIM; i++) {
        if (y[i] > maxval)
            maxi = i, maxval = y[i];
    }
    return maxi;
}


// Main functions


// Main training function
void mainTrain() {

    // data
    float* x_train, * y_train;
    int n_train = loadMnist("bin/all_x.bin", "bin/all_y.bin", x_train, y_train);
    // int n_train = loadMnist("bin/test_x.bin", "bin/test_y.bin", x_train, y_train);

    // plot data
    if (0) {
        for (int _ = 0; _ < 10; _++) {
            int i = randu() % n_train;
            plotDigit(&x_train[X_DIM * i]);
        }
        exit(0);
    }

    // random weights
    if (0) {
        for (int i = 0; i < WN; i++)
            WEIGHTS[i] = 0.1f * (2.0f * randf() - 1.0f);
    }
    fprintf(stderr, "%d weights\n", WN);

    // check the correctness of gradient
    if (0) fprintf(stderr, "grad_err: %g\n",
        checkGrad(WN, [&](const float* w, float* val, float* grad) {
            lossFun((const FloatWN*)w, &x_train[0], &y_train[0], val, (FloatWN*)grad);
            }, WEIGHTS, 0.004f, true)
    );

    // Optimization
    auto callback = [&](int epoch) {
        // test accuracy
        const int test_count = 1000;
        int correct_count = 0;
        float y[Y_DIM];
        for (int _ = 0; _ < test_count; _++) {
            int i = randu() % n_train;
            evalFun((FloatWN*)WEIGHTS, &x_train[i * X_DIM], y);
            int class_guessed = getClass(y);
            int class_correct = getClass(&y_train[i * Y_DIM]);
            if (class_correct == class_guessed) correct_count += 1;
        }
        printf("%d/%d correct\n", correct_count, test_count);
        // export weights
        FILE* fp = fopen("weights.txt", "w");
        for (int i = 0; i < WN; i++) fprintf(fp, "%.4f,", WEIGHTS[i]);
        fclose(fp);
    };
    minimizeAdam(WN, n_train, lossFunBatch, WEIGHTS, x_train, y_train, \
        1000, 0.01f, 0.9f, 0.999f, \
        100, 1e-6f, callback);
}


// Main solving function - https://dmoj.ca/problem/tle18p1

// fast-io
char _i[8 << 20], _o[4096];
#define scanu(x) { for (x=_i[_i0++]-48; 47<(_=_i[_i0++]); x=x*10+_-48); }
#if 0
#define fread fread_unlocked
#define fwrite fwrite_unlocked
#endif

void mainSolve() {
#ifdef _WIN32
    freopen("stdin.txt", "r", stdin);
#endif
    fread(_i, 1, 8 << 20, stdin);  // read input
    char _; int _i0 = 0, _o0 = 0;
    int n; scanu(n);
    while (n--) {
        float x28[28 * 28], x[X_DIM], y[Y_DIM];
        for (int i = 0; i < 28 * 28; i++) {
            int a = 0; scanu(a);
            int b = 0; scanu(b);
            x28[i] = float(a) + 1e-5f * float(b);
        }
        pool(1, x28, x);
        // plotDigit(x);
        evalFun((const FloatWN*)WEIGHTS, x, y);
        int ans = getClass(y);
        _o[_o0++] = ans + '0'; _o[_o0++] = '\n';
    }
    fwrite(_o, 1, _o0, stdout);
}


// Main
int main(int argc, char* argv[]) {

    // mainTrain();
    mainSolve();

    return 0;
}