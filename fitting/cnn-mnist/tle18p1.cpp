// Solve the "Hello, World" problem - https://dmoj.ca/problem/tle18p1
// Using a CNN developed in C++ from scratch.

#pragma GCC optimize "Ofast"
#include <bits/stdc++.h>

using std::abs;
using std::max;
using std::min;


// Uncomment this line when debugging analytical gradient
// #define float double


// Quick-and-dirty PRNG from Numerical Recipes
uint32_t _IDUM = 1u;
uint32_t randu() { return _IDUM = _IDUM * 1664525u + 1013904223u; }
float randf() { return (float)randu() / 4294967296.0f; }


// Load the MNIST dataset
int loadMnist(const char* x_path, const char* y_path, float*& x, float*& y) {

    // read x - between 0 and 1
    FILE* fp = fopen(x_path, "rb");
    fseek(fp, 0, SEEK_END);
    size_t fsize = ftell(fp);
    fseek(fp, 0, SEEK_SET);
    uint8_t* raw = new uint8_t[fsize];
    fread(raw, fsize, 1, fp);
    fclose(fp);
    int ndata = (int)(fsize / 784);
    x = new float[ndata * 784];
    for (int i = 0; i < ndata * 784; i++)
        x[i] = (float)raw[i] / 255.0f;

    // read y - vectors of 10, 0 false 1 true
    fp = fopen(y_path, "rb");
    fread(raw, ndata, 1, fp);
    fclose(fp);
    y = new float[ndata * 10];
    for (int i = 0; i < ndata; i++)
        for (int j = 0; j < 10; j++)
            y[10 * i + j] = (raw[i] == j ? 1.0f : 0.0f);
    delete raw;

    return ndata;
}

// Plot a digit
void plotDigit(const float* x) {
    for (int i = 0; i < 28; i++) {
        for (int j = 0; j < 28; j++) {
            fprintf(stderr, "%s", x[i * 28 + j] > 0.5f ? "##" : ". ");
        }
        fprintf(stderr, "\n");
    }
}


// Checks if the analytical gradient of a function is correct, returns a representation of error
float checkGrad(
    int ndim,
    std::function<void(const float* x, float* val, float* grad)> fun,
    float* x,
    float eps = 0.004f,
    bool verbose = false
) {
    float val;
    float* grad0 = new float[ndim];  // reference gradient
    float* grad = new float[ndim];  // temp evaluation gradient
    for (int i = 0; i < ndim; i++)
        grad0[i] = 2.0f * randf() - 1.0f;  // random
    fun(x, &val, grad0);  // evaluate reference gradient
    float maxerr = 0.0f;
    for (int i = 0; i < ndim; i++) {  // for each dimension
        float val1, val0;
        float x0 = x[i];  // forward difference
        x[i] = x0 + eps;
        fun(x, &val1, grad);
        x[i] = x0 - eps;  // backward difference
        fun(x, &val0, grad);
        x[i] = x0;
        float dvdx = (val1 - val0) / (2.0f * eps);  // central difference
        float err = abs(grad0[i]) < 1.0f ?  // adaptive error
            dvdx - grad0[i] : dvdx / grad0[i] - 1.0f;
        maxerr = max(maxerr, err);
        if (verbose) {
            fprintf(stderr, "%.4lf ", abs(err));
            // fprintf(stderr, "%.4lf ", fmod(100.0f * abs(err), 1.0f));
            // fprintf(stderr, "%d ", max(min((int)round(-log10(abs(err))), 9), 0));
        }
    }
    delete grad; delete grad0;
    return maxerr;
}



// CNN architecture

#define C1I 28  /* layer 1 input dimension */
#define C1S 5  /* conv layer 1 size */
#define C1N 12  /* conv layer 1 count */
#define C1O ((C1I)-(C1S)+1)  /* conv layer 1 output dimension */
/* relu */
/* maxpool */
#define C2I ((C1O)>>1)  /* conv layer 2 input dimension */
#define C2S 3  /* conv layer 2 size */
#define C2N 6  /* conv layer 2 count */
#define C2O ((C2I)-(C2S)+1)  /* conv layer 2 output dimension */
/* relu */
/* maxpool */
#define C2OA ((C2O)>>1)  /* conv layer 2 output dimension activated */
/* flatten */
#define L3I ((C2N)*(C2OA)*(C2OA))  /* layer 3 input size */
#define L3N 16  /* layer 3 output size */
/* relu */
#define L4N 16  /* layer 4 output size */
/* relu */
#define L5N 16  /* layer 5 output size */
/* relu */
#define L6N 10  /* output layer */
/* softmax */
/* cross-entropy loss */


#define X_DIM ((C1I)*(C1I))
#define Y_DIM L6N


// Number of weights in each layer
#define W1N ((C1S)*(C1S)*(C1N))
#define W2N ((C2S)*(C2S)*(C2N)*(C1N))
#define W3N (((L3I)+1)*(L3N))
#define W4N (((L3N)+1)*(L4N))
#define W5N (((L4N)+1)*(L5N))
#define W6N (((L5N)+1)*(L6N))
#define WN (W1N+W2N+W3N+W4N+W5N+W6N)

// A struct of floats for easy access
struct FloatWN {
    float w1[C1N][C1S * C1S];
    float w2[C2N][C1N][C2S * C2S];
    float w3[L3N][L3I + 1];
    float w4[L4N][L3N + 1];
    float w5[L5N][L4N + 1];
    float w6[L6N][L5N + 1];
};

// COO matrix for convolutional layers
int MC1[C1O * C1O][C1S * C1S];  // layer 1 convolution indices
int MP1[C2I * C2I][2 * 2];  // layer 1 pooling indices
int MC2[C2O * C2O][C2S * C2S];  // layer 2 convolution indices
int MP2[C2OA * C2OA][2 * 2];  // layer 2 pooling indices
void initMatrices() {
    // MC1
    for (int i = 0; i < C1O; i++) for (int j = 0; j < C1O; j++)
        for (int di = 0; di < C1S; di++) for (int dj = 0; dj < C1S; dj++) {
            int b = (i + di) * C1I + (j + dj);
            MC1[i * C1O + j][di * C1S + dj] = b;
        }
    // MP1
    for (int i = 0; i < C2I; i++) for (int j = 0; j < C2I; j++)
        for (int di = 0; di < 2; di++)  for (int dj = 0; dj < 2; dj++) {
            int b = (i * 2 + di) * C1O + (j * 2 + dj);
            MP1[i * C2I + j][di * 2 + dj] = b;
        }
    // MC2
    for (int i = 0; i < C2O; i++) for (int j = 0; j < C2O; j++)
        for (int di = 0; di < C2S; di++) for (int dj = 0; dj < C2S; dj++) {
            int b = (i + di) * C2I + (j + dj);
            MC2[i * C2O + j][di * C2S + di] = b;
        }
    // MP2
    for (int i = 0; i < C2OA; i++) for (int j = 0; j < C2OA; j++)
        for (int di = 0; di < 2; di++) for (int dj = 0; dj < 2; dj++) {
            int b = (i * 2 + di) * C2O + (j * 2 + dj);
            MP2[i * C2OA + j][di * 2 + dj] = b;
        }
}


// Activation function
void activate(float x, float* y, float* dydx) {
    // *y = max(x, 0.0f); *dydx = x > 0.0f ? 1.0f : 0.0f;  // standard ReLU
    *y = max(x, 0.1f * x); *dydx = x > 0.0f ? 1.0f : 0.1f;  // much better?!
    // *y = sin(x); *dydx = cos(x);  // easier to see if something goes wrong
}


// Put weights here

float WEIGHTS[WN] = { -1.516,-1.439,0.703,0.399,0.468,-0.041,-1.540,-0.153,0.585,-1.005,0.104,0.009,-1.515,-0.956,-1.808,-0.039,0.716,-0.575,-0.395,0.021,-0.305,0.374,-0.156,-0.078,-0.139,-0.264,-0.199,-0.076,-0.406,-0.106,-0.542,-1.338,-0.036,-0.508,-0.008,-1.103,-0.388,0.189,0.002,0.852,-0.308,0.776,0.818,1.292,0.091,-0.078,0.077,-0.172,-0.847,-0.815,-1.452,-0.559,0.216,0.066,-0.419,-2.688,-1.597,-0.354,-0.155,-0.453,-1.879,-1.612,-0.635,-0.051,-0.062,-1.556,-0.858,-0.092,0.187,-0.104,-1.748,-0.636,-0.193,-0.147,-0.362,0.167,-0.536,-0.126,0.274,-1.052,0.407,-0.539,0.116,-0.099,-0.383,0.378,-0.445,-0.241,-0.245,-0.238,-0.173,-0.080,-0.165,0.212,-0.054,-0.362,0.056,-0.034,-0.192,-0.507,0.459,0.246,-0.251,-0.738,-0.593,0.373,0.552,-0.946,-1.023,-0.479,0.642,0.543,-2.065,-0.117,0.271,0.125,-0.933,-1.719,0.222,0.184,-1.258,-0.917,-0.090,0.071,-0.349,-0.725,-0.655,-0.877,-0.144,-0.149,-0.801,-1.183,0.053,0.139,0.545,-0.424,-0.724,-0.327,0.089,0.788,-0.196,-0.000,-0.591,-0.171,0.539,-1.062,-0.421,-0.222,0.178,-0.163,-0.030,0.461,0.190,-0.255,0.246,-1.247,-0.603,-0.309,0.205,0.058,-0.973,-1.114,-1.259,-0.963,-0.248,-1.038,0.511,-0.202,-0.481,-1.039,0.597,0.691,0.713,0.381,-0.064,0.009,-1.175,-1.075,0.254,0.530,0.329,-0.600,-0.698,0.042,-0.515,1.234,0.584,-0.543,0.336,-1.080,0.287,-0.003,-0.678,-0.005,-0.300,0.295,-0.041,-0.455,-0.068,-1.095,-0.253,-0.202,-0.527,-1.036,-0.330,-0.562,-0.866,-2.076,-2.045,-0.650,0.002,-0.169,-0.661,-0.352,-0.164,-0.105,-0.238,-0.194,0.030,0.054,-0.245,-0.206,-0.132,-0.137,-0.599,-0.922,-0.286,0.002,-0.158,0.183,-1.218,-0.974,0.083,-0.447,-0.267,-0.540,-0.738,-0.687,-0.579,-0.569,-1.003,0.351,0.325,0.145,-0.857,-1.120,-0.185,0.322,0.303,-0.586,-0.139,0.254,0.013,-0.223,-0.064,-0.392,0.046,-0.288,-0.287,-0.729,-0.101,-0.435,-0.633,-0.787,-0.388,-1.027,-0.942,-0.843,-0.710,-0.474,-0.670,-0.394,-0.302,-0.372,-0.137,0.356,0.849,0.173,-0.400,-0.392,-0.008,0.172,0.276,0.530,0.400,-0.034,0.465,0.496,0.329,0.426,-0.285,0.368,0.559,0.659,-0.198,-0.836,-1.796,-2.580,-1.871,-1.675,-0.461,-0.575,-0.706,-0.653,-0.311,-0.724,-0.614,-0.690,0.153,0.235,0.146,0.115,0.179,0.256,0.098,0.066,0.051,-0.880,-0.462,0.044,-0.022,-0.012,0.243,-0.089,0.066,0.069,1.875,0.051,0.026,-0.111,-0.102,-0.477,0.010,-0.156,-0.108,-0.615,-0.671,0.289,0.254,0.238,-0.253,0.233,0.184,0.254,-0.772,0.098,-0.159,-0.028,-0.076,0.433,-0.118,-0.046,-0.085,-0.734,0.203,-0.137,-0.174,-0.094,0.318,-0.172,-0.178,-0.259,0.341,0.618,-0.578,-0.647,-0.684,-0.419,-0.625,-0.497,-0.505,-0.781,-0.047,-0.484,-0.453,-0.505,0.253,-0.421,-0.548,-0.414,0.664,-0.587,0.302,0.265,0.313,0.394,0.264,0.397,0.350,0.967,0.271,-0.271,-0.264,-0.337,0.695,-0.238,-0.293,-0.178,0.945,0.187,-0.299,-0.125,-0.270,0.741,-0.177,-0.309,-0.292,-0.421,0.111,-0.230,-0.229,-0.193,1.017,-0.115,-0.303,-0.119,0.046,-0.315,-0.125,-0.133,-0.063,-0.177,0.008,-0.042,-0.165,0.863,1.606,0.147,-0.029,0.138,2.785,0.102,0.034,0.032,0.720,-0.090,-1.193,-1.070,-1.064,0.398,-1.119,-1.041,-1.219,-0.121,0.082,-0.250,-0.306,-0.378,0.489,-0.369,-0.379,-0.308,-0.113,0.269,0.068,-0.033,0.033,-0.985,0.060,0.081,0.003,0.606,0.030,-0.161,-0.327,-0.274,-0.355,-0.225,-0.261,-0.196,-0.289,-0.300,-0.344,-0.461,-0.324,-0.747,-0.406,-0.418,-0.401,-1.494,-0.056,0.707,0.765,0.640,2.647,0.721,0.752,0.733,0.671,0.844,-0.069,0.067,-0.039,0.006,-0.019,0.084,0.071,0.537,1.781,-0.348,-0.342,-0.347,2.412,-0.341,-0.289,-0.289,-0.092,-0.065,-0.146,-0.122,-0.114,-0.508,-0.048,-0.155,-0.223,-0.534,-0.912,-0.127,-0.022,-0.032,-0.299,-0.052,0.001,-0.104,-0.587,-0.043,-0.345,-0.514,-0.399,-0.299,-0.530,-0.475,-0.337,-0.044,0.141,-0.081,-0.033,-0.079,-0.031,0.009,0.050,0.008,0.266,0.393,0.043,0.168,0.213,0.553,0.032,0.176,0.156,-0.656,0.674,0.597,0.525,0.473,0.821,0.633,0.617,0.644,-0.050,-0.869,0.411,0.281,0.305,-0.533,0.235,0.280,0.344,0.260,0.080,-0.563,-0.554,-0.641,-1.392,-0.700,-0.594,-0.594,0.168,-0.018,0.204,0.173,0.192,-0.094,0.235,0.332,0.318,-0.509,-0.364,-0.844,-0.771,-0.790,1.176,-0.923,-0.809,-0.843,1.184,-0.235,0.322,0.173,0.152,0.005,0.165,0.183,0.232,-0.136,0.516,-0.867,-1.029,-1.013,1.006,-0.991,-0.948,-0.858,1.452,-0.030,-0.264,-0.285,-0.290,0.021,-0.299,-0.327,-0.399,-0.514,-1.417,0.556,0.535,0.581,-0.074,0.572,0.530,0.495,-0.418,0.101,-0.458,-0.460,-0.328,0.072,-0.370,-0.267,-0.266,-0.233,0.119,0.249,0.294,0.243,0.781,0.228,0.118,0.129,1.518,0.176,0.047,0.121,0.113,-0.217,0.056,0.084,0.030,0.024,0.737,-0.158,-0.219,-0.346,-0.306,-0.245,-0.220,-0.325,-0.557,0.352,-0.063,-0.156,-0.221,0.488,-0.161,-0.109,-0.228,0.486,-0.021,0.017,0.013,0.174,-0.477,0.110,0.129,0.158,-0.299,-0.771,-0.336,-0.402,-0.387,-0.054,-0.408,-0.372,-0.347,-0.065,-0.723,-0.089,-0.186,-0.233,-0.347,-0.257,-0.087,-0.147,-0.017,1.107,0.380,0.412,0.347,0.205,0.338,0.331,0.399,0.788,-0.319,-0.484,-0.514,-0.408,0.010,-0.494,-0.452,-0.406,0.368,-2.007,0.080,0.155,0.201,-2.272,0.106,0.151,0.165,-0.951,0.295,0.586,0.526,0.511,1.330,0.650,0.585,0.515,0.115,-0.125,-0.636,-0.454,-0.651,-0.073,-0.620,-0.617,-0.640,-0.515,1.174,0.791,0.723,0.827,1.247,0.712,0.775,0.638,0.557,-0.426,0.279,0.375,0.268,0.212,0.219,0.227,0.233,-0.492,-0.676,-0.308,-0.306,-0.268,0.394,-0.302,-0.367,-0.392,0.046,0.382,-0.362,-0.232,-0.299,0.659,-0.251,-0.306,-0.389,-0.767,-1.012,0.067,0.024,0.015,-0.052,0.026,0.027,0.159,1.505,-0.407,1.055,1.168,1.052,-0.769,1.121,1.213,1.136,-0.026,-0.321,0.872,0.701,0.853,0.667,0.884,0.810,0.730,0.914,0.294,0.633,0.644,0.800,0.353,0.822,0.744,0.809,0.247,0.584,0.828,0.851,0.817,1.260,0.904,0.859,0.833,0.473,-0.426,1.340,1.287,1.157,0.067,1.228,1.239,1.177,-0.720,-0.376,-0.317,-0.285,-0.471,0.459,-0.396,-0.418,-0.327,0.503,-0.952,-0.710,-0.708,-0.622,-1.584,-0.637,-0.667,-0.727,-1.581,0.206,1.036,0.973,0.976,0.531,1.076,0.985,1.051,0.082,0.042,-0.016,0.051,0.016,0.170,0.017,0.030,0.010,0.027,0.196,-0.097,-0.107,-0.050,-0.389,-0.169,-0.159,-0.152,-0.530,-0.622,0.338,0.302,0.372,-0.116,0.323,0.242,0.336,-0.940,0.314,-0.369,-0.453,-0.419,-0.106,-0.409,-0.469,-0.493,-0.407,-0.904,0.747,0.811,0.683,0.495,0.675,0.769,0.710,0.527,0.697,0.882,0.881,0.703,0.750,0.698,0.805,0.787,0.794,0.083,0.847,0.800,0.840,0.472,0.740,0.832,0.797,0.196,0.208,0.502,0.467,0.337,0.383,0.431,0.495,0.426,0.042,0.640,1.168,1.015,1.029,-1.323,1.054,1.060,1.116,-0.453,0.232,0.006,0.057,-0.566,-0.118,0.325,0.797,0.391,-0.304,0.302,0.740,-0.512,0.222,0.284,0.721,0.159,-0.221,-0.564,1.491,1.199,-1.366,-0.157,-0.361,0.275,-0.355,-0.541,-0.940,0.555,1.835,0.087,-0.885,-0.779,0.531,1.330,1.767,0.672,0.675,-0.309,-1.481,-1.827,0.416,0.033,-0.273,-0.129,2.153,-1.026,-0.874,-0.051,1.655,1.934,-0.200,-1.042,0.293,1.117,0.446,-1.766,-0.360,0.343,0.578,0.079,-0.063,1.635,0.343,-0.322,-0.445,1.467,0.729,0.749,-0.055,0.253,0.621,2.099,-0.741,-0.760,-1.286,0.681,-0.579,0.195,-0.046,-1.962,0.379,-0.393,1.547,-0.349,-0.252,-0.601,-0.192,1.839,0.415,-0.001,0.738,0.978,0.034,-0.655,0.274,0.420,1.509,1.067,0.828,1.811,0.298,0.268,0.467,-0.146,0.395,0.049,0.449,0.283,0.542,1.209,-0.208,0.243,0.541,-0.172,0.106,-0.140,-1.025,-0.189,-0.819,0.377,0.520,0.274,0.945,1.582,-0.496,1.677,0.895,0.018,-0.496,-0.564,-0.176,0.296,-0.289,-1.104,-0.584,-0.286,1.368,-0.441,-2.152,-0.475,-1.573,-0.686,-0.775,-2.056,-0.323,-0.728,-0.060,1.766,3.212,0.303,0.129,-1.478,-2.323,-0.141,0.262,-0.896,-2.453,-3.370,-1.195,0.040,-0.989,-2.181,-2.272,0.905,0.497,-2.837,-1.180,-1.936,-0.070,-0.919,-0.909,-0.734,0.074,-0.003,-0.558,-1.816,-1.236,-0.428,1.200,0.985,-0.107,-0.188,0.754,2.929,1.169,0.468,-1.541,0.146,-0.020,-0.103,-1.042,-1.830,-0.483,-0.589,-0.599,-0.893,-1.076,0.400,-0.354,0.500,0.827,-0.002,1.956,1.313,0.960,1.002,1.762,1.303,1.131,0.488,-0.381,0.075,-0.148,-0.565,-1.459,-0.002,-1.512,-0.509,-0.668,0.660,0.874,-1.163,-2.097,-0.105,0.149,0.015,0.258,-0.034,0.654,0.372,1.040,0.199,-0.376,-0.600,-1.848,2.560,0.760,0.235,-0.664,-1.724,-0.267,0.343,0.774,0.136,-1.356,-1.429,0.181,-1.726,-1.407,-0.607,0.965,0.061,-1.253,0.083,1.451,-0.061,-0.857,-0.898,0.614,2.108,0.511,-0.546,0.368,0.787,1.052,-0.722,0.528,-0.578,-0.877,-0.657,-0.172,0.045,0.296,-0.393,-0.044,0.576,1.432,1.529,-2.202,-0.998,0.262,0.511,-1.256,-0.772,0.597,0.864,0.436,-0.066,0.966,1.596,0.465,-0.197,-0.095,0.550,-1.257,-1.582,-0.503,1.149,0.220,0.298,-1.071,-1.201,-0.027,-0.119,0.747,0.422,-1.750,0.543,0.738,0.336,-0.657,-1.820,0.806,-0.023,-0.890,-2.179,-0.992,-0.156,-0.304,-1.439,-0.823,0.511,0.066,-0.599,0.834,0.971,-0.808,0.166,0.148,-0.959,-1.152,-0.981,0.328,0.174,0.593,0.261,0.164,1.163,-0.074,0.417,0.132,0.085,-0.164,0.456,-0.173,-1.127,1.706,-1.779,-0.704,-0.145,-1.019,-0.940,0.924,0.652,0.251,-0.390,-1.418,-0.770,-1.101,-0.475,-0.815,-0.156,-0.531,-0.823,-0.277,-0.279,0.377,0.223,-0.373,0.112,-0.168,-1.017,1.728,1.116,-1.784,-1.872,-0.596,0.235,0.998,-1.235,-0.255,0.578,-0.401,-0.136,0.429,1.409,0.044,0.026,-1.217,0.191,1.185,-0.924,-0.379,-1.420,0.654,-0.090,0.685,-0.695,-0.641,-0.531,0.104,-0.768,0.980,0.351,0.267,0.114,-0.453,0.542,1.467,1.656,-0.238,-0.447,0.846,0.662,-0.055,-0.283,0.177,-1.183,-1.008,-0.266,0.095,0.829,-0.866,-0.682,1.806,0.452,0.543,-0.020,-0.480,0.195,0.755,1.059,-1.244,0.420,0.610,-0.644,-0.648,-0.725,0.545,1.517,-0.900,-2.541,-0.923,-0.682,-0.271,-0.550,-0.431,1.271,0.752,0.340,-0.378,0.731,-1.066,0.341,-0.556,-0.949,-0.132,-0.043,-0.328,-1.169,-0.881,0.057,0.213,1.037,1.148,-1.516,0.020,0.151,1.063,0.503,-0.117,1.205,0.463,-0.654,-1.066,-0.432,0.610,0.644,-0.903,-0.289,0.349,1.390,0.999,0.235,-0.222,-0.797,0.727,0.551,0.090,-1.867,-0.160,-1.222,0.009,-0.032,-0.084,-0.860,-0.955,0.663,0.996,-0.909,-0.110,0.120,-0.305,-0.449,-0.415,-0.318,-0.416,0.436,0.565,0.447,0.060,-0.013,-0.100,-0.535,0.819,-0.560,-0.244,-0.700,0.419,-1.293,0.011,0.717,0.265,1.072,1.314,0.778,-0.620,-0.858,-0.017,1.248,1.792,2.314,0.495,0.104,0.699,0.903,1.573,-0.507,-0.062,-0.718,0.192,0.282,0.630,-1.698,-1.156,0.041,0.375,1.295,0.196,1.506,1.152,1.084,-0.275,-0.342,0.655,0.709,0.553,-0.555,-0.869,0.199,0.641,-0.119,0.775,-0.929,-0.456,1.180,0.036,1.636,-0.427,-0.867,-0.452,-1.553,0.865,0.311,1.145,2.361,1.532,1.104,1.795,2.635,0.409,0.194,-1.531,1.401,-0.551,-1.040,-0.464,-0.862,-1.891,-0.829,-1.131,-1.850,-0.752,-0.216,0.651,0.040,-0.396,-0.418,0.170,0.028,-0.649,0.974,-0.225,-0.307,-2.526,-1.329,-1.335,-0.583,-0.666,-1.671,-0.751,-0.820,-0.476,-0.355,-0.701,-0.986,-0.969,-0.805,0.225,-0.869,-0.362,-0.731,-2.158,-0.522,0.164,0.139,0.409,-1.054,-0.354,-0.944,-0.584,0.078,0.215,-0.405,-0.158,0.834,1.024,0.378,-0.266,0.041,-0.010,-2.747,2.593,-0.550,-0.607,-0.502,-0.793,0.039,1.403,-0.570,0.245,-0.430,1.890,1.890,-0.270,-0.722,-0.191,0.752,0.128,-0.062,-0.451,-0.184,-0.073,0.451,-0.194,0.447,0.672,-0.542,-1.261,-0.638,0.648,0.943,-1.313,-0.957,0.402,1.380,-0.327,-0.665,-2.287,-1.036,0.065,1.424,1.754,-0.784,-1.478,-0.022,0.777,-0.105,0.312,-0.052,0.602,-0.045,0.601,-0.267,-0.293,0.814,0.715,0.907,1.054,0.021,1.139,0.931,1.068,1.220,-0.106,-2.090,-0.651,-1.028,1.315,-0.147,0.355,-1.043,0.516,0.523,-0.792,-0.557,-1.629,0.558,-0.283,0.141,0.931,0.407,1.392,-1.476,0.317,0.747,0.650,3.525,1.080,1.184,-0.285,2.027,-0.360,0.295,-1.568,-1.312,0.594,-2.238,-0.583,-1.329,-0.717,-1.216,-1.903,-0.798,-0.273,-0.323,-0.489,-0.555,0.374,-1.342,-0.179,0.300,-1.039,1.279,0.018,-0.464,-0.474,0.156,0.294,-0.448,-0.872,-0.097,0.406,0.706,-1.056,-2.147,-0.901,-0.782,-0.448,-0.951,-0.443,-0.596,-0.384,0.571,1.655,0.154,0.546,1.187,1.622,1.325,0.119,1.089,1.144,-1.158,-1.063,1.908,0.710,1.743,1.273,1.286,0.316,1.019,1.394,1.106,-0.339,-2.295,-1.412,-0.584,-0.143,-0.292,-1.410,0.067,0.143,-0.387,-0.573,1.145,2.341,0.939,0.119,-0.163,1.001,0.839,0.817,-0.110,-0.081,-0.050,0.378,0.306,0.470,0.049,0.304,-0.590,0.042,-0.157,-0.047,0.523,0.401,-0.398,0.088,0.157,0.073,0.341,1.321,0.864,-0.732,0.174,-0.561,0.093,0.440,0.586,1.127,-0.393,0.300,0.008,-0.756,1.267,0.724,-1.010,-0.410,-0.276,-0.265,-0.170,-0.978,-1.445,-0.284,0.833,-0.690,-0.027,0.035,-0.552,-1.013,-0.206,1.004,0.471,0.642,-2.024,-0.433,0.733,0.089,-0.256,-2.173,-0.837,0.601,-0.474,-0.129,-0.594,0.347,1.228,1.007,-0.253,1.032,0.712,0.175,0.452,0.815,-0.319,0.753,0.149,0.717,0.632,-1.032,-0.524,0.369,0.133,-0.782,0.039,0.288,-0.305,-0.298,-0.029,-0.087,1.051,0.787,0.227,-0.108,-0.308,-1.550,-0.420,0.634,-0.242,0.822,-1.857,0.487,0.697,0.925,-0.023,-0.575,0.540,-1.286,-0.936,-0.193,0.766,0.252,-0.798,-1.151,-1.457,-0.326,-0.435,-0.891,-0.930,0.678,1.110,1.205,-0.067,-1.737,-1.639,-0.655,0.160,-1.864,-1.360,-0.626,-2.297,1.600,-0.072,0.175,-0.211,0.338,-0.256,0.048,0.655,1.254,1.006,0.196,1.225,0.800,-1.983,0.915,-0.412,-0.911,-0.625,-2.270,-0.610,-0.338,-0.687,-0.349,-0.983,-0.103,0.098,-0.509,0.281,0.196,-0.466,-1.553,0.582,0.676,0.276,0.553,0.609,0.845,-1.145,-1.085,-0.912,-1.340,-2.654,-1.996,1.374,1.332,0.100,-0.961,-0.739,0.645,1.433,-0.574,0.105,-0.029,-1.309,-0.381,0.513,0.938,0.435,1.533,-0.024,0.914,0.767,1.371,-1.298,-0.700,0.353,0.390,0.087,0.385,-1.415,-0.872,-0.709,-0.756,-1.154,-0.640,0.260,-0.429,-0.951,-0.673,-0.427,0.068,-0.052,0.385,0.319,-0.643,-0.338,0.546,0.668,-0.746,-1.669,-2.384,0.019,0.368,0.752,0.489,-0.712,0.640,1.173,0.006,1.569,0.099,0.031,-0.334,-1.143,-0.359,-0.000,-1.620,-2.381,0.220,0.469,0.621,0.436,-0.604,0.255,-0.836,0.084,-0.100,-0.633,-0.307,-0.480,-0.877,-0.516,-0.337,-0.649,-1.140,-0.727,-0.609,-0.439,-0.551,-1.516,-1.001,-0.757,-0.201,-0.068,-0.744,-0.491,-0.237,0.414,-0.482,-0.738,0.039,-0.039,0.413,-0.724,-1.229,0.856,0.812,0.335,0.282,0.886,0.250,0.237,-0.175,0.075,0.268,-0.241,0.287,0.736,0.338,0.652,-0.751,-0.119,0.829,0.385,0.032,0.176,0.237,0.526,-0.886,0.122,-0.067,-0.095,0.106,0.153,0.614,0.192,-0.059,-0.060,0.020,-0.105,-0.207,-0.691,-0.245,-0.204,0.201,-0.035,0.006,-0.100,-0.335,-0.465,-1.130,-0.561,-0.502,-0.117,0.011,-0.662,-0.349,-0.275,0.774,0.110,0.312,-0.638,-0.209,0.048,-0.424,0.365,-0.632,-0.353,-0.182,0.138,-0.152,0.066,0.056,-0.522,-0.407,-0.242,-0.100,0.425,0.578,0.011,1.475,1.472,0.804,1.184,0.448,-0.371,-0.084,0.360,-0.089,-0.117,-0.683,-0.011,0.565,0.099,0.352,-0.331,0.635,1.564,0.516,-0.658,-0.473,0.282,0.108,-0.124,1.340,-0.136,-0.289,-0.900,-0.163,0.087,0.569,-0.530,-0.063,-0.068,0.102,0.199,0.050,0.607,-0.127,-0.501,-0.110,-0.475,0.201,0.142,1.045,-0.918,-0.552,-0.142,0.138,-0.537,-1.102,-0.589,-0.925,-0.021,0.196,-1.117,-0.432,0.006,0.205,0.060,-0.757,-0.191,0.446,0.683,1.488,0.466,-0.323,0.414,-0.046,-0.028,-0.941,-0.576,0.191,-0.264,-0.282,-0.188,1.002,0.521,1.047,0.816,0.390,0.285,-0.758,-0.183,0.025,-1.188,-1.106,-0.631,0.636,0.790,-1.033,-1.165,0.508,1.667,1.018,0.080,0.003,0.762,0.294,-0.783,-1.027,-0.794,-0.969,-0.129,-1.424,-0.984,1.385,0.038,-0.247,-0.226,-0.352,1.333,-1.029,0.349,1.235,0.795,-0.124,-0.031,-0.940,-1.969,-0.917,-0.086,-0.322,-0.138,-1.075,0.436,-0.805,0.342,0.141,-0.136,0.165,-0.021,-0.214,-0.799,-0.591,0.094,0.269,-0.781,-1.287,-1.400,-2.062,-1.853,-1.429,-0.916,-2.108,-0.468,0.890,1.354,0.687,0.773,0.331,0.482,1.183,0.584,0.979,0.698,-0.946,-0.406,0.435,-0.397,-0.927,-0.704,0.534,0.988,1.040,-0.528,0.190,0.135,-0.159,0.735,-0.681,0.645,1.271,0.365,-0.339,-0.023,-0.430,0.389,1.316,0.415,0.407,0.761,-0.797,0.062,-0.605,-1.937,-1.090,-0.122,-0.341,-0.968,-1.291,1.627,1.447,2.093,2.430,0.751,-1.062,0.611,0.419,0.161,-0.123,0.691,0.422,0.729,0.026,-0.845,-1.544,0.171,-0.043,1.581,-0.087,-1.035,0.342,-0.096,-0.888,0.098,0.191,-2.848,-1.693,-0.528,-1.031,-1.444,-0.660,-0.997,-0.285,0.999,0.361,0.187,0.348,0.250,-0.187,0.135,1.121,1.053,-0.458,-0.439,-0.421,-0.435,-0.175,-0.493,-1.134,0.390,1.899,0.640,0.955,-0.894,0.717,0.942,-1.265,-0.111,0.307,-1.493,-1.746,-1.076,-0.812,0.512,0.383,0.238,3.052,-0.647,0.272,0.091,2.029,1.741,-0.867,-0.229,0.560,0.386,1.293,-0.021,-0.065,-0.234,0.200,-1.455,-2.452,-1.002,-0.045,-0.281,-0.307,-0.386,-0.883,-1.168,-0.942,0.592,1.540,0.100,1.022,-0.545,-0.033,0.082,0.385,-0.244,0.157,0.510,0.785,-0.211,0.986,2.008,0.544,0.807,1.504,0.846,0.549,0.468,-0.338,1.043,0.885,0.647,-0.306,0.272,-0.245,0.176,1.068,-0.455,-0.776,-2.650,-2.093,-0.668,0.703,0.034,-1.897,-1.143,1.505,1.158,-0.364,-0.162,0.350,0.883,1.094,-1.422,-0.334,0.133,-0.985,0.250,-0.348,0.105,-0.011,-0.947,-1.457,-0.058,0.167,-1.273,-0.745,-0.503,1.202,0.363,-0.507,-0.658,-2.327,-1.144,-0.551,-0.892,-0.557,-1.025,-0.249,0.568,0.410,-1.721,-1.937,-0.356,0.159,0.259,0.002,-0.408,-2.019,-0.919,0.156,0.111,-0.078,0.508,0.137,-0.676,-0.204,0.040,0.523,0.789,0.105,-0.043,0.384,0.979,-2.456,-2.148,-0.572,-0.323,-0.575,1.558,0.170,-2.444,-2.747,-0.367,1.708,0.897,-0.576,-0.875,-0.002,0.682,0.329,0.442,0.871,-2.826,-0.509,-2.222,-0.888,-0.708,0.139,-0.600,-1.403,-0.446,0.366,0.813,1.200,0.257,0.084,1.568,0.859,-1.579,0.324,0.329,-1.179,0.170,0.226,0.089,-0.051,-0.686,-0.312,-0.011,0.273,-1.229,0.188,0.424,-0.096,1.163,0.623,0.855,0.350,0.361,0.774,-0.220,0.616,-0.693,1.017,1.095,0.342,0.166,-0.089,0.224,0.289,-0.379,1.720,0.678,1.420,-0.443,0.168,-0.870,-1.342,-0.894,-0.512,-0.517,-2.412,-1.052,2.379,-0.691,-2.805,-0.662,0.663,1.482,0.193,-1.486,-0.313,1.188,-0.174,-0.547,-0.094,0.576,0.806,0.969,0.762,0.948,1.145,2.010,0.240,0.255,0.428,0.588,1.969,0.006,-0.648,-0.923,0.348,-0.375,-1.723,-2.420,-1.617,-0.655,-0.642,0.861,-0.218,0.056,-0.145,1.325,0.721,-0.088,-2.166,-3.950,1.100,0.276,0.251,-1.377,-1.515,-0.111,-0.304,-0.139,-0.185,0.603,-1.157,0.227,-0.413,0.328,0.783,0.611,0.072,0.019,0.558,0.925,-0.336,-0.210,-2.070,-2.410,-0.698,0.402,0.539,-1.211,-1.117,1.014,1.421,0.752,-0.635,0.328,0.926,0.683,0.585,0.236,-0.197,1.025,-0.071,0.592,0.327,-0.484,0.192,0.095,-1.504,-1.463,-1.130,-1.198,1.156,0.701,1.179,0.684,0.569,1.232,0.496,0.999,0.531,0.203,-0.132,-1.254,-0.086,0.074,-0.543,0.167,-0.066,-0.280,0.195,0.468,-1.242,-1.251,-0.203,-1.196,-0.344,-2.444,-2.120,0.345,0.962,0.648,-0.715,-0.200,-0.188,-0.289,1.050,0.986,-0.678,-0.727,-0.584,-0.717,-0.230,0.294,-0.330,-1.951,1.025,-0.819,0.917,0.411,-0.288,-0.212,0.447,1.480,-0.293,0.256,1.012,1.068,0.558,-0.228,-0.561,0.964,1.255,-0.387,-0.789,-0.528,1.092,-0.264,-0.391,-1.096,0.247,0.927,0.446,0.917,1.440,0.437,0.431,-1.846,-1.474,-1.049,-0.101,-0.909,-1.306,-1.110,-0.446,0.025,0.375,-0.831,-0.602,0.130,0.778,-0.247,-0.527,-0.271,1.033,1.078,0.217,-0.122,-2.098,-1.321,1.449,0.980,1.468,-1.019,0.167,1.244,1.612,1.986,0.187,0.580,0.116,0.299,1.053,0.445,-0.088,-0.400,-0.361,-1.062,0.536,-0.380,-1.204,-0.906,-0.375,-0.713,-0.798,0.288,1.895,0.168,-0.114,0.480,-0.510,-1.784,-1.299,-1.128,-1.827,1.162,0.350,-0.763,0.336,-0.076,0.237,0.376,-1.467,1.066,0.542,-0.572,1.311,-1.297,-1.824,1.521,0.390,1.531,1.125,-0.560,-0.495,-0.528,-0.244,0.636,-0.015,0.316,0.810,-2.473,-1.658,0.061,1.357,0.873,-0.514,-0.717,0.696,1.247,-0.077,0.125,0.552,0.627,0.457,0.297,-0.245,-2.239,0.818,0.455,-1.977,-1.475,-1.177,0.031,-0.308,-0.266,-0.841,0.554,-0.875,-0.771,0.665,-0.209,-0.402,-0.313,-1.171,-1.622,-0.591,-0.722,0.040,0.302,0.580,0.074,0.790,0.479,0.562,1.871,0.503,0.912,1.520,0.157,2.055,1.077,-0.281,0.536,-0.392,0.226,0.627,-1.702,-0.306,-0.158,-0.535,0.120,-0.814,-0.573,0.737,-2.657,-2.306,-2.049,-0.380,1.010,1.033,0.872,0.330,-0.075,0.933,2.114,1.023,0.078,-0.481,0.260,0.131,-0.227,-0.762,-0.593,0.057,0.053,-1.702,-1.389,-1.070,-0.649,-1.018,-1.060,-0.791,-0.226,-0.806,0.151,-0.234,-1.541,-2.488,0.778,-0.682,-0.395,-0.460,-1.123,-0.670,-2.046,-2.363,-0.291,0.550,-0.943,0.198,1.436,1.552,0.274,-0.652,-0.670,-0.213,0.534,-0.330,1.191,-0.589,-0.941,-0.479,1.849,-1.418,-0.340,1.184,0.798,-0.233,0.113,1.121,-0.392,1.814,1.730,0.409,-0.701,-0.602,-0.815,-0.012,-1.162,-0.386,0.239,0.264,-1.431,-0.482,-0.739,-0.544,-1.190,-0.780,-0.437,2.214,-0.162,-0.639,-0.193,0.838,1.599,0.202,0.393,-0.390,-0.544,0.046,-0.215,-0.645,-0.085,0.381,1.420,0.600,-0.295,-0.281,-0.112,0.418,-0.928,-0.435,1.574,-0.156,0.715,0.742,-0.828,1.179,0.424,1.168,0.478,-0.233,1.484,0.290,-0.004,-0.754,-1.839,0.542,1.283,-0.084,0.388,1.002,-0.299,-1.557,-1.029,-0.342,0.073,0.613,-0.194,-0.129,0.157,1.630,1.169,0.332,0.553,-0.772,1.010,1.297,-0.529,-0.845,-0.407,0.078,0.561,2.277,0.831,0.329,-0.462,0.156,0.188,0.569,1.522,-0.463,0.212,-0.628,0.886,1.110,-1.374,-1.474,-0.308,-1.450,-1.330,-1.158,-0.846,-1.795,-0.705,-0.451,0.147,-0.138,-0.464,0.147,-0.805,0.592,0.303,-2.036,-0.074,-0.599,0.347,0.131,-0.577,0.556,-1.410,-0.540,-0.284,-0.667,-1.164,-1.081,-1.210,0.821,0.425,-1.039,0.173,0.419,0.548,0.712,-0.030,-0.358,-1.189,-2.592,-0.452,0.019,-1.613,-0.220,0.514,0.346,1.668,1.718,1.428,1.190,1.238,0.432,-0.595,0.820,1.387,1.691,-2.654,-1.163,-0.201,-0.196,-1.546,0.199,-0.024,-0.415,-1.403,-2.206,0.898,0.264,-0.299,-0.952,-0.114,-0.641,0.317,0.877,0.495,-0.961,-0.311,-0.379,0.892,0.336,0.400,-0.319,-1.210,-0.717,-0.457,-0.383,0.681,-0.391,-2.718,-1.311,-0.562,0.462,0.116,0.114,0.859,-0.336,-0.491,-0.615,1.564,1.010,-1.430,1.796,0.611,0.386,-0.334,0.561,0.767,0.860,-0.212,0.060,-0.321,-0.994,-1.487,0.312,-0.278,-0.668,-0.526,0.160,0.796,0.087,0.423,-1.240,-0.654,0.756,0.350,0.292,-2.330,-2.815,1.177,1.003,2.261,0.745,0.346,0.457,1.043,1.281,-0.370,0.286,0.812,0.680,0.009,-0.006,1.465,0.017,0.252,-2.156,0.199,-0.296,-1.773,-2.473,-1.363,0.066,-1.017,-1.084,-0.374,0.051,-1.386,-0.277,-1.175,1.358,0.948,-0.299,-0.559,0.037,0.404,0.866,-0.173,-0.315,-0.021,-0.131,0.254,0.162,0.219,-1.139,-0.039,0.573,0.548,-0.976,-0.933,0.152,1.327,-0.308,0.056,0.170,-0.497,-0.292,0.366,0.753,0.418,0.362,-0.167,1.019,0.144,-1.137,0.605,0.708,-1.126,2.545,0.397,1.067,0.283,-0.692,0.466,-1.113,-0.144,1.324,-1.374,-2.735,-0.273,-0.649,-0.963,-0.435,0.072,0.934,0.511,-0.577,-0.050,-1.192,-0.642,-0.393,0.046,0.249,-2.403,-1.879,-0.803,0.748,0.433,0.399,1.508,-1.015,-0.755,-0.267,-0.530,-1.674,0.074,0.214,0.169,0.235,-0.205,-0.899,0.702,1.491,0.771,0.175,-0.329,-0.328,-0.078,-0.187,-0.181,-1.423,-1.353,0.130,1.862,1.087,-0.799,-0.964,0.459,0.246,0.693,0.156,0.676,0.054,0.799,0.643,0.131,-0.379,1.011,-0.516,0.645,0.238,-0.190,-0.923,-1.790,-1.833,-0.222,-0.090,-0.973,-2.455,-1.182,0.056,-0.061,0.796,0.581,1.674,-0.319,-0.819,-0.181,0.280,0.015,-0.919,-0.383,0.043,-0.240,2.160,0.406,-1.181,-0.030,-0.225,0.431,-0.107,1.056,1.098,-0.221,-1.338,0.500,1.926,1.099,-0.792,-0.462,-1.771,-1.434,0.043,0.730,-1.285,0.478,-0.603,-0.582,0.467,-1.261,-0.202,0.069,0.064,-0.166,-0.447,0.447,0.436,-0.335,-1.902,-0.346,-0.534,0.250,1.374,-0.089,0.844,0.239,0.456,-0.820,-0.026,-0.514,-0.868,-0.468,-0.871,0.189,-0.032,-0.560,-0.008,-0.184,-0.366,0.406,-0.245,-0.462,-0.229,-0.281,-0.054,0.121,0.291,-0.357,0.023,-0.447,-0.888,-0.338,-0.039,0.926,0.516,0.466,0.704,-0.074,-0.180,0.047,-0.525,-0.845,0.636,0.284,0.073,0.473,-0.125,0.091,0.178,0.045,-1.351,-1.565,-0.129,0.313,0.886,-0.280,0.362,0.522,-0.855,0.252,0.880,-0.912,-0.949,0.374,0.112,0.298,0.566,-0.093,-1.021,0.096,0.351,-0.076,0.225,0.190,-0.521,-0.090,0.715,0.060,-0.149,-0.275,-0.319,-0.595,-0.028,-0.201,0.158,-1.671,-0.773,0.568,-0.078,-0.199,0.087,-0.611,-0.351,0.739,0.192,-0.008,-0.316,-0.387,-0.736,-0.504,0.216,-0.057,-2.404,-0.229,-0.623,0.178,-0.064,0.600,0.446,0.821,-0.134,-0.218,-0.389,-0.894,-0.976,-0.457,-1.078,-0.131,-0.882,0.341,0.222,0.784,-0.081,-0.856,-0.175,0.300,-0.174,0.381,0.032,0.152,-1.029,-0.343,-0.004,-0.048,-0.402,-0.310,-0.899,-1.295,0.279,-0.417,-0.368,0.355,-0.457,-0.432,-0.010,-0.865,0.400,-0.599,-0.083,-0.852,-0.559,-0.052,0.147,0.262,0.026,-0.508,0.450,-0.271,0.075,-0.506,-0.295,-0.232,-0.107,0.056,-0.242,-0.719,0.885,-0.315,0.048,0.116,0.010,-0.754,0.593,-0.705,-0.431,0.804,0.138,-0.319,0.271,0.579,-1.165,0.974,-0.294,0.178,0.195,0.451,-0.217,-0.831,-0.123,0.633,0.286,-0.419,0.365,-0.521,-0.431,0.593,-0.094,0.107,-0.628,-0.905,-0.659,-0.264,-0.811,-0.397,-2.346,-0.555,0.415,0.058,-0.592,0.049,-0.338,-0.345,1.143,0.441,0.317,-0.651,-0.670,-0.808,-0.685,0.022,0.152,-1.459,0.269,0.869,-0.327,-0.989,-0.626,0.063,0.310,-0.201,0.506,-0.664,-0.073,-0.572,0.140,-0.097,-0.002,0.183,-0.612,0.150,-0.261,0.461,-0.363,2.003,-0.135,-0.416,-0.431,-0.332,-0.215,-1.216,-0.031,-0.048,0.744,-1.157,-0.226,-0.421,0.147,0.087,-0.722,0.319,-0.133,0.746,0.409,-0.889,-0.277,-0.637,-0.220,0.001,0.632,-0.141,-0.230,-0.083,-2.146,-0.079,0.064,0.033,-0.412,-0.052,0.386,0.527,-1.014,0.300,0.388,-0.109,-0.724,0.539,0.227,-0.066,0.084,-0.181,-0.133,-0.199,-0.260,-0.980,0.168,0.154,0.828,0.009,0.031,0.296,-0.119,-0.127,0.504,0.401,-0.232,-1.157,-1.705,0.697,-0.434,-0.603,0.238,0.110,0.278,0.258,-0.302,0.215,0.277,-0.872,0.011,0.406,0.515,-0.174,-0.108,-0.363,0.293,-0.131,-0.175,0.022,-0.698,0.343,0.569,-0.265,0.293,-0.205,-0.007,-0.010,1.016,0.339,-0.043,-1.729,-1.003,0.215,-0.507,-0.130,-0.667,-1.068,0.327,0.686,0.483,-0.248,0.214,-0.056,0.007,0.885,0.602,-0.326,0.151,0.245,0.046,-0.198,-0.196,-0.095,-0.172,0.218,0.480,-0.033,-0.118,-0.482,-0.037,0.034,0.292,0.214,-0.183,0.058,-1.235,-0.287,0.208,-1.236,-0.151,-0.611,0.514,0.555,-0.047,-0.449,-0.486,-0.459,0.094,0.786,-0.074,-0.668,-0.072,-0.635,-0.209,0.061,-0.074,0.051,-0.251,0.344,0.692,-0.253,0.013,-0.409,0.027,-0.472,0.980,0.904,-0.072,0.049,-1.227,-1.217,-0.199,0.073,-0.527,-0.502,0.352,0.578,-0.010,0.055,0.047,-0.195,-0.197,0.562,0.240,-0.007,-0.181,-1.576,0.376,0.014,0.017,-0.021,-0.182,0.789,0.790,-0.795,-0.373,0.080,-0.842,-0.069,0.649,0.627,0.060,-0.243,-0.845,0.040,-0.270,-0.986,0.140,0.055,0.056,0.244,-0.031,0.301,-0.209,-0.180,0.080,0.614,-0.028,-0.061,-0.050,-2.082,-0.076,-1.291,0.148,0.249,-0.341,0.381,0.617,-0.247,0.712,-0.179,-0.030,0.033,0.927,0.369,0.622,0.432,-2.090,-0.266,-1.700,-0.042,0.242,0.181,0.006,0.011,-0.065,-0.464,-0.198,-0.168,-0.219,-0.327,0.074,-0.221,-0.112,-1.451,-0.953,-0.167,0.043,-0.213,0.047,0.836,0.994,-0.030,-0.354,0.301,-1.088,-0.403,0.608,1.090,-0.093,-0.098,-1.729,0.021,-0.154,-0.390,0.053,-0.336,-0.003,-0.301,-0.176,0.096,-0.332,0.240,-1.843,0.351,0.193,-0.352,0.423,-0.475,-0.018,-0.100,0.095,0.018,0.505,0.059,-0.030,0.420,-0.219,0.556,0.055,0.429,0.125,0.242,0.212,-1.654,-0.403,0.089,0.110,-0.651,0.111,-1.742,0.171,0.304,0.300,0.604,-0.112,0.247,-0.383,0.202,0.173,0.648,0.393,-0.485,0.030,-0.028,0.408,0.252,-0.802,-0.725,-0.079,-0.043,-0.047,0.372,0.251,-0.199,-0.184,0.713,0.616,0.095,-0.124,0.033,0.402,0.337,0.577,0.519,0.421,0.029,-0.230,0.023,-0.946,0.287,0.250,0.239,0.323,0.071,0.260,0.308,-0.179,-0.071,0.249,-0.092,0.089,0.244,0.056,0.720,0.276,0.408,-0.080,0.234,-1.598,-0.469,0.541,0.404,0.078,0.245,0.174,0.001,0.000,0.453,0.243,0.037,-0.516,0.316,-0.336,-0.105,0.412,0.210,-0.569,-1.193,0.236,-0.124,0.154,0.248,0.233,0.005,0.244,-0.954,-0.046,-0.486,0.137,0.471,-0.089,0.463,0.355,-1.103,0.158,-0.651,-1.958,-0.188,0.112,-0.221,0.229,0.464,0.164,-0.019,0.636,0.428,0.542,0.098,0.277,0.381,0.597,0.268,0.869,0.797,-0.824,0.316,0.117,-0.038,0.142,0.242,-0.279,-0.869,0.132,0.561,0.352,-0.669,0.184,0.026,0.402,0.049,1.309,-1.136,-0.450,0.165,-0.027,0.341,0.679,-0.061,0.676,-0.217,0.613,-0.535,0.406,0.085,0.009,0.017,0.395,1.008, };


// Training

// Loss function for a single data
void lossFun(const FloatWN* w, const float* x, const float* y, float* val, FloatWN* grad) {

    // Conv layer 1
    float v1r[C1N][C1O * C1O], g1r[C1N][C1O * C1O];  // output value, gradient
    for (int i = 0; i < C1N; i++) {
        for (int j = 0; j < C1O * C1O; j++) {
            float s = 0.0f;
            for (int k = 0; k < C1S * C1S; k++)
                s += w->w1[i][k] * x[MC1[j][k]];  // multiply matrix
            activate(s, &v1r[i][j], &g1r[i][j]);  // apply activation function
        }
    }
    // Pooling
    float v1p[C1N][C2I * C2I], g1p[C1N][C2I * C2I];  // output value, gradient
    int i1p[C1N][C2I * C2I];  // indices of max value
    for (int i = 0; i < C1N; i++) {
        for (int j = 0; j < C2I * C2I; j++) {
            float s = -1e10f, g = 0.0f;
            for (int k = 0; k < 4; k++) {
                int it = MP1[j][k];
                float v = v1r[i][it];
                if (v > s) {
                    s = v, g = g1r[i][it];  // maxpool
                    i1p[i][j] = it;
                }
            }
            v1p[i][j] = s, g1p[i][j] = g;
        }
    }

    // Conv layer 2
    float v2r[C2N][C2O * C2O], g2r[C2N][C2O * C2O];  // output value, gradient
    for (int i = 0; i < C2N; i++) {
        for (int j = 0; j < C2O * C2O; j++) {
            float s = 0.0f;
            for (int pi = 0; pi < C1N; pi++) {
                for (int k = 0; k < C2S * C2S; k++)
                    s += w->w2[i][pi][k] * v1p[pi][MC2[j][k]];  // multiply matrix
            }
            activate(s, &v2r[i][j], &g2r[i][j]);  // apply activation function
        }
    }
    // Pooling
    float v2p[C2N][C2OA * C2OA],  // output value
        g2p[C2N][C2OA * C2OA];  // gradient
    int i2p[C2N][C2OA * C2OA];  // index of max value
    for (int i = 0; i < C2N; i++) {
        for (int j = 0; j < C2OA * C2OA; j++) {
            float s = -1e10f, g = 0.0f;
            for (int k = 0; k < 4; k++) {  // iterate through possible values
                int it = MP2[j][k];
                float v = v2r[i][it];
                if (v > s) {  // update
                    s = v, g = g2r[i][it];
                    i2p[i][j] = it;
                }
            }
            v2p[i][j] = s, g2p[i][j] = g;
        }
    }

    // 4 full-connected layers
    const int LN_I[4] = { L3I, L3N, L4N, L5N };  // layer input sizes
    const int LN_O[4] = { L3N, L4N, L5N, L6N };  // layer output sizes
    float v3i[L3I + 1];  // full-connected layer input
    std::memcpy(v3i, &v2p[0][0], sizeof(float) * L3I);
    v3i[L3I] = 1.0f;
    float v3[L3N + 1], g3[L3N + 1],  // layer 3
        v4[L4N + 1], g4[L4N + 1],  // layer 4
        v5[L5N + 1], g5[L5N + 1],  // layer 5
        v6r[L6N];  // output (no bias)
    const float* fci[4] = { v3i, v3, v4, v5 };  // layer inputs
    float* fcv[4] = { v3, v4, v5, v6r },  // layer outputs
        * fcg[4] = { g3, g4, g5, nullptr };  // gradient outputs
    const float* fcw[4] = { &w->w3[0][0], &w->w4[0][0], &w->w5[0][0], &w->w6[0][0] };  // layer weights
    for (int l = 0; l < 4; l++) {  // for each layer
        if (l < 3) {  // pad 1 for bias
            fcv[l][LN_O[l]] = 1.0f;
            fcg[l][LN_O[l]] = 0.0f;
        }
        for (int i = 0; i < LN_O[l]; i++) {
            float s = 0.0f;
            for (int j = 0; j <= LN_I[l]; j++)  // multiply matrix
                s += fcw[l][i * (LN_I[l] + 1) + j] * fci[l][j];
            if (l < 3) activate(s, &fcv[l][i], &fcg[l][i]);  // activate
            else fcv[l][i] = s;  // output layer
        }
    }

    // Softmax
    float v6[L6N];
    float g6[L6N][L6N];  // Jacobian, g6[i][j] = ∂v6[i]/∂v6r[j]
    float smax = -1e10f;  // prevent overflow in exp
    for (int i = 0; i < L6N; i++)
        smax = max(smax, v6r[i]);
    float sexp = 0.0f;  // sum of exps
    for (int i = 0; i < L6N; i++)
        sexp += (v6[i] = exp(v6r[i] -= smax));
    for (int i = 0; i < L6N; i++)  // value
        v6[i] /= sexp;
    for (int i = 0; i < L6N; i++)  // gradient
        for (int j = 0; j < L6N; j++)
            g6[i][j] = i == j
            ? v6[i] * (1.0f - v6[i])
            : -v6[i] * v6[j];

    // Cross-entropy
    float loss = 0.0f;  // final value
    float go[L6N];  // output derivative
    for (int i = 0; i < L6N; i++) {
        v6[i] = max(min(v6[i], 1.0f - 1e-7f), 1e-7f);
        float dv = -y[i] * log(v6[i]) - (1.0f - y[i]) * log(1.0f - v6[i]);
        go[i] = -y[i] / v6[i] + (1.0f - y[i]) / (1.0f - v6[i]);
        loss += dv;
    }
    *val = loss;

    // Backprop output layer
    float b6[L6N];  // ∂[loss]/∂[v6r]
    for (int i = 0; i < L6N; i++) {
        b6[i] = 0.0f;
        for (int j = 0; j < L6N; j++)
            b6[i] += g6[i][j] * go[j];
        for (int j = 0; j <= L5N; j++)
            grad->w6[i][j] = b6[i] * v5[j];
    }

    // Backprop full-connected layers
    float b5[L5N], b4[L4N], b3[L3N];
    float* fcb[4] = { b3, b4, b5, b6 };  // ∂[loss]/∂[v]
    float* fcwg[4] = { &grad->w3[0][0], &grad->w4[0][0], &grad->w5[0][0] };  // gradient of weights
    for (int l = 3; l--;) {
        for (int i = 0; i < LN_O[l]; i++) {
            fcb[l][i] = 0.0f;  // gradient to layer
            for (int j = 0; j < LN_O[l + 1]; j++)
                fcb[l][i] += fcb[l + 1][j] * fcw[l + 1][j * (LN_I[l + 1] + 1) + i] * fcg[l][i];
            for (int j = 0; j <= LN_I[l]; j++)  // gradient to weights
                fcwg[l][i * (LN_I[l] + 1) + j] = fcb[l][i] * fci[l][j];
        }
    }

    // Backprop conv layer 2
    float b2p[C2N][C2OA * C2OA];  // ∂[loss]/∂[v2p], after pooling
    for (int i = 0; i < C2N; i++)
        for (int j = 0; j < C2OA * C2OA; j++) {
            b2p[i][j] = 0.0f;
            for (int k = 0; k < L3N; k++)
                b2p[i][j] += b3[k] * w->w3[k][i * C2OA * C2OA + j] * g2p[i][j];
        }
    float b2r[C2N][C2O * C2O];  // ∂[loss]/∂[v2r], before pooling
    for (int i = 0; i < C2N; i++)
        for (int j = 0; j < C2O * C2O; j++)
            b2r[i][j] = 0.0f;  // initialize to zero
    for (int i = 0; i < C2N; i++)
        for (int j = 0; j < C2OA * C2OA; j++)
            b2r[i][i2p[i][j]] = b2p[i][j];  // fill non-zero values
    for (int i = 0; i < W2N; i++)
        (&grad->w2[0][0][0])[i] = 0.0f;  // init gradient to zero
    for (int i = 0; i < C2N; i++)
        for (int j = 0; j < C2O * C2O; j++)  // write the gradient of weights
            for (int pi = 0; pi < C1N; pi++)
                for (int k = 0; k < C2S * C2S; k++)
                    grad->w2[i][pi][k] += v1p[pi][MC2[j][k]] * b2r[i][j];

    // Backprop conv layer 1 - almost there!
    float b1p[C1N][C2I * C2I];  // ∂[loss]/∂[v1p], after pooling
    for (int i = 0; i < C1N; i++)
        for (int j = 0; j < C2I * C2I; j++)
            b1p[i][j] = 0.0f;  // clear to zero
    for (int i = 0; i < C2N; i++)
        for (int j = 0; j < C2O * C2O; j++)
            for (int pi = 0; pi < C1N; pi++)
                for (int k = 0; k < C2S * C2S; k++) {  // add gradient to b1p
                    int it = MC2[j][k];
                    b1p[pi][it] += b2r[i][j] * w->w2[i][pi][k] * g1p[pi][it];
                }
    float b1r[C1N][C1O * C1O];  // ∂[loss]/∂[v1r], before pooling
    for (int i = 0; i < C1N; i++)
        for (int j = 0; j < C1O * C1O; j++)
            b1r[i][j] = 0.0f;  // initialize to zero
    for (int i = 0; i < C1N; i++)
        for (int j = 0; j < C2I * C2I; j++)
            b1r[i][i1p[i][j]] = b1p[i][j];  // fill non-zero values
    for (int i = 0; i < W1N; i++)
        (&grad->w1[0][0])[i] = 0.0f;  // clear gradient to zero
    for (int i = 0; i < C1N; i++)
        for (int j = 0; j < C1O * C1O; j++)
            for (int k = 0; k < C1S * C1S; k++)  // write the gradient of weights
                grad->w1[i][k] += x[MC1[j][k]] * b1r[i][j];

}

// Loss function for a data batch
void lossFunBatch(int ndata, const float* w, const float* x, const float* y, float* val, float* grad) {
    *val = 0.0f;
    for (int i = 0; i < WN; i++) grad[i] = 0.0f;  // init to zero
    float val_t, grad_t[WN];
    for (int di = 0; di < ndata; di++) {
        lossFun((const FloatWN*)w, &x[di * X_DIM], &y[di * Y_DIM], &val_t, (FloatWN*)grad_t);
        *val += val_t;  // add data
        for (int i = 0; i < WN; i++) grad[i] += grad_t[i];
    }
    // divide by number of data to average
    *val /= float(ndata);
    for (int i = 0; i < WN; i++) grad[i] /= float(ndata);
}

// Adam optimizer
void minimizeAdam(
    int ndim, int ndata,
    std::function<void(int ndata, const float* w, const float* x, const float* y, float* val, float* grad)> lossfun,
    float* w, const float* x, const float* y,
    int batch_size, float learning_step,
    float beta_1, float beta_2, int max_epoch, float gtol,
    std::function<void(int epoch)> callback
) {

    // loss and gradient
    float loss_t, loss = 0.0f;
    float* grad_t = new float[ndim];  // gradient in evaluation
    float* grad = new float[ndim];  // smoothed gradient
    float* grad2 = new float[ndim];  // smoothed squared gradient
    for (int i = 0; i < ndim; i++) {
        grad[i] = 0.0f;
        grad2[i] = 0.0f;
    }

    // epoches
    for (int epoch = 0; epoch < max_epoch; callback(++epoch)) {

        // batches
        for (int batch = 0; batch < ndata; batch += batch_size) {
            // get data
            int n_batch = min(ndata - batch, batch_size);
            // evaluate function
            lossfun(n_batch, w, &x[batch * X_DIM], &y[batch * Y_DIM], &loss_t, grad_t);
            // check NAN
            bool has_nan = false;
            for (int i = 0; i < WN; i++) {
                if (std::isnan(grad_t[i])) has_nan = true;
            }
            if (has_nan) continue;
            // update
            loss = beta_1 * loss + (1.0f - beta_1) * loss_t;
            for (int i = 0; i < ndim; i++) {
                grad[i] = beta_1 * grad[i] + (1.0f - beta_1) * grad_t[i];
                grad2[i] = beta_2 * grad2[i] + (1.0f - beta_2) * grad_t[i] * grad_t[i];
                w[i] -= learning_step * grad[i] / (sqrt(grad2[i]) + 1e-8f);
            }
            // verbose
            fprintf(stderr, "\r%.1f%% loss=%f", float(batch + batch_size) * 100.0f / float(ndata), loss);
        }

        // check
        float grad_norm = 0.0f;
        for (int i = 0; i < ndim; i++) grad_norm += grad[i] * grad[i];
        grad_norm = sqrt(grad_norm);
        fprintf(stderr, "\rEpoch %d/%d, loss=%f, grad=%f\n", epoch + 1, max_epoch, loss, grad_norm);
        if (grad_norm < gtol) break;
    }

    delete x; delete y;
    delete grad_t; delete grad; delete grad2;
}

// SGD optimizer
void minimizeSGD(
    int ndim, int ndata,
    std::function<void(int ndata, const float* w, const float* x, const float* y, float* val, float* grad)> lossfun,
    float* w, const float* x, const float* y,
    int batch_size, float learning_step,
    float momentum, int max_epoch, float gtol,
    std::function<void(int epoch)> callback
) {

    // loss and gradient
    float loss_t, loss = 0.0f;
    float* grad_t = new float[ndim];  // gradient in evaluation
    float* grad = new float[ndim];  // smoothed gradient
    for (int i = 0; i < ndim; i++) {
        grad[i] = 0.0f;
    }

    // epoches
    for (int epoch = 0; epoch < max_epoch; callback(++epoch)) {

        // batches
        for (int batch = 0; batch < ndata; batch += batch_size) {
            // get data
            int n_batch = min(ndata - batch, batch_size);
            // evaluate function
            lossfun(n_batch, w, &x[batch * X_DIM], &y[batch * Y_DIM], &loss_t, grad_t);
            // check NAN
            bool has_nan = false;
            for (int i = 0; i < WN; i++) {
                if (std::isnan(grad_t[i])) has_nan = true;
            }
            if (has_nan) continue;
            // update
            loss = momentum * loss + (1.0f - momentum) * loss_t;
            for (int i = 0; i < ndim; i++) {
                grad[i] = momentum * grad[i] + (1.0f - momentum) * grad_t[i];
                w[i] -= learning_step * grad[i];
            }
            // verbose
            fprintf(stderr, "\r%.1f%% loss=%f", float(batch + batch_size) * 100.0f / float(ndata), loss);
        }

        // check
        float grad_norm = 0.0f;
        for (int i = 0; i < ndim; i++) grad_norm += grad[i] * grad[i];
        grad_norm = sqrt(grad_norm);
        fprintf(stderr, "\rEpoch %d/%d, loss=%f, grad=%f\n", epoch + 1, max_epoch, loss, grad_norm);
        if (grad_norm < gtol) break;
    }

    delete x; delete y;
    delete grad_t; delete grad;
}



// Testing

// Evaluate a test data
void evalFun(const FloatWN* w, const float* x, float* y) {
    float gtemp;

    // Conv layer 1
    float v1r[C1N][C1O * C1O];
    for (int i = 0; i < C1N; i++) {
        for (int j = 0; j < C1O * C1O; j++) {
            float s = 0.0f;
            for (int k = 0; k < C1S * C1S; k++)
                s += w->w1[i][k] * x[MC1[j][k]];  // multiply matrix
            activate(s, &v1r[i][j], &gtemp);  // apply activation function
        }
    }
    // Pooling
    float v1p[C1N][C2I * C2I];
    for (int i = 0; i < C1N; i++) {
        for (int j = 0; j < C2I * C2I; j++) {
            float s = -1e10f;
            for (int k = 0; k < 4; k++)
                s = max(s, v1r[i][MP1[j][k]]);
            v1p[i][j] = s;
        }
    }

    // Conv layer 2
    float v2r[C2N][C2O * C2O];
    for (int i = 0; i < C2N; i++) {
        for (int j = 0; j < C2O * C2O; j++) {
            float s = 0.0f;
            for (int pi = 0; pi < C1N; pi++) {
                for (int k = 0; k < C2S * C2S; k++)
                    s += w->w2[i][pi][k] * v1p[pi][MC2[j][k]];  // multiply matrix
            }
            activate(s, &v2r[i][j], &gtemp);  // apply activation function
        }
    }
    // Pooling
    float v2p[C2N][C2OA * C2OA];
    for (int i = 0; i < C2N; i++) {
        for (int j = 0; j < C2OA * C2OA; j++) {
            float s = -1e10f;
            for (int k = 0; k < 4; k++)
                s = max(s, v2r[i][MP2[j][k]]);
            v2p[i][j] = s;
        }
    }

    // 4 full-connected layers
    const int LN_I[4] = { L3I, L3N, L4N, L5N };  // layer input sizes
    const int LN_O[4] = { L3N, L4N, L5N, L6N };  // layer output sizes
    float v3i[L3I + 1];  // full-connected layer input
    std::memcpy(v3i, &v2p[0][0], sizeof(float) * L3I);
    v3i[L3I] = 1.0f;
    float v3[L3N + 1], v4[L4N + 1], v5[L5N + 1], v6r[L6N];
    const float* fci[4] = { v3i, v3, v4, v5 };  // layer inputs
    float* fcv[4] = { v3, v4, v5, v6r };  // layer outputs
    const float* fcw[4] = { &w->w3[0][0], &w->w4[0][0], &w->w5[0][0], &w->w6[0][0] };  // layer weights
    for (int l = 0; l < 4; l++) {  // for each layer
        if (l < 3)  // pad 1 for bias
            fcv[l][LN_O[l]] = 1.0f;
        for (int i = 0; i < LN_O[l]; i++) {
            float s = 0.0f;
            for (int j = 0; j <= LN_I[l]; j++)  // multiply matrix
                s += fcw[l][i * (LN_I[l] + 1) + j] * fci[l][j];
            if (l < 3) activate(s, &fcv[l][i], &gtemp);  // activate
            else fcv[l][i] = s;  // output layer
        }
    }

    // output
    for (int i = 0; i < Y_DIM; i++)
        y[i] = v6r[i];
}

// Convert output vector to class
int getClass(const float* y) {
    int maxi = -1;
    float maxval = -1e10f;
    for (int i = 0; i < Y_DIM; i++) {
        if (y[i] > maxval)
            maxi = i, maxval = y[i];
    }
    return maxi;
}



// Main functions


// Main training function
void mainTrain() {

    // data
    float* x_train, * y_train;
    int n_train = loadMnist("bin/all_x.bin", "bin/all_y.bin", x_train, y_train);

    // random weights
    if (0) {
        for (int i = 0; i < WN; i++)
            WEIGHTS[i] = 0.1f * (2.0f * randf() - 1.0f);
    }
    fprintf(stderr, "%d weights\n", WN);

    // check the correctness of gradient
    if (0) fprintf(stderr, "grad_err: %g\n",
        checkGrad(WN, [&](const float* w, float* val, float* grad) {
            lossFun((const FloatWN*)w, &x_train[0], &y_train[0], val, (FloatWN*)grad);
            }, WEIGHTS, 0.004f, true)
    );

    // Optimization
    auto callback = [&](int epoch) {
        // test accuracy
        const int test_count = 1000;
        int correct_count = 0;
        float y[Y_DIM];
        for (int _ = 0; _ < test_count; _++) {
            int i = randu() % n_train;
            evalFun((FloatWN*)WEIGHTS, &x_train[i * X_DIM], y);
            int class_guessed = getClass(y);
            int class_correct = getClass(&y_train[i * Y_DIM]);
            if (class_correct == class_guessed) correct_count += 1;
        }
        printf("%d/%d correct\n", correct_count, test_count);
        // export weights
        FILE* fp = fopen("weights.txt", "w");
        for (int i = 0; i < WN; i++) fprintf(fp, "%.3f,", WEIGHTS[i]);
        fclose(fp);
    };
    // minimizeAdam(WN, n_train, lossFunBatch, WEIGHTS, x_train, y_train, \
        50, 0.01f, 0.9f, 0.999f, \
        5, 1e-6f, callback);
    minimizeSGD(WN, n_train, lossFunBatch, WEIGHTS, x_train, y_train, \
        500, 0.03f, 0.9f, \
        5, 1e-6f, callback);
}


// Main solving function - https://dmoj.ca/problem/tle18p1
void mainSolve() {
#ifdef _WIN32
    freopen("stdin.txt", "r", stdin);
#endif
    std::cin.sync_with_stdio(0); std::cin.tie(0);
    int n; std::cin >> n;
    while (n--) {
        float x[X_DIM], y[Y_DIM];
        for (int i = 0; i < X_DIM; i++) std::cin >> x[i];
        evalFun((const FloatWN*)WEIGHTS, x, y);
        printf("%d\n", getClass(y));
    }
}


// Main
int main(int argc, char* argv[]) {
    initMatrices();

    mainTrain();
    // mainSolve();

    return 0;
}