// Solve the "Hello, World" problem - https://dmoj.ca/problem/tle18p1
// Using a CNN developed in C++ from scratch.

#include <bits/stdc++.h>

using std::abs;
using std::max;
using std::min;



// Uncomment this line when debugging analytical gradient
// #define float double


// Quick-and-dirty PRNG from Numerical Recipes
uint32_t _IDUM = 1u;
uint32_t randu() { return _IDUM = _IDUM * 1664525u + 1013904223u; }
float randf() { return (float)randu() / 4294967296.0f; }


// Load the MNIST dataset
int loadMnist(const char* x_path, const char* y_path, float*& x, float*& y) {

    // read x - between 0 and 1
    FILE* fp = fopen(x_path, "rb");
    fseek(fp, 0, SEEK_END);
    size_t fsize = ftell(fp);
    fseek(fp, 0, SEEK_SET);
    uint8_t* raw = new uint8_t[fsize];
    fread(raw, fsize, 1, fp);
    fclose(fp);
    int ndata = (int)(fsize / 784);
    x = new float[ndata * 784];
    for (int i = 0; i < ndata * 784; i++)
        x[i] = (float)raw[i] / 255.0f;

    // read y - vectors of 10, 0 false 1 true
    fp = fopen(y_path, "rb");
    fread(raw, ndata, 1, fp);
    fclose(fp);
    y = new float[ndata * 10];
    for (int i = 0; i < ndata; i++)
        for (int j = 0; j < 10; j++)
            y[10 * i + j] = (raw[i] == j ? 1.0 : 0.0);
    delete raw;

    return ndata;
}

// Plot a digit
void plotDigit(const float* x) {
    for (int i = 0; i < 28; i++) {
        for (int j = 0; j < 28; j++) {
            fprintf(stderr, "%s", x[i * 28 + j] > 0.5f ? "##" : ". ");
        }
        fprintf(stderr, "\n");
    }
}


// Checks if the analytical gradient of a function is correct, returns a representation of error
float checkGrad(
    int ndim,
    std::function<void(const float* x, float* val, float* grad)> fun,
    float* x,
    float eps = 0.004f,
    bool verbose = false
) {
    float val;
    float* grad0 = new float[ndim];  // reference gradient
    float* grad = new float[ndim];  // temp evaluation gradient
    for (int i = 0; i < ndim; i++)
        grad0[i] = 2.0 * randf() - 1.0;  // random
    fun(x, &val, grad0);  // evaluate reference gradient
    float maxerr = 0.0f;
    for (int i = 0; i < ndim; i++) {  // for each dimension
        float val1, val0;
        float x0 = x[i];  // forward difference
        x[i] = x0 + eps;
        fun(x, &val1, grad);
        x[i] = x0 - eps;  // backward difference
        fun(x, &val0, grad);
        x[i] = x0;
        float dvdx = (val1 - val0) / (2.0f * eps);  // central difference
        float err = abs(grad0[i]) < 1.0 ?  // adaptive error
            dvdx - grad0[i] : dvdx / grad0[i] - 1.0;
        maxerr = max(maxerr, err);
        if (verbose) {
            fprintf(stderr, "%.4lf ", abs(err));
            // fprintf(stderr, "%.4lf ", fmod(100.0f * abs(err), 1.0f));
            // fprintf(stderr, "%d ", max(min((int)round(-log10(abs(err))), 9), 0));
        }
    }
    delete grad; delete grad0;
    return maxerr;
}



// CNN architecture

#define C1I 28  /* layer 1 input dimension */
#define C1S 5  /* conv layer 1 size */
#define C1N 12  /* conv layer 1 count */
#define C1O ((C1I)-(C1S)+1)  /* conv layer 1 output dimension */
/* relu */
/* maxpool */
#define C2I ((C1O)>>1)  /* conv layer 2 input dimension */
#define C2S 3  /* conv layer 2 size */
#define C2N 6  /* conv layer 2 count */
#define C2O ((C2I)-(C2S)+1)  /* conv layer 2 output dimension */
/* relu */
/* maxpool */
#define C2OA ((C2O)>>1)  /* conv layer 2 output dimension activated */
/* flatten */
#define L3I ((C2N)*(C2OA)*(C2OA))  /* layer 3 input size */
#define L3N 16  /* layer 3 output size */
/* relu */
#define L4N 16  /* layer 4 output size */
/* relu */
#define L5N 16  /* layer 5 output size */
/* relu */
#define L6N 10  /* output layer */
/* softmax */
/* cross-entropy loss */


#define X_DIM ((C1I)*(C1I))
#define Y_DIM L6N


// Number of weights in each layer
#define W1N ((C1S)*(C1S)*(C1N))
#define W2N ((C2S)*(C2S)*(C2N)*(C1N))
#define W3N (((L3I)+1)*(L3N))
#define W4N (((L3N)+1)*(L4N))
#define W5N (((L4N)+1)*(L5N))
#define W6N (((L5N)+1)*(L6N))
#define WN (W1N+W2N+W3N+W4N+W5N+W6N)

// A struct of floats for easy access
struct FloatWN {
    float w1[C1N][C1S * C1S];
    float w2[C2N][C1N][C2S * C2S];
    float w3[L3N][L3I + 1];
    float w4[L4N][L3N + 1];
    float w5[L5N][L4N + 1];
    float w6[L6N][L5N + 1];
};

// COO matrix for convolutional layers
int MC1[C1O * C1O][C1S * C1S];  // layer 1 convolution indices
int MP1[C2I * C2I][2 * 2];  // layer 1 pooling indices
int MC2[C2O * C2O][C2S * C2S];  // layer 2 convolution indices
int MP2[C2OA * C2OA][2 * 2];  // layer 2 pooling indices
void initMatrices() {
    // MC1
    for (int i = 0; i < C1O; i++) for (int j = 0; j < C1O; j++)
        for (int di = 0; di < C1S; di++) for (int dj = 0; dj < C1S; dj++) {
            int b = (i + di) * C1I + (j + dj);
            MC1[i * C1O + j][di * C1S + dj] = b;
        }
    // MP1
    for (int i = 0; i < C2I; i++) for (int j = 0; j < C2I; j++)
        for (int di = 0; di < 2; di++)  for (int dj = 0; dj < 2; dj++) {
            int b = (i * 2 + di) * C1O + (j * 2 + dj);
            MP1[i * C2I + j][di * 2 + dj] = b;
        }
    // MC2
    for (int i = 0; i < C2O; i++) for (int j = 0; j < C2O; j++)
        for (int di = 0; di < C2S; di++) for (int dj = 0; dj < C2S; dj++) {
            int b = (i + di) * C2I + (j + dj);
            MC2[i * C2O + j][di * C2S + di] = b;
        }
    // MP2
    for (int i = 0; i < C2OA; i++) for (int j = 0; j < C2OA; j++)
        for (int di = 0; di < 2; di++) for (int dj = 0; dj < 2; dj++) {
            int b = (i * 2 + di) * C2O + (j * 2 + dj);
            MP2[i * C2OA + j][di * 2 + dj] = b;
        }
}


// Activation function
void activate(float x, float* y, float* dydx) {
    // *y = max(x, 0.0f); *dydx = x > 0.0f ? 1.0f : 0.0f;  // standard ReLU
    *y = max(x, 0.1f * x); *dydx = x > 0.0f ? 1.0f : 0.1f;  // much better?!
    // *y = sin(x); *dydx = cos(x);  // easier to see if something goes wrong
}


// Put weights here

float WEIGHTS[WN] = { -0.090,-0.032,-0.257,-0.204,-0.048,-0.126,-0.130,-0.387,0.058,-0.241,-0.356,-0.555,-0.438,-0.133,-0.686,-0.285,-0.467,-0.186,-0.016,-0.033,-0.407,0.078,-0.086,-0.007,-0.233,-0.008,0.025,-0.160,-0.311,-0.692,-0.007,0.044,0.328,0.477,0.070,0.024,0.205,0.488,0.752,0.587,-0.278,-0.286,-0.273,0.090,0.523,-0.387,-0.834,-1.065,-1.040,-0.862,-0.162,-0.522,-0.204,-0.288,-0.396,-0.356,-0.773,-0.419,-0.007,0.372,-0.234,-0.735,-0.051,0.430,0.503,-0.224,-0.627,0.173,0.194,0.001,-0.495,-0.265,-0.008,-0.026,-0.280,-0.516,-0.662,-0.672,-0.822,-0.936,0.051,-0.242,-0.340,-0.358,-0.389,0.519,0.284,-0.022,0.088,-0.170,0.382,0.283,0.021,0.242,0.049,0.492,0.269,0.242,0.325,0.219,-0.734,-0.668,-0.411,0.311,0.719,0.086,0.410,0.297,0.381,0.870,0.400,0.414,-0.249,-0.989,-0.872,-0.384,-0.266,-0.642,-0.870,-0.824,-0.733,-0.970,-0.184,-0.021,-0.014,-0.589,-0.132,-0.201,0.020,-0.169,-0.410,-0.165,-0.026,0.145,-0.018,0.101,0.170,0.049,-0.068,-0.090,0.082,0.186,-0.112,-0.158,-0.340,-0.306,-0.017,0.184,0.066,-0.317,-0.364,-0.351,-1.882,-2.069,-1.328,-0.563,-0.620,-1.616,-1.232,-0.803,-0.651,-1.106,-2.248,-1.274,-0.671,-0.774,-1.258,-1.987,-1.133,-0.477,-1.317,-1.754,-1.819,-0.812,-0.112,-0.478,-0.462,-0.135,0.130,-0.190,-0.193,-0.285,-0.192,0.178,-0.251,-0.166,-0.226,-0.063,0.125,-0.022,-0.152,-0.073,-0.169,-0.123,-0.195,-0.240,-0.343,-0.113,0.142,0.073,-0.128,0.062,-0.008,0.433,0.740,-0.234,-0.473,-0.519,0.257,1.023,0.095,-0.237,-0.551,-0.074,0.218,0.551,0.328,0.004,-0.107,-0.190,-0.356,-0.664,-0.416,-0.330,-0.150,-0.684,-0.398,-0.223,0.095,-0.149,-0.634,-0.505,-0.480,-0.003,0.021,0.660,0.161,-0.130,-0.251,0.366,0.006,0.044,-0.029,-0.235,0.031,-0.373,-0.326,0.013,0.092,-0.369,0.002,0.467,0.303,0.559,0.565,-0.209,0.474,0.448,0.522,0.288,-0.855,0.247,0.181,0.096,-0.265,-1.503,-0.833,-1.154,-1.151,-1.050,-0.402,-0.639,-1.185,-0.593,-0.362,0.582,0.189,-0.191,-0.037,-0.160,-0.264,0.193,0.193,0.494,0.196,-0.296,0.392,0.404,0.470,0.206,-0.757,-0.295,0.132,0.109,0.151,-0.479,-1.113,-1.068,-0.997,-1.261,0.234,0.555,0.425,0.477,-0.244,0.406,0.516,0.441,-0.538,-0.013,-0.300,-0.330,-0.266,-0.038,-0.347,-0.380,-0.394,-0.118,-0.331,-0.332,-0.398,-0.388,-0.269,-0.464,-0.310,-0.307,-0.090,-0.146,-0.003,-0.141,-0.131,0.424,-0.019,-0.185,-0.137,-0.143,0.057,-0.698,-0.733,-0.749,-0.995,-0.754,-0.803,-0.733,-0.409,-0.290,-0.364,-0.233,-0.281,-0.331,-0.323,-0.251,-0.290,-0.682,1.269,-0.162,-0.198,-0.118,1.642,-0.196,-0.203,-0.283,1.228,-0.121,-0.186,-0.256,-0.293,-0.078,-0.234,-0.105,-0.113,-0.422,-0.347,-0.612,-0.581,-0.632,-0.201,-0.549,-0.676,-0.541,-0.062,-0.425,-0.367,-0.404,-0.356,0.285,-0.404,-0.272,-0.318,-0.329,-0.244,-0.301,-0.294,-0.367,-0.349,-0.268,-0.323,-0.208,0.075,0.642,-0.253,-0.078,-0.223,0.302,-0.130,-0.263,-0.245,-0.178,-0.653,0.678,0.679,0.716,0.025,0.794,0.606,0.790,-0.150,-0.462,-0.633,-0.642,-0.572,-0.569,-0.500,-0.551,-0.674,-0.564,-0.394,0.584,0.408,0.574,-0.141,0.538,0.471,0.469,-0.658,0.600,0.367,0.490,0.495,0.036,0.441,0.518,0.341,-0.154,-0.690,-1.118,-1.173,-1.245,-0.714,-1.237,-1.246,-1.176,-0.012,-0.109,0.343,0.242,0.307,-0.127,0.335,0.355,0.278,0.078,-0.099,0.762,0.596,0.648,0.973,0.697,0.662,0.727,1.027,-0.284,-0.407,-0.524,-0.387,-0.060,-0.469,-0.481,-0.464,-0.116,0.062,0.134,0.192,0.066,-0.301,0.147,0.178,0.160,-0.859,-0.547,-0.029,0.108,0.002,-0.429,0.021,0.124,0.112,-0.354,-1.157,-0.289,-0.283,-0.288,-0.604,-0.282,-0.230,-0.230,-0.671,-0.525,-1.211,-1.187,-1.179,-0.258,-1.113,-1.220,-1.289,-0.315,0.205,0.422,0.527,0.516,-0.133,0.496,0.549,0.444,0.097,-0.444,0.697,0.528,0.643,0.031,0.512,0.567,0.706,0.161,0.487,0.089,0.138,0.091,0.217,0.180,0.220,0.178,0.372,-0.938,-0.613,-0.488,-0.443,-1.057,-0.625,-0.481,-0.500,0.458,-0.024,-0.100,-0.172,-0.225,0.302,-0.064,-0.081,-0.053,-0.075,0.085,-0.105,-0.235,-0.211,0.130,-0.281,-0.236,-0.172,0.439,1.499,-0.213,-0.204,-0.291,0.877,-0.350,-0.244,-0.243,0.748,-0.004,-0.744,-0.775,-0.755,0.158,-0.713,-0.615,-0.630,0.285,-0.302,-0.015,0.059,0.039,-1.194,-0.093,0.020,-0.013,-0.964,0.416,-0.475,-0.624,-0.645,-0.366,-0.632,-0.614,-0.565,0.066,0.145,-0.724,-0.887,-0.871,0.762,-0.849,-0.805,-0.716,0.587,0.444,-0.140,-0.161,-0.166,0.011,-0.175,-0.203,-0.275,0.011,0.062,0.410,0.390,0.436,0.109,0.427,0.384,0.349,0.078,-0.505,0.308,0.306,0.437,-0.516,0.395,0.499,0.499,-0.547,-0.037,-0.763,-0.718,-0.769,0.162,-0.784,-0.894,-0.883,0.328,-0.753,-0.873,-0.799,-0.806,-1.040,-0.863,-0.836,-0.890,-1.408,-0.320,-1.241,-1.301,-1.428,-0.118,-1.327,-1.302,-1.407,0.638,-0.268,-0.039,-0.132,-0.198,0.103,-0.137,-0.086,-0.204,0.149,1.079,-0.301,-0.305,-0.144,1.622,-0.208,-0.189,-0.160,1.460,-0.062,-0.123,-0.188,-0.174,-0.001,-0.194,-0.158,-0.133,0.216,0.180,-0.250,-0.347,-0.394,0.037,-0.419,-0.248,-0.308,-0.010,-0.146,-0.634,-0.602,-0.668,0.149,-0.677,-0.683,-0.616,0.393,-0.734,0.148,0.118,0.224,-0.937,0.138,0.180,0.227,-0.184,-0.413,0.429,0.504,0.550,-0.526,0.455,0.500,0.514,-0.512,0.026,0.796,0.736,0.720,-0.140,0.860,0.794,0.724,-0.152,0.038,-0.887,-0.705,-0.903,-0.249,-0.871,-0.869,-0.892,-0.030,-0.249,-0.279,-0.346,-0.242,-0.233,-0.357,-0.295,-0.431,-0.281,-0.318,-0.047,0.048,-0.058,-0.262,-0.108,-0.099,-0.093,-0.306,-0.179,-1.077,-1.075,-1.036,-0.485,-1.071,-1.135,-1.161,-0.536,-0.237,-0.210,-0.080,-0.146,-0.075,-0.099,-0.154,-0.237,-0.273,-0.245,-0.397,-0.440,-0.449,0.251,-0.438,-0.437,-0.305,0.309,-0.204,-0.244,-0.132,-0.248,-0.092,-0.179,-0.086,-0.163,-0.069,-0.109,-0.782,-0.954,-0.802,-0.093,-0.771,-0.845,-0.924,-0.095,-0.379,-0.764,-0.752,-0.597,-0.048,-0.575,-0.652,-0.587,-0.162,-0.126,-0.373,-0.350,-0.384,-0.262,-0.297,-0.342,-0.368,-0.167,0.069,-0.311,-0.364,-0.495,0.089,-0.423,-0.413,-0.474,-0.199,-0.101,0.602,0.634,0.447,0.178,0.523,0.501,0.591,-0.084,-0.033,0.124,0.126,0.212,-0.288,0.197,0.167,0.107,-0.212,0.038,-0.274,-0.337,-0.334,0.308,-0.234,-0.325,-0.259,0.349,-0.067,-0.274,-0.207,-0.242,-0.056,-0.241,-0.227,-0.248,-0.369,-0.935,-0.424,-0.434,-0.377,-0.828,-0.496,-0.485,-0.479,-0.096,0.045,0.313,0.277,0.347,-0.239,0.298,0.216,0.311,0.183,0.431,0.068,-0.016,0.018,0.692,0.028,-0.032,-0.056,0.318,0.501,-0.022,0.042,-0.086,0.163,-0.094,-0.000,-0.059,0.057,-0.107,-0.216,-0.217,-0.395,0.012,-0.400,-0.293,-0.311,0.275,-0.071,0.226,0.178,0.219,-0.110,0.118,0.211,0.176,0.091,-1.543,-0.103,-0.138,-0.268,-1.138,-0.174,-0.110,-0.179,-1.030,0.148,0.632,0.480,0.494,-0.614,0.519,0.525,0.580,-0.729,-1.707,0.185,0.923,0.749,-1.366,-0.059,0.440,0.341,0.222,1.400,0.047,0.603,0.153,0.179,0.967,0.412,0.974,0.331,0.432,-0.249,0.542,0.774,0.099,-0.447,-0.162,-0.747,1.098,0.702,0.606,-0.757,0.970,1.378,-0.143,-0.314,-0.888,-0.205,1.191,-0.193,-0.288,-0.823,-0.043,0.696,0.380,0.246,-0.284,0.586,0.973,0.391,-0.004,-0.775,-0.764,0.513,1.695,0.884,-1.431,-0.646,0.255,0.084,-0.246,1.467,-0.141,1.011,0.169,0.462,2.381,1.594,0.891,-0.027,0.523,-0.153,0.926,0.332,0.066,0.878,1.423,-0.965,0.710,1.612,0.666,-1.310,0.015,0.265,0.200,0.029,1.359,-0.178,0.656,0.028,-0.059,1.363,0.170,0.602,-0.057,0.487,1.414,0.538,0.575,-0.203,0.965,0.735,-0.228,0.657,0.820,0.801,-0.197,0.651,0.446,0.609,0.300,1.150,0.487,0.701,0.612,0.516,0.926,0.236,0.833,0.607,0.614,0.894,0.825,0.658,0.495,0.584,0.161,-0.403,0.028,0.317,-0.052,-0.839,-0.251,-0.050,-0.409,-0.274,-0.171,-0.184,0.335,-0.407,-0.398,1.310,-0.461,0.107,-0.167,-0.382,0.662,-0.106,-0.214,-0.256,0.680,0.061,-0.076,-0.321,-0.432,-0.308,-0.582,-0.898,0.088,-0.232,0.055,-0.102,0.208,-0.055,-0.800,-0.309,0.188,0.898,0.298,-0.602,-0.104,-0.149,-0.410,-0.295,-1.229,-1.092,-0.085,-0.245,0.336,-0.222,-0.357,0.264,-0.259,-0.632,0.081,-0.412,0.068,0.104,-0.383,0.198,-0.967,-0.843,-0.753,0.110,0.649,0.091,-0.001,-0.146,-0.264,-0.823,0.402,1.094,1.056,-0.389,-0.482,-0.871,-0.888,-0.352,0.488,0.151,0.219,0.285,-0.858,1.242,-1.312,0.473,1.764,0.934,-0.165,0.050,1.034,0.193,0.013,-0.797,-1.489,-1.097,-1.336,-1.142,-0.213,-0.289,-0.383,-0.528,-1.022,-0.021,-0.188,-0.379,-0.120,-0.530,0.023,0.030,-1.001,-0.149,0.555,0.491,0.738,-0.805,-0.216,-0.276,-0.187,-0.807,-0.751,-0.068,-0.290,0.035,-0.203,-0.112,-0.372,-0.231,-0.048,-0.268,-0.078,-0.201,0.025,-0.100,-0.480,-0.249,-0.180,0.463,0.556,-0.010,-0.293,-0.365,0.037,-0.174,-0.559,-0.363,0.065,0.443,0.442,-0.177,-0.759,0.331,-0.602,-0.095,0.045,-0.300,-0.160,-1.412,-0.411,-0.041,-0.664,-1.131,-0.213,1.645,0.074,-0.545,0.057,0.826,-0.968,-0.345,-0.750,-0.404,-0.022,-0.582,-0.432,0.198,0.215,-0.010,-0.192,0.378,0.326,0.582,0.071,0.325,0.449,0.342,0.245,0.235,0.251,0.281,0.269,0.451,0.406,0.005,-0.157,-0.277,-0.091,0.132,-0.705,0.402,0.149,0.051,0.103,0.620,0.110,0.221,0.093,-0.076,0.484,0.474,0.128,0.403,-0.033,0.513,0.474,0.593,0.457,-0.186,-0.152,-0.412,0.041,-0.184,-0.143,-0.876,-0.605,-0.277,-0.467,-0.738,0.644,0.842,-0.052,0.351,-0.200,-0.156,0.469,-1.110,-0.280,-0.231,0.272,0.189,-0.139,-0.331,1.114,-0.138,-0.980,-1.000,-0.531,0.234,-0.426,0.303,0.437,0.811,0.463,-0.345,0.120,0.480,0.692,0.116,0.028,0.033,0.505,0.076,0.103,0.494,0.270,0.724,0.159,0.422,0.416,-0.235,0.004,-0.077,0.230,-0.099,0.138,0.212,0.116,0.124,0.140,0.514,0.337,0.345,0.278,0.124,0.479,0.299,0.246,0.036,-0.056,-0.010,0.255,0.270,0.054,-0.242,-0.145,0.162,0.028,0.083,-0.649,-0.008,-0.255,0.425,0.313,0.286,-0.573,0.108,0.580,-0.368,-0.045,-0.121,0.163,-0.026,-0.925,0.208,0.027,0.032,-0.223,-0.443,0.577,-0.332,-0.038,-0.284,-0.751,0.029,-0.481,0.101,-0.099,-0.420,-0.156,0.644,0.758,0.234,-0.378,0.071,-1.018,0.524,-0.471,-0.072,0.022,-0.472,0.006,-0.424,-0.223,-0.297,-0.516,-0.265,-0.073,0.238,0.902,0.316,-0.372,-0.687,0.043,0.210,0.453,-0.441,-0.370,0.933,0.450,0.025,-0.292,-1.395,-0.277,0.259,0.486,0.348,-0.891,0.001,0.381,0.002,-0.368,-0.366,0.007,0.352,-0.607,0.129,0.107,-0.196,-0.113,-0.070,2.412,1.247,0.425,0.696,0.055,1.374,-0.815,0.611,-0.013,-0.076,0.137,0.165,-0.114,-0.526,-1.482,-0.532,0.820,1.089,0.988,-0.324,0.405,-0.061,-0.101,-0.098,0.784,0.498,0.414,-0.015,0.230,-0.120,0.869,-0.426,-0.277,0.288,-0.249,-0.281,-0.746,-0.735,-0.050,-1.303,-0.973,-0.453,-0.053,1.155,0.404,-0.122,-0.180,-0.365,-0.347,-0.179,-0.183,-0.187,-0.197,-0.270,-0.432,-0.060,-0.444,-0.370,0.017,-0.503,-0.079,-0.460,-0.296,0.000,-0.611,-0.403,-0.551,-0.212,0.489,0.168,-1.583,-1.074,0.109,-0.666,0.820,-1.060,-0.549,0.006,-0.148,-0.089,-0.150,-1.063,-0.710,0.378,0.037,-0.739,-0.880,-0.176,0.145,-1.587,-0.962,-0.459,-0.385,-0.138,-0.900,-0.785,-0.280,0.599,0.308,0.080,-0.067,0.263,0.580,0.454,0.175,0.071,0.412,0.539,0.581,0.352,0.168,0.345,0.619,0.481,0.201,-0.225,-0.096,-0.034,0.155,0.105,-0.811,0.187,0.239,0.033,0.431,-0.275,0.307,0.434,0.049,-0.196,-0.045,-0.010,0.299,0.266,-0.166,0.216,0.027,0.575,0.230,0.122,0.200,0.104,0.246,-0.125,-0.031,-0.133,0.376,0.180,0.160,0.060,1.373,0.460,0.451,0.359,0.221,-0.077,0.606,-0.742,0.238,0.263,-0.192,0.706,0.286,0.550,0.489,-0.222,-0.031,-0.776,-0.242,0.638,0.214,0.441,0.553,0.555,0.277,-0.239,0.381,0.594,0.421,-0.102,-0.087,-0.086,0.725,0.185,0.060,0.296,0.114,0.613,0.252,0.268,0.206,0.156,0.076,-0.044,0.316,0.213,0.241,0.448,0.342,0.166,-0.057,0.430,0.349,0.442,0.215,-0.049,0.267,0.317,0.406,0.294,-0.092,0.072,0.168,0.384,-0.039,-0.568,0.074,0.063,0.025,0.048,-0.832,-0.082,0.024,0.306,0.057,-0.256,-0.244,0.233,0.174,-0.346,-0.468,-0.449,0.152,-0.170,-0.839,-0.267,-0.238,0.176,-0.222,-0.637,0.176,-0.009,0.200,-0.078,-0.253,-0.242,-0.109,-0.064,-0.356,-0.695,0.024,-0.121,0.271,0.013,-0.035,-0.284,-0.964,1.021,0.977,0.862,0.481,-0.110,-0.303,-0.706,-0.653,-0.153,-0.186,-0.443,-0.087,-0.615,-0.407,-0.336,-0.667,-0.160,-0.039,-0.180,-0.022,-0.540,0.322,0.605,-0.097,-0.135,-1.488,0.187,1.021,1.215,0.640,-0.049,-0.058,0.327,-0.501,-1.151,0.061,-0.084,-0.668,-1.668,-1.893,0.474,0.193,-0.431,0.322,-0.766,2.273,1.413,0.119,-0.404,-1.768,2.454,1.869,1.025,0.409,2.392,-0.565,-1.477,0.129,1.950,0.809,-0.157,0.334,0.109,-0.854,-0.332,-0.280,-0.474,-0.495,0.004,-0.270,-0.096,0.121,0.454,0.025,-0.377,0.198,0.005,0.776,0.627,0.343,0.507,0.110,-0.310,-0.697,-1.732,-0.146,0.218,-0.795,-1.455,-1.540,-0.474,-0.039,-0.534,0.068,-0.251,-0.264,-0.300,0.201,-0.026,-0.107,0.177,0.334,0.357,0.347,-0.057,0.076,-0.008,-0.180,-0.734,-1.519,-0.081,-0.062,-0.529,-0.717,-1.251,0.208,-0.295,-0.918,-0.652,0.261,-0.849,-1.006,-0.467,0.266,0.072,0.021,0.876,0.587,1.379,-0.172,1.594,0.884,0.916,-0.709,-1.864,-0.379,-0.323,-1.189,-1.142,-0.995,-0.803,1.360,0.116,-0.849,-0.296,0.362,0.206,-0.218,-0.215,0.328,-0.032,-0.400,-1.098,-0.441,0.190,-0.191,-0.251,-0.550,-0.006,0.072,-0.426,0.003,0.150,0.314,-0.164,-0.590,0.092,-0.292,-1.397,-0.259,0.253,-0.230,-0.255,-0.295,-0.132,-0.348,0.296,-1.098,0.498,0.507,0.505,-0.692,-0.908,0.353,0.275,-0.087,-0.597,0.153,0.297,0.030,-0.599,0.081,0.076,-1.036,-0.358,0.435,0.242,0.436,1.009,1.087,0.778,0.134,-0.751,-0.191,0.075,0.739,-0.182,-0.412,0.115,-0.299,-0.442,0.174,0.692,0.650,-1.205,-1.214,0.621,-0.601,-0.992,-0.293,0.830,0.712,0.319,-0.175,-0.095,-0.084,-0.089,-1.300,-0.337,0.631,0.277,-0.471,-1.032,0.224,0.706,-0.622,-0.125,-0.297,0.689,-0.216,-1.072,-0.053,-0.297,-0.602,-0.229,-0.107,0.115,-0.085,-0.000,-0.288,-0.400,-0.201,-0.835,-0.171,0.185,-0.281,-0.317,-0.338,-0.043,0.262,-0.547,-0.093,-0.011,-0.195,-0.215,-0.649,0.181,0.063,-1.043,-0.916,0.380,0.444,0.106,-0.332,-0.485,-0.799,0.102,-1.091,-0.323,0.292,-0.319,-0.240,-0.890,-0.568,0.586,-0.611,-0.165,0.448,0.284,-0.297,-1.576,-0.254,-0.575,0.336,0.265,0.395,0.150,0.554,0.553,1.011,0.723,0.859,-0.710,-0.855,-0.524,-0.288,-0.004,-0.399,-0.459,-0.403,0.407,0.471,1.030,0.474,0.500,0.134,0.875,0.019,0.754,0.242,0.167,-0.244,1.746,1.285,0.438,0.156,-0.440,-0.075,-0.460,-0.562,-0.277,-0.936,-0.665,-0.570,-0.174,-0.333,-0.855,0.555,0.459,-0.020,-0.165,-0.522,-0.359,0.013,-0.633,-0.990,-0.652,-0.090,-0.215,0.272,-0.040,0.196,-1.241,-0.756,-0.805,-0.384,0.141,-0.412,-0.645,-0.744,-0.212,-0.479,0.939,-0.115,-0.184,-0.299,0.308,0.901,1.280,1.102,0.851,0.246,1.448,0.811,1.310,0.910,0.594,-0.206,-1.573,-0.191,-0.093,0.121,0.017,-0.708,-0.082,0.126,0.066,1.175,0.352,0.595,0.315,0.310,0.116,0.662,0.471,0.524,0.530,0.684,0.480,0.587,0.171,0.610,-0.053,-0.645,-0.559,0.022,0.215,-0.569,-0.079,0.010,0.245,0.548,1.016,0.139,0.467,0.300,0.385,-0.449,0.419,0.479,0.576,0.645,0.039,0.524,0.492,0.798,0.242,0.474,-0.571,-0.007,0.289,-0.269,-0.590,-0.330,-0.042,0.211,0.097,0.140,0.048,-0.068,0.385,0.016,0.483,0.527,0.126,-0.234,-0.282,1.073,0.561,-0.217,0.086,0.031,-0.011,0.275,-0.407,0.587,-0.442,-0.807,-0.570,-0.371,-0.641,-0.005,0.364,-0.360,-0.619,-0.049,0.011,0.442,-0.308,-0.074,-0.562,-0.132,0.601,0.326,0.046,0.071,-0.139,0.195,0.329,0.467,0.403,0.272,0.696,0.614,0.656,-0.448,-0.474,-0.436,-0.152,-1.007,-1.020,0.024,0.325,0.742,0.175,-0.200,0.063,0.510,1.835,0.780,0.664,0.440,-0.857,0.029,-0.311,0.921,-0.021,-1.827,-2.406,-0.168,0.314,-0.804,-1.441,-0.805,0.110,0.473,-0.061,0.100,0.608,0.230,-0.279,-0.114,1.066,0.193,-0.004,0.512,0.205,-0.138,-0.261,-0.056,0.521,-0.137,-1.328,0.255,0.315,0.013,-0.851,-1.938,-0.325,-1.077,-0.475,-0.109,-0.400,0.268,-0.039,-0.302,0.038,0.402,-0.004,-0.242,0.059,-0.228,0.071,0.102,-0.016,0.029,-0.331,-0.308,-0.317,0.265,-0.169,-0.444,-0.647,-0.365,-0.446,-0.172,0.099,0.233,-0.203,-0.374,-0.448,-0.390,0.865,0.262,-0.131,-0.137,0.294,1.248,1.941,-0.054,0.250,0.535,-0.548,1.137,0.384,-0.005,-0.881,-1.898,-1.098,-1.399,-1.047,-1.707,-1.572,-0.015,-0.413,0.058,0.282,-0.184,-0.472,0.556,0.817,0.496,0.349,0.315,0.873,0.782,0.539,0.615,-0.002,0.564,0.309,0.671,0.340,-0.741,-0.460,-0.232,-0.134,-0.131,-1.301,0.696,-0.043,1.050,-0.078,0.386,0.492,0.204,0.642,-0.778,-0.030,0.245,-0.588,0.864,-0.279,-0.271,0.401,0.257,0.807,0.033,0.049,-0.452,0.171,-0.406,-0.229,-0.034,0.157,-1.369,-0.751,-0.744,0.205,0.986,0.626,-0.046,1.063,0.822,1.817,-0.522,-0.147,0.909,0.288,1.294,-0.642,-0.184,-0.373,-0.785,-0.867,-0.480,0.089,-0.138,-0.207,0.598,0.750,0.520,0.772,-0.156,0.562,1.076,0.196,0.188,0.319,0.539,1.344,-0.083,0.883,0.343,0.158,0.458,0.076,0.599,-0.298,-0.889,-0.243,-0.452,-0.098,-0.009,0.670,0.403,0.313,0.206,-0.132,0.598,0.473,0.538,0.414,0.273,0.588,0.363,0.642,0.224,-0.264,0.367,0.123,0.463,0.373,-0.582,-0.391,-0.206,-0.029,-0.268,-0.884,-0.299,-0.140,0.452,0.668,-0.024,-1.025,0.049,0.596,-0.174,-0.602,-0.121,0.659,-0.314,-0.301,-0.666,-0.179,-0.310,-0.189,-0.096,-0.394,-0.682,-0.407,-0.384,-0.699,-0.959,0.175,-1.007,-0.433,0.121,0.173,0.004,0.139,-0.513,-0.409,0.491,0.385,-0.042,-0.135,0.198,-0.263,-0.658,0.157,0.367,-0.043,-0.576,-1.108,0.210,-0.316,-0.254,0.233,-0.068,-0.074,-0.436,-0.215,0.085,-0.013,0.008,0.044,-0.108,-0.961,-1.612,0.159,-0.829,-0.382,-0.187,-1.069,0.071,-0.156,0.139,0.120,0.350,-0.186,-0.297,-0.046,0.234,0.731,-1.546,-1.167,0.016,0.339,-0.444,0.967,-0.013,0.350,1.672,0.859,1.818,0.391,1.893,1.108,-0.235,0.733,1.575,0.642,-1.655,-2.250,-1.047,-0.137,-0.380,-0.178,-0.600,0.185,-0.538,-0.447,-0.099,-0.060,0.437,0.010,-0.905,-0.463,-0.005,0.495,-0.446,-0.598,-0.004,-0.635,-0.362,-0.374,-0.402,0.138,-0.692,-0.490,-0.949,-0.825,0.071,0.022,0.126,-0.125,-0.084,0.005,-0.064,-0.034,-0.172,-0.233,-0.156,-0.232,-0.045,-0.347,-0.324,-0.136,-0.290,0.041,-0.069,-0.322,-0.154,-0.316,-0.104,-0.265,-0.190,-0.018,0.157,0.195,-0.263,-0.151,-0.495,0.127,-0.452,0.508,-0.688,-1.304,-1.007,-0.872,-1.886,-1.409,-0.229,-1.308,-1.646,-0.089,0.259,0.385,-0.356,-0.605,-0.228,-0.403,-0.409,-0.201,-0.569,1.074,0.083,-0.479,-0.551,0.336,0.352,-0.479,-1.249,-0.730,-0.824,0.115,0.332,0.272,0.606,-0.221,-0.439,0.485,0.643,0.250,-0.386,-0.176,-0.433,0.391,0.689,-0.395,0.814,-0.501,-0.649,-0.275,-0.087,-0.499,-1.040,-0.172,-0.065,0.003,0.527,0.277,0.488,0.530,0.008,0.305,0.104,0.041,0.407,0.413,-0.123,-0.358,0.081,0.592,0.244,1.060,0.256,0.171,0.099,0.460,-0.049,-0.244,-1.131,-0.892,-1.950,0.796,0.516,1.040,0.982,-1.220,0.026,0.875,0.628,-0.208,-0.007,-0.708,-0.595,-0.052,0.066,0.237,0.344,-0.695,-1.200,-0.678,0.172,0.451,-0.276,-0.788,-0.574,-1.195,0.564,0.654,0.527,0.304,-0.135,-0.312,-0.044,0.670,0.583,0.750,-0.595,-0.623,0.357,0.631,0.318,0.625,-0.261,-0.365,-0.332,0.030,0.328,-0.173,-0.446,-0.316,-0.588,0.013,-0.032,0.583,0.207,-0.090,-0.058,-0.154,0.164,0.323,0.324,-0.194,0.003,0.200,0.502,0.074,0.665,-0.058,-0.695,0.243,0.099,0.661,0.428,-1.056,-1.240,-0.807,0.292,-0.281,0.505,0.142,-0.069,-0.029,-0.572,1.040,1.020,0.791,-0.214,-0.053,0.829,1.377,0.826,0.073,0.568,-0.117,-0.604,-0.084,0.266,0.484,-0.187,-0.570,-0.166,-1.141,0.333,-0.496,-0.597,0.118,-0.223,0.407,-0.655,-0.095,-0.226,-0.643,-0.270,-0.211,0.131,0.802,0.449,-0.481,-0.673,0.456,0.497,0.230,-0.859,-0.850,1.014,1.024,0.444,-0.332,-1.105,-0.950,-0.691,-0.358,0.234,-1.100,-0.412,-0.404,-0.153,-0.013,-0.018,0.288,0.080,-0.065,-0.018,-0.117,-0.462,-0.061,-0.694,1.673,1.135,0.456,0.722,0.306,2.193,-0.170,0.234,0.778,0.763,1.232,-0.038,0.318,-0.482,-0.552,-1.026,0.265,0.401,0.619,-0.071,-0.188,-0.380,-0.597,-0.208,0.482,0.060,-0.073,-0.181,0.212,-0.131,1.282,-0.272,-0.682,-0.195,0.266,0.433,-1.017,-0.659,0.029,-0.174,-0.551,-0.308,0.123,0.965,0.651,-0.239,-0.259,-0.294,-0.225,-0.350,-0.387,-0.478,-0.196,0.083,-0.375,-0.143,-0.415,-0.630,-0.068,-0.566,0.035,-0.578,-0.186,0.020,-0.525,-0.029,-0.377,-0.008,0.403,0.401,-1.422,-1.114,-0.277,-0.353,0.792,-1.412,-1.314,0.183,0.088,-0.540,-0.006,-0.457,-0.813,0.008,0.051,-0.365,-1.312,-0.215,-0.122,-0.926,-0.011,0.066,0.257,-0.010,0.186,-1.055,-0.289,0.082,0.389,0.398,0.253,0.112,0.361,0.480,0.029,0.029,0.479,0.237,0.261,-0.161,0.034,0.298,0.318,0.023,0.306,0.494,0.064,-0.060,0.052,0.398,-0.073,0.120,0.485,0.705,0.026,-0.739,0.462,0.208,0.186,-0.574,-0.272,0.582,-0.207,0.290,-0.293,0.754,0.439,0.579,0.243,-0.288,0.024,0.019,-0.105,-0.428,-0.217,-1.108,-0.829,-1.066,-0.086,0.660,1.282,-0.236,-0.011,0.913,0.552,0.002,-0.543,-1.257,-0.798,-0.934,-0.962,-0.517,-0.869,-1.082,0.683,-0.185,0.307,-0.537,0.002,0.087,-0.099,0.330,0.603,0.921,0.753,0.186,0.297,0.711,0.296,0.122,-0.141,0.220,0.577,0.036,0.072,0.114,0.199,0.677,-0.117,0.339,0.513,0.245,-0.124,-0.351,0.286,0.231,-0.135,0.101,0.193,0.055,-0.074,0.091,0.441,0.212,0.122,-0.087,0.411,0.428,0.469,0.258,0.071,0.125,0.302,0.404,0.108,0.037,-0.176,-0.086,0.068,0.179,-0.300,0.096,0.180,-0.001,0.068,0.561,-0.239,0.167,0.421,-0.116,-0.291,0.074,0.642,0.022,-0.622,0.307,0.037,0.099,-0.235,-0.406,0.945,-0.346,-0.498,-1.023,-0.237,0.635,-0.364,-0.987,0.268,0.655,0.285,-0.724,-0.027,0.440,0.542,-0.067,0.460,0.453,0.515,0.700,0.086,0.113,0.485,0.706,0.390,0.175,-0.448,0.025,0.200,-0.148,-0.408,-0.564,-0.242,0.755,0.809,0.597,-0.066,1.055,1.020,0.422,-0.018,-0.105,0.088,0.537,0.578,-0.115,-0.395,0.222,0.786,0.622,-0.177,-0.017,0.566,0.536,-0.338,-0.376,-0.102,0.546,0.694,0.885,0.305,-0.467,0.436,0.695,-0.208,-0.210,-0.283,0.411,-0.243,0.302,0.070,0.600,0.835,0.165,0.229,0.726,0.525,0.580,-0.175,0.203,0.982,1.642,-0.365,1.073,1.262,0.564,-1.047,-0.223,0.432,0.379,0.182,0.705,-0.168,0.664,0.122,-0.125,0.297,0.236,0.865,0.090,-0.256,0.222,0.267,0.569,-0.674,0.244,0.442,0.115,0.789,0.564,0.466,0.204,0.534,0.738,0.885,0.426,0.535,0.497,0.464,0.726,0.545,0.255,0.522,0.625,0.569,0.241,-0.112,0.476,0.470,0.268,0.151,0.123,-0.339,0.050,0.287,-0.078,-0.433,-0.330,-0.022,-0.067,-0.326,-0.048,-0.511,0.118,-0.317,-0.663,0.014,-0.184,0.160,-0.589,-0.702,-0.566,0.091,0.057,-0.318,-0.502,-0.174,-0.125,0.384,0.654,0.626,-0.052,-0.522,0.019,0.490,0.773,0.508,0.455,0.355,-0.320,0.508,-0.558,-0.851,-0.010,-0.112,-0.525,-0.300,-0.174,-0.116,-0.668,-0.175,0.730,0.116,-0.204,0.587,-0.734,-0.288,-1.098,-0.089,0.597,-1.434,-2.475,-2.067,0.445,-0.260,-0.558,-0.128,0.046,0.151,0.986,0.435,0.608,0.578,-0.413,-1.039,-0.681,0.646,0.092,-0.218,0.372,0.776,1.783,-0.110,0.228,0.456,2.028,1.991,-0.070,-0.167,-0.588,0.835,-0.235,-1.173,-0.457,-0.656,-0.161,-0.882,-0.845,0.437,-0.197,0.789,0.818,0.156,0.848,1.162,1.053,0.025,-0.532,0.863,1.079,0.139,-0.673,-0.548,0.575,-0.004,0.045,-0.071,-0.310,0.108,-0.035,-0.207,0.416,0.313,-0.105,-1.563,-0.591,0.574,0.042,0.455,0.081,0.019,-0.426,-0.699,-0.193,0.506,0.121,-0.730,-0.471,-0.096,-0.040,-0.035,-0.270,-0.213,-0.381,0.262,-0.201,-0.239,-0.014,-0.567,-0.959,-0.499,-0.066,-0.376,1.150,0.376,0.101,-1.346,-0.700,0.677,1.749,0.358,-1.636,-0.797,0.662,0.417,-0.793,0.165,-0.379,-0.198,0.905,0.027,0.426,0.643,-0.390,-1.468,-0.509,0.688,0.840,-0.644,-0.287,-0.697,-0.483,-0.482,-0.497,0.142,0.481,0.561,0.285,-0.436,-0.440,0.069,0.284,-0.776,-0.788,-0.346,-0.018,-0.023,0.134,-0.250,-0.429,-0.200,0.396,-0.095,0.099,0.494,-0.313,-0.707,-0.801,-0.351,-0.086,0.214,-0.616,-1.035,0.087,-0.294,-0.343,-0.295,-0.448,0.104,0.048,-0.186,0.005,-1.104,0.226,-0.347,-0.203,-0.478,-0.395,0.112,-0.431,0.512,-0.612,-0.127,0.115,0.125,-0.080,0.468,-0.266,-0.966,0.267,1.093,0.588,0.013,-0.297,0.018,0.118,-0.451,0.704,-0.010,0.243,-0.657,0.143,-0.419,-0.005,0.718,-0.061,-0.738,-0.789,-0.170,-0.875,-0.342,0.545,-0.410,-0.136,0.762,-0.526,-0.490,-0.030,-0.119,0.133,-0.767,-1.179,-0.862,-0.466,-0.422,0.030,0.790,-0.517,0.064,-1.443,0.320,0.541,-0.622,-0.151,-0.568,0.553,-0.372,-0.747,0.204,0.100,0.741,-0.446,-0.059,-0.393,0.393,0.786,-0.498,0.016,0.076,-0.448,0.012,-0.030,0.328,-0.481,-0.863,-0.359,0.583,-0.345,0.256,0.783,-1.319,-0.313,0.247,0.137,0.095,0.074,-0.167,-0.224,-0.140,-0.326,-0.068,-0.166,-0.181,-0.012,-0.243,-0.179,-0.094,-0.068,-0.361,-0.019,-0.260,-0.720,-0.562,-0.275,-0.199,0.306,-0.378,1.387,-0.143,0.013,0.614,-0.659,-0.524,-0.314,0.112,-0.267,0.378,-0.780,-0.417,0.248,-1.024,0.038,0.135,-0.285,-0.132,-0.015,1.160,0.298,0.000,-0.209,-1.276,0.278,0.512,0.191,0.352,0.303,-0.105,0.023,-0.341,-0.076,-0.387,-0.068,-0.117,-0.217,-0.219,-0.480,0.065,-0.142,0.008,-0.332,0.005,-0.276,-0.651,0.835,-0.657,0.031,-0.124,0.253,-0.127,-0.347,0.190,-0.490,0.333,-0.113,-0.405,0.072,-0.106,0.061,-0.936,-0.745,0.102,-0.101,0.024,0.966,0.321,-0.507,-0.285,-0.294,-0.490,0.255,0.327,0.307,0.551,0.047,0.307,-1.019,-0.389,0.055,-0.551,-0.411,-0.235,-0.236,-0.119,-0.168,-0.525,-0.209,-0.057,0.032,-0.165,-0.183,-0.319,-0.068,-0.111,-0.495,-0.611,-0.152,-0.353,-0.293,-0.331,-0.555,-0.327,-0.051,-0.128,-0.698,-0.646,-0.247,-0.115,-0.060,-0.575,-0.079,-0.497,0.129,-0.284,0.494,-0.182,0.068,0.081,0.021,0.244,0.574,-0.687,-0.233,0.562,-0.019,-0.119,0.484,-0.079,-0.462,0.291,0.279,0.084,-0.491,0.392,-0.658,-0.838,-0.726,0.021,-0.190,-0.882,-0.006,-0.868,-0.415,-0.277,-0.058,-0.178,-0.015,-0.556,-0.263,0.835,-0.158,0.036,-0.363,-0.410,-0.277,-0.107,0.036,-0.158,-0.065,-0.106,-0.090,-0.671,-1.219,-0.125,-0.214,0.449,-0.137,-0.828,-0.510,0.111,0.622,0.191,0.015,0.126,0.137,-0.395,-0.175,0.525,0.092,-0.426,0.595,0.200,0.036,-0.284,0.848,-0.041,0.067,-0.361,0.005,0.080,0.512,0.080,-0.222,0.058,-0.067,-0.290,-0.419,0.512,-0.718,0.197,0.273,-0.179,0.648,-0.064,-0.414,0.536,-0.985,-1.155,0.534,0.446,-0.113,0.600,-0.385,0.030,-0.408,-0.903,-0.095,0.219,-0.695,-0.573,-0.712,-0.226,-0.278,-1.735,0.075,-0.107,-0.354,-0.501,0.141,-0.539,-1.515,-0.410,-0.170,-0.146,-0.285,-0.855,-0.049,-0.159,-0.366,-0.179,-0.050,0.021,-0.135,-0.296,-0.059,-0.007,-0.555,-1.061,0.178,-0.538,-0.030,-0.180,-0.182,1.134,-0.109,-0.032,0.323,-0.379,-0.620,0.160,0.658,0.022,0.164,-0.427,-0.158,-0.590,-0.430,-0.094,-0.606,-1.097,0.594,-0.229,0.090,-0.179,-0.501,-1.593,0.216,0.534,0.418,-0.156,-0.500,0.403,0.337,-0.097,-0.269,-0.208,-1.464,0.509,-0.512,-0.927,-0.016,0.369,0.050,-0.114,0.091,0.537,-0.054,-0.147,-0.532,-0.145,-0.213,-0.506,-0.822,0.049,0.397,0.412,-0.073,-0.168,-0.417,0.139,-0.267,0.998,-0.034,0.119,-1.626,1.083,1.079,-0.064,0.232,0.044,0.257,0.774,-0.065,0.275,0.303,0.021,-0.507,0.124,0.496,-0.231,0.492,-0.259,-0.283,-0.406,-0.127,0.007,-0.590,-0.282,-0.306,0.006,-0.356,-0.305,0.199,0.278,-0.333,-0.391,-0.610,-0.094,-0.569,-1.105,0.096,0.322,0.466,0.504,-0.345,0.062,-0.386,-0.298,0.270,-0.445,0.204,0.467,0.131,-0.082,0.785,-0.174,-2.114,-0.341,-0.249,0.701,0.138,0.269,1.295,-0.087,0.560,0.248,0.176,-0.378,0.239,0.216,-0.120,0.370,-0.447,0.409,0.178,-0.185,-0.292,0.516,0.076,-0.589,0.011,-0.334,-0.274,0.262,-0.146,-0.458,0.341,-0.283,-0.405,0.211,0.390,-0.095,-1.365,-0.109,-2.376,-1.253,-0.313,-0.383,0.849,0.171,-0.375,0.049,1.609,-0.846,0.451,-0.905,-0.049,1.610,-0.548,0.478,0.347,-0.282,0.327,-0.694,-0.249,0.223,0.211,-0.169,0.541,-0.671,0.180,0.361,0.278,0.290,-0.166,-0.195,0.217,0.609,-0.081,-0.469,0.707,0.159,0.460,0.285,0.764,0.766,-0.828,0.244,0.114,0.508,0.012,-1.089,0.272,-0.345,0.465,-0.344,0.598,0.139,-0.077,0.320,0.408,-1.756,-0.216,0.549,0.359,-0.029,0.525,-0.221,0.460,0.099,0.044,0.797,0.222,0.169,0.392,1.157,0.482,-0.342,0.244,-0.004,-0.675,0.428,0.406,0.842,-0.157,-0.619,0.358,0.494,-0.153,0.810,0.377,-0.929,0.893,0.550,-0.712,-0.945,-0.166,0.304,-0.798,0.644,0.316,-0.536,-1.294,0.600,-0.768,0.133,-0.676,0.191,0.464,-0.059,0.213,0.592,-0.099,0.463,-0.053,0.534,0.004,0.401,0.572,0.040,0.036,1.288,0.052,-0.759,0.113,0.288,0.367,-0.190,-0.181,0.858,-0.205,-0.028,-0.047,-0.181,-0.494,-0.048,1.113,0.242,0.112,0.571,-0.098,0.313,1.007,0.258,-0.246,0.289,0.002,-0.004,-0.885,1.088,-0.454,0.335,-0.596,0.149, };


// Training

// Loss function for a single data
void lossFun(const FloatWN* w, const float* x, const float* y, float* val, FloatWN* grad) {

    // Conv layer 1
    float v1r[C1N][C1O * C1O], g1r[C1N][C1O * C1O];  // output value, gradient
    for (int i = 0; i < C1N; i++) {
        for (int j = 0; j < C1O * C1O; j++) {
            float s = 0.0f;
            for (int k = 0; k < C1S * C1S; k++)
                s += w->w1[i][k] * x[MC1[j][k]];  // multiply matrix
            activate(s, &v1r[i][j], &g1r[i][j]);  // apply activation function
        }
    }
    // Pooling
    float v1p[C1N][C2I * C2I], g1p[C1N][C2I * C2I];  // output value, gradient
    int i1p[C1N][C2I * C2I];  // indices of max value
    for (int i = 0; i < C1N; i++) {
        for (int j = 0; j < C2I * C2I; j++) {
            float s = -1e10f, g = 0.0f;
            for (int k = 0; k < 4; k++) {
                int it = MP1[j][k];
                float v = v1r[i][it];
                if (v > s) {
                    s = v, g = g1r[i][it];  // maxpool
                    i1p[i][j] = it;
                }
            }
            v1p[i][j] = s, g1p[i][j] = g;
        }
    }

    // Conv layer 2
    float v2r[C2N][C2O * C2O], g2r[C2N][C2O * C2O];  // output value, gradient
    for (int i = 0; i < C2N; i++) {
        for (int j = 0; j < C2O * C2O; j++) {
            float s = 0.0f;
            for (int pi = 0; pi < C1N; pi++) {
                for (int k = 0; k < C2S * C2S; k++)
                    s += w->w2[i][pi][k] * v1p[pi][MC2[j][k]];  // multiply matrix
            }
            activate(s, &v2r[i][j], &g2r[i][j]);  // apply activation function
        }
    }
    // Pooling
    float v2p[C2N][C2OA * C2OA],  // output value
        g2p[C2N][C2OA * C2OA];  // gradient
    int i2p[C2N][C2OA * C2OA];  // index of max value
    for (int i = 0; i < C2N; i++) {
        for (int j = 0; j < C2OA * C2OA; j++) {
            float s = -1e10f, g = 0.0f;
            for (int k = 0; k < 4; k++) {  // iterate through possible values
                int it = MP2[j][k];
                float v = v2r[i][it];
                if (v > s) {  // update
                    s = v, g = g2r[i][it];
                    i2p[i][j] = it;
                }
            }
            v2p[i][j] = s, g2p[i][j] = g;
        }
    }

    // 4 full-connected layers
    const int LN_I[4] = { L3I, L3N, L4N, L5N };  // layer input sizes
    const int LN_O[4] = { L3N, L4N, L5N, L6N };  // layer output sizes
    float v3i[L3I + 1];  // full-connected layer input
    std::memcpy(v3i, &v2p[0][0], sizeof(float) * L3I);
    v3i[L3I] = 1.0f;
    float v3[L3N + 1], g3[L3N + 1],  // layer 3
        v4[L4N + 1], g4[L4N + 1],  // layer 4
        v5[L5N + 1], g5[L5N + 1],  // layer 5
        v6r[L6N];  // output (no bias)
    const float* fci[4] = { v3i, v3, v4, v5 };  // layer inputs
    float* fcv[4] = { v3, v4, v5, v6r },  // layer outputs
        * fcg[4] = { g3, g4, g5, nullptr };  // gradient outputs
    const float* fcw[4] = { &w->w3[0][0], &w->w4[0][0], &w->w5[0][0], &w->w6[0][0] };  // layer weights
    for (int l = 0; l < 4; l++) {  // for each layer
        if (l < 3) {  // pad 1 for bias
            fcv[l][LN_O[l]] = 1.0f;
            fcg[l][LN_O[l]] = 0.0f;
        }
        for (int i = 0; i < LN_O[l]; i++) {
            float s = 0.0f;
            for (int j = 0; j <= LN_I[l]; j++)  // multiply matrix
                s += fcw[l][i * (LN_I[l] + 1) + j] * fci[l][j];
            if (l < 3) activate(s, &fcv[l][i], &fcg[l][i]);  // activate
            else fcv[l][i] = s;  // output layer
        }
    }

    // Softmax
    float v6[L6N];
    float g6[L6N][L6N];  // Jacobian, g6[i][j] = ∂v6[i]/∂v6r[j]
    float smax = -1e10f;  // prevent overflow in exp
    for (int i = 0; i < L6N; i++)
        smax = max(smax, v6r[i]);
    float sexp = 0.0f;  // sum of exps
    for (int i = 0; i < L6N; i++)
        sexp += (v6[i] = exp(v6r[i] -= smax));
    for (int i = 0; i < L6N; i++)  // value
        v6[i] /= sexp;
    for (int i = 0; i < L6N; i++)  // gradient
        for (int j = 0; j < L6N; j++)
            g6[i][j] = i == j
            ? v6[i] * (1.0 - v6[i])
            : -v6[i] * v6[j];

    // Cross-entropy
    float loss = 0.0f;  // final value
    float go[L6N];  // output derivative
    for (int i = 0; i < L6N; i++) {
        v6[i] = max(min(v6[i], 1.0f - 1e-7f), 1e-7f);
        float dv = -y[i] * log(v6[i]) - (1.0 - y[i]) * log(1.0 - v6[i]);
        go[i] = -y[i] / v6[i] + (1.0 - y[i]) / (1.0 - v6[i]);
        loss += dv;
    }
    *val = loss;

    // Backprop output layer
    float b6[L6N];  // ∂[loss]/∂[v6r]
    for (int i = 0; i < L6N; i++) {
        b6[i] = 0.0f;
        for (int j = 0; j < L6N; j++)
            b6[i] += g6[i][j] * go[j];
        for (int j = 0; j <= L5N; j++)
            grad->w6[i][j] = b6[i] * v5[j];
    }

    // Backprop full-connected layers
    float b5[L5N], b4[L4N], b3[L3N];
    float* fcb[4] = { b3, b4, b5, b6 };  // ∂[loss]/∂[v]
    float* fcwg[4] = { &grad->w3[0][0], &grad->w4[0][0], &grad->w5[0][0] };  // gradient of weights
    for (int l = 3; l--;) {
        for (int i = 0; i < LN_O[l]; i++) {
            fcb[l][i] = 0.0f;  // gradient to layer
            for (int j = 0; j < LN_O[l + 1]; j++)
                fcb[l][i] += fcb[l + 1][j] * fcw[l + 1][j * (LN_I[l + 1] + 1) + i] * fcg[l][i];
            for (int j = 0; j <= LN_I[l]; j++)  // gradient to weights
                fcwg[l][i * (LN_I[l] + 1) + j] = fcb[l][i] * fci[l][j];
        }
    }

    // Backprop conv layer 2
    float b2p[C2N][C2OA * C2OA];  // ∂[loss]/∂[v2p], after pooling
    for (int i = 0; i < C2N; i++)
        for (int j = 0; j < C2OA * C2OA; j++) {
            b2p[i][j] = 0.0f;
            for (int k = 0; k < L3N; k++)
                b2p[i][j] += b3[k] * w->w3[k][i * C2OA * C2OA + j] * g2p[i][j];
        }
    float b2r[C2N][C2O * C2O];  // ∂[loss]/∂[v2r], before pooling
    for (int i = 0; i < C2N; i++)
        for (int j = 0; j < C2O * C2O; j++)
            b2r[i][j] = 0.0f;  // initialize to zero
    for (int i = 0; i < C2N; i++)
        for (int j = 0; j < C2OA * C2OA; j++)
            b2r[i][i2p[i][j]] = b2p[i][j];  // fill non-zero values
    for (int i = 0; i < W2N; i++)
        (&grad->w2[0][0][0])[i] = 0.0f;  // init gradient to zero
    for (int i = 0; i < C2N; i++)
        for (int j = 0; j < C2O * C2O; j++)  // write the gradient of weights
            for (int pi = 0; pi < C1N; pi++)
                for (int k = 0; k < C2S * C2S; k++)
                    grad->w2[i][pi][k] += v1p[pi][MC2[j][k]] * b2r[i][j];

    // Backprop conv layer 1 - almost there!
    float b1p[C1N][C2I * C2I];  // ∂[loss]/∂[v1p], after pooling
    for (int i = 0; i < C1N; i++)
        for (int j = 0; j < C2I * C2I; j++)
            b1p[i][j] = 0.0f;  // clear to zero
    for (int i = 0; i < C2N; i++)
        for (int j = 0; j < C2O * C2O; j++)
            for (int pi = 0; pi < C1N; pi++)
                for (int k = 0; k < C2S * C2S; k++) {  // add gradient to b1p
                    int it = MC2[j][k];
                    b1p[pi][it] += b2r[i][j] * w->w2[i][pi][k] * g1p[pi][it];
                }
    float b1r[C1N][C1O * C1O];  // ∂[loss]/∂[v1r], before pooling
    for (int i = 0; i < C1N; i++)
        for (int j = 0; j < C1O * C1O; j++)
            b1r[i][j] = 0.0f;  // initialize to zero
    for (int i = 0; i < C1N; i++)
        for (int j = 0; j < C2I * C2I; j++)
            b1r[i][i1p[i][j]] = b1p[i][j];  // fill non-zero values
    for (int i = 0; i < W1N; i++)
        (&grad->w1[0][0])[i] = 0.0f;  // clear gradient to zero
    for (int i = 0; i < C1N; i++)
        for (int j = 0; j < C1O * C1O; j++)
            for (int k = 0; k < C1S * C1S; k++)  // write the gradient of weights
                grad->w1[i][k] += x[MC1[j][k]] * b1r[i][j];

}

// Loss function for a data batch
void lossFunBatch(int ndata, const float* w, const float* x, const float* y, float* val, float* grad) {
    *val = 0.0f;
    for (int i = 0; i < WN; i++) grad[i] = 0.0f;  // init to zero
    float val_t, grad_t[WN];
    for (int di = 0; di < ndata; di++) {
        lossFun((const FloatWN*)w, &x[di * X_DIM], &y[di * Y_DIM], &val_t, (FloatWN*)grad_t);
        *val += val_t;  // add data
        for (int i = 0; i < WN; i++) grad[i] += grad_t[i];
    }
    // divide by number of data to average
    *val /= float(ndata);
    for (int i = 0; i < WN; i++) grad[i] /= float(ndata);
}

// Adam optimizer
void minimizeAdam(
    int ndim, int ndata,
    std::function<void(int ndata, const float* w, const float* x, const float* y, float* val, float* grad)> lossfun,
    float* w, const float* x, const float* y,
    int batch_size, float learning_step,
    float beta_1, float beta_2, int max_epoch, float gtol,
    std::function<void(int epoch)> callback
) {

    // loss and gradient
    float loss_t, loss = 0.0f;
    float* grad_t = new float[ndim];  // gradient in evaluation
    float* grad = new float[ndim];  // smoothed gradient
    float* grad2 = new float[ndim];  // smoothed squared gradient
    for (int i = 0; i < ndim; i++) {
        grad[i] = 0.0f;
        grad2[i] = 0.0f;
    }

    // epoches
    for (int epoch = 0; epoch < max_epoch; callback(++epoch)) {

        // batches
        for (int batch = 0; batch < ndata; batch += batch_size) {
            // get data
            int n_batch = min(ndata - batch, batch_size);
            // evaluate function
            lossfun(n_batch, w, &x[batch * X_DIM], &y[batch * Y_DIM], &loss_t, grad_t);
            // check NAN
            bool has_nan = false;
            for (int i = 0; i < WN; i++) {
                if (std::isnan(grad_t[i])) has_nan = true;
            }
            if (has_nan) continue;
            // update
            loss = beta_1 * loss + (1.0f - beta_1) * loss_t;
            for (int i = 0; i < ndim; i++) {
                grad[i] = beta_1 * grad[i] + (1.0f - beta_1) * grad_t[i];
                grad2[i] = beta_2 * grad2[i] + (1.0f - beta_2) * grad_t[i] * grad_t[i];
                w[i] -= learning_step * grad[i] / (sqrt(grad2[i]) + 1e-8f);
            }
            // verbose
            if ((batch / batch_size) % 10 == 0)
                fprintf(stderr, "\r%.1f%% loss=%f", float(batch + batch_size) * 100.0f / float(ndata), loss);
        }

        // check
        float grad_norm = 0.0f;
        for (int i = 0; i < ndim; i++) grad_norm += grad[i] * grad[i];
        grad_norm = sqrt(grad_norm);
        fprintf(stderr, "\rEpoch %d/%d, loss=%f, grad=%f\n", epoch + 1, max_epoch, loss, grad_norm);
        if (grad_norm < gtol) break;
    }

    delete x; delete y;
    delete grad_t; delete grad; delete grad2;
}

// SGD optimizer
void minimizeSGD(
    int ndim, int ndata,
    std::function<void(int ndata, const float* w, const float* x, const float* y, float* val, float* grad)> lossfun,
    float* w, const float* x, const float* y,
    int batch_size, float learning_step,
    float momentum, int max_epoch, float gtol,
    std::function<void(int epoch)> callback
) {

    // loss and gradient
    float loss_t, loss = 0.0f;
    float* grad_t = new float[ndim];  // gradient in evaluation
    float* grad = new float[ndim];  // smoothed gradient
    for (int i = 0; i < ndim; i++) {
        grad[i] = 0.0f;
    }

    // epoches
    for (int epoch = 0; epoch < max_epoch; callback(++epoch)) {

        // batches
        for (int batch = 0; batch < ndata; batch += batch_size) {
            // get data
            int n_batch = min(ndata - batch, batch_size);
            // evaluate function
            lossfun(n_batch, w, &x[batch * X_DIM], &y[batch * Y_DIM], &loss_t, grad_t);
            // check NAN
            bool has_nan = false;
            for (int i = 0; i < WN; i++) {
                if (std::isnan(grad_t[i])) has_nan = true;
            }
            if (has_nan) continue;
            // update
            loss = momentum * loss + (1.0f - momentum) * loss_t;
            for (int i = 0; i < ndim; i++) {
                grad[i] = momentum * grad[i] + (1.0f - momentum) * grad_t[i];
                w[i] -= learning_step * grad[i];
            }
            // verbose
            if ((batch / batch_size) % 10 == 0)
                fprintf(stderr, "\r%.1f%% loss=%f", float(batch + batch_size) * 100.0f / float(ndata), loss);
        }

        // check
        float grad_norm = 0.0f;
        for (int i = 0; i < ndim; i++) grad_norm += grad[i] * grad[i];
        grad_norm = sqrt(grad_norm);
        fprintf(stderr, "\rEpoch %d/%d, loss=%f, grad=%f\n", epoch + 1, max_epoch, loss, grad_norm);
        if (grad_norm < gtol) break;
    }

    delete x; delete y;
    delete grad_t; delete grad;
}



// Testing

// Evaluate a test data
void evalFun(const FloatWN* w, const float* x, float* y) {
    float gtemp;

    // Conv layer 1
    float v1r[C1N][C1O * C1O];
    for (int i = 0; i < C1N; i++) {
        for (int j = 0; j < C1O * C1O; j++) {
            float s = 0.0f;
            for (int k = 0; k < C1S * C1S; k++)
                s += w->w1[i][k] * x[MC1[j][k]];  // multiply matrix
            activate(s, &v1r[i][j], &gtemp);  // apply activation function
        }
    }
    // Pooling
    float v1p[C1N][C2I * C2I];
    for (int i = 0; i < C1N; i++) {
        for (int j = 0; j < C2I * C2I; j++) {
            float s = -1e10f;
            for (int k = 0; k < 4; k++)
                s = max(s, v1r[i][MP1[j][k]]);
            v1p[i][j] = s;
        }
    }

    // Conv layer 2
    float v2r[C2N][C2O * C2O];
    for (int i = 0; i < C2N; i++) {
        for (int j = 0; j < C2O * C2O; j++) {
            float s = 0.0f;
            for (int pi = 0; pi < C1N; pi++) {
                for (int k = 0; k < C2S * C2S; k++)
                    s += w->w2[i][pi][k] * v1p[pi][MC2[j][k]];  // multiply matrix
            }
            activate(s, &v2r[i][j], &gtemp);  // apply activation function
        }
    }
    // Pooling
    float v2p[C2N][C2OA * C2OA];
    for (int i = 0; i < C2N; i++) {
        for (int j = 0; j < C2OA * C2OA; j++) {
            float s = -1e10f;
            for (int k = 0; k < 4; k++)
                s = max(s, v2r[i][MP2[j][k]]);
            v2p[i][j] = s;
        }
    }

    // 4 full-connected layers
    const int LN_I[4] = { L3I, L3N, L4N, L5N };  // layer input sizes
    const int LN_O[4] = { L3N, L4N, L5N, L6N };  // layer output sizes
    float v3i[L3I + 1];  // full-connected layer input
    std::memcpy(v3i, &v2p[0][0], sizeof(float) * L3I);
    v3i[L3I] = 1.0f;
    float v3[L3N + 1], v4[L4N + 1], v5[L5N + 1], v6r[L6N];
    const float* fci[4] = { v3i, v3, v4, v5 };  // layer inputs
    float* fcv[4] = { v3, v4, v5, v6r };  // layer outputs
    const float* fcw[4] = { &w->w3[0][0], &w->w4[0][0], &w->w5[0][0], &w->w6[0][0] };  // layer weights
    for (int l = 0; l < 4; l++) {  // for each layer
        if (l < 3)  // pad 1 for bias
            fcv[l][LN_O[l]] = 1.0f;
        for (int i = 0; i < LN_O[l]; i++) {
            float s = 0.0f;
            for (int j = 0; j <= LN_I[l]; j++)  // multiply matrix
                s += fcw[l][i * (LN_I[l] + 1) + j] * fci[l][j];
            if (l < 3) activate(s, &fcv[l][i], &gtemp);  // activate
            else fcv[l][i] = s;  // output layer
        }
    }

    // output
    for (int i = 0; i < Y_DIM; i++)
        y[i] = v6r[i];
}

// Convert output vector to class
int getClass(const float* y) {
    int maxi = -1;
    float maxval = -1e10f;
    for (int i = 0; i < Y_DIM; i++) {
        if (y[i] > maxval)
            maxi = i, maxval = y[i];
    }
    return maxi;
}



// Main functions


// Main training function
void mainTrain() {

    // data
    float* x_train, * y_train;
    int n_train = loadMnist("bin/all_x.bin", "bin/all_y.bin", x_train, y_train);

    // random weights
    if (0) {
        for (int i = 0; i < WN; i++)
            WEIGHTS[i] = 0.1 * (2.0f * randf() - 1.0f);
    }
    fprintf(stderr, "%d weights\n", WN);

    // check the correctness of gradient
    if (0) fprintf(stderr, "grad_err: %g\n",
        checkGrad(WN, [&](const float* w, float* val, float* grad) {
            lossFun((const FloatWN*)w, &x_train[0], &y_train[0], val, (FloatWN*)grad);
            }, WEIGHTS, 0.004, true)
    );

    // Optimization
    auto callback = [&](int epoch) {
        // test accuracy
        const int test_count = 1000;
        int correct_count = 0;
        float y[Y_DIM];
        for (int _ = 0; _ < test_count; _++) {
            int i = randu() % n_train;
            evalFun((FloatWN*)WEIGHTS, &x_train[i * X_DIM], y);
            int class_guessed = getClass(y);
            int class_correct = getClass(&y_train[i * Y_DIM]);
            if (class_correct == class_guessed) correct_count += 1;
        }
        printf("%d/%d correct\n", correct_count, test_count);
        // export weights
        FILE* fp = fopen("weights.txt", "w");
        for (int i = 0; i < WN; i++) fprintf(fp, "%.3f,", WEIGHTS[i]);
        fclose(fp);
    };
    minimizeAdam(WN, n_train, lossFunBatch, WEIGHTS, x_train, y_train, \
        50, 0.01f, 0.9f, 0.999f, \
        5, 1e-6f, callback);
    // minimizeSGD(WN, n_train, lossFunBatch, WEIGHTS, x_train, y_train, \
        50, 0.03f, 0.9f, \
        5, 1e-6f, callback);
}


// Main solving function - https://dmoj.ca/problem/tle18p1
void mainSolve() {
#ifdef _WIN32
    freopen("stdin.txt", "r", stdin);
#endif
    std::cin.sync_with_stdio(0); std::cin.tie(0);
    int n; std::cin >> n;
    while (n--) {
        float x[X_DIM], y[Y_DIM];
        for (int i = 0; i < X_DIM; i++) std::cin >> x[i];
        evalFun((const FloatWN*)WEIGHTS, x, y);
        printf("%d\n", getClass(y));
    }
}


// Main
int main(int argc, char* argv[]) {
    initMatrices();

    // mainTrain();
    mainSolve();

    return 0;
}